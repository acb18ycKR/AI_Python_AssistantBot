{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CLASS\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "logging.langsmith(\"CLASS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFPlumberLoader('./data/converted_data_with_metadata.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 3, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n미국 민권위원회, 연방정부의 얼굴인식 기술 사용에 따른 민권 영향 분석\\nKEY Contents\\nn 미국 민권위원회에 따르면 연방정부와 법 집행기관에서 얼굴인식 기술이 빠르게 도입되고\\n있으나 이를 관리할 지침과 감독의 부재로 민권 문제를 초래할 위험 존재\\nn 미국 민권위원회는 연방정부의 책임 있는 얼굴인식 기술 사용을 위해 운영 프로토콜 개발과\\n실제 사용 상황의 얼굴인식 기술 평가 및 불평등 완화, 지역사회의 의견 수렴 등을 권고\\n£연방정부의 얼굴인식 기술 도입에 대한 지침과 감독 부재로 민권 문제를 초래할 위험 존재\\nn 미국 민권위원회(U.S. Commission on Civil Rights)가 2024년 9월 19일 연방정부의 얼굴인식\\n기술 사용이 민권에 미치는 영향을 분석한 보고서를 발간\\n∙ AI 기술의 일종인 얼굴인식 기술은 연방정부와 법 집행기관에서 빠르게 도입되고 있으며, 일례로\\n법무부 연방수사국(FBI)은 범죄 수사 및 용의자 수색용 단서 확보를 위해 얼굴인식 기술을 가장 빈번히 사용\\n∙ 그러나 얼굴인식 기술의 책임 있는 사용을 위한 연방 지침과 감독은 실제 활용 사례보다 뒤처졌으며,\\n현재 연방정부의 얼굴인식 기술이나 여타 AI 기술 사용을 명시적으로 규제하는 법률도 부재\\nn 보고서에 따르면 얼굴인식 기술의 무분별한 사용은 편향, 개인정보 침해, 적법 절차의 미준수\\n및 차별적 영향과 같은 민권 문제를 초래할 위험 보유\\n∙ 얼굴인식 기술의 정확도는 인종, 성별, 연령 등 인구통계학적 요인에 따라 달라질 수 있으며, 이는 식별\\n오류 및 부정확한 체포로 이어져 유색인종을 비롯한 특정 집단에 차별적 결과를 초래할 위험 존재\\n∙ 정부 기관이 사전 영장이나 정당한 이유 없이 얼굴인식 기술을 광범위하게 사용할 경우 개인을\\n지속적으로 추적하고 감시함으로써 개인정보 보호 권리에 심각한 영향을 미칠 위험 존재\\n∙ 법 집행기관의 얼굴인식 기술 사용 시 부정확한 식별 및 편향으로 인해 개인이 법의 보호를 받아\\n공정하고 올바르게 대우받을 권리를 침해할 가능성도 존재\\n£민권위원회, 연방정부의 책임 있는 얼굴인식 기술 사용을 위한 권고사항 제시\\nn 민권위원회는 연방정부의 얼굴인식 기술 사용과 관련해 다음과 같은 권고사항을 제시\\n∙ 국립표준기술연구소(NIST)는 정부 기관의 얼굴인식 기술 시스템 도입 시의 효과와 공평성, 정확성\\n평가에 사용할 수 있는 운영 테스트 프로토콜의 개발 필요\\n∙ 각 연방정부 기관의 최고AI책임자는 실제 사용 상황에서 얼굴인식 기술을 평가하고 차별이나 편견으로\\n인한 불평등을 완화하며, 얼굴인식 기술의 사용으로 영향을 받는 지역사회의 의견을 수렴 필요\\n∙ 얼굴인식 기술 제공업체는 다양한 인구통계 집단에 대한 높은 정확도를 보장하기 위해 지속적인 교육과\\n지원, 업데이트를 제공 필요\\n☞ 출처: U.S. Commission on Civil Rights, The Civil Rights Implications of the Federal Use of Facial Recognition Technology, 2024.09.19.\\n1\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 4, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2024-11월호\\n미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발표\\nKEY Contents\\nn 미국 백악관 예산관리국이 바이든 대통령의 AI 행정명령에 따라 연방정부의 책임 있는 AI 조달을\\n지원하기 위한 지침을 발표\\nn 지침은 정부 기관의 AI 조달 시 AI의 위험과 성과를 관리할 수 있는 모범 관행의 수립 및 최상의 AI\\n솔루션을 사용하기 위한 공급업체 시장의 경쟁 보장, 정부 기관 간 협업을 요구\\n£백악관 예산관리국, 연방정부의 AI 조달 시 책임성을 증진하기 위한 모범 관행 제시\\nn 미국 백악관 예산관리국(OMB)이 바이든 대통령의 AI 행정명령에 따른 후속 조치로 2024년 10월 3일\\n‘정부의 책임 있는 AI 조달 지침(M-24-18)’을 발표\\n∙ 미국 연방정부는 2023년 1,000억 달러 이상의 IT 제품과 서비스를 구매한 미국 경제 최대 규모의 단일\\n구매자로서 구매력을 활용해 책임 있는 AI의 발전을 뒷받침할 계획\\n∙ 이번 지침은 △AI 위험과 성과 관리 △AI 시장의 경쟁 촉진 △연방정부 전반의 협업 보장이라는 3개\\n전략적 목표에 대하여 권고사항을 제시\\nn (AI 위험과 성과 관리) 예산관리국의 지침은 AI 시스템의 구축, 훈련, 배포 방식의 복잡성을 고려해\\nAI의 위험과 성과를 관리하기 위한 모범 관행을 다음과 같이 제시\\n∙ 정부 기관의 개인정보 보호 담당자가 AI 조달 프로세스에 조기에 지속적으로 참여해 개인정보 보호\\n위험을 식별 및 관리하고 법률과 정책 준수를 보장\\n∙ 정부 기관과 공급업체와 간 협력으로 AI 솔루션이 조달되는 시기와 해당 조달로 인해 시민 권리와\\n안전에 영향을 미치는 AI에 대하여 추가로 위험관리가 필요한 시점을 파악\\n∙ 성과 기반의 혁신적 조달 기법을 활용해 정부 기관이 위험을 효과적으로 관리 및 완화하고 성과를 향상할\\n수 있도록 장려하는 한편, 정부 데이터와 지식재산권을 보호하는 방식으로 계약 조건을 협상\\nn (AI 시장의 경쟁 촉진) 지침은 정부 기관이 최상의 AI 솔루션을 사용할 수 있도록 공급업체 시장에서\\n강력한 경쟁을 보장할 것을 요구\\n∙ 계약 요건 수립 시 공급업체 의존성을 최소화할 수 있는 인수 원칙을 적용하고, 시장 조사와 요구사항\\n개발, 공급업체 평가 절차에서 상호운용성과 투명성을 고려하며, 혁신적 조달 관행을 활용해 우수한\\n계약업체 성과와 정부 기관의 임무 성과를 보장\\nn (연방정부 전반의 협업 보장) 빠르게 발전하는 AI 기술환경의 위험관리를 위해 AI 전문지식을 갖춘\\n공무원과 조달, 개인정보보호, 사이버보안 전문가를 포함하는 협업 팀을 구성해 전략적 조달을 지원\\n∙ 각 정부 기관은 기관 간 협의회를 구성해 효과적이고 책임 있는 AI 조달을 지원하고, 협업 시 기관 목표에\\n가장 적합한 AI 투자 식별 및 우선순위 지정, AI 배포 역량 개발, AI 모범 활용 사례 채택 증진 등을 고려\\n☞ 출처: The White House, FACT SHEET: OMB Issues Guidance to Advance the Responsible Acquisition of AI in\\nGovernment, 2024.10.03.\\n2\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 5, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간\\nKEY Contents\\nn 유로폴의 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오\\n분석 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\nn 그러나 AI 도입을 위해서는 기술적 과제 해결 및 다양한 윤리적·사회적 이슈 대응이\\n필요하며, EU AI 법에 부합하도록 기존 AI 시스템에 대한 평가와 수정도 필요\\n£유로폴, 법 집행에서 AI 기술의 윤리적이고 투명한 구현을 위한 고려사항 제시\\nn EU 사법기관 유로폴(Europol)이 2024년 9월 24일 법 집행에서 효과적 범죄 퇴치를 위한 AI의\\n활용 가능성을 탐색한 보고서를 발간\\n∙ 보고서는 법 집행에서 AI 기술을 윤리적이고 투명하게 구현하기 위한 지침 역할을 하며, AI의 이점과\\n과제를 함께 다룸으로써 법 집행에서 AI 사용 시 윤리적 고려 사항에 대한 인식 제고를 추구\\nn 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오 분석, 생체인식\\n시스템 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\n∙ 법 집행기관은 AI 기반 데이터 분석을 활용해 범죄 활동에 대한 탐지와 대응 능력을 강화하고, AI\\n도구로 구조화되지 않은 데이터를 신속히 분석해 비상 상황의 의사결정을 위한 통찰력 확보 가능\\n∙ 기계번역과 같은 AI 기반 도구는 여러 국가가 참여하는 조사에서 원활한 국제협력을 위해서도 필수적\\nn 그러나 법 집행에서 AI 도구의 효과적이고 책임 있는 활용을 위해 해결되어야 할 기술적 과제 및\\n다양한 윤리적·사회적 우려도 존재\\n∙ 일례로 관할권 간 데이터 수집과 보관 관행의 차이에 따른 데이터셋의 편향으로 인해 AI 산출물의\\n무결성(無缺性)이 손상될 수 있어 표준화된 데이터 수집 규약 필요\\n∙ 데이터 규모나 활용 사례의 복잡성과 관계없이 AI 도구를 효과적으로 사용하려면 다양한 데이터\\n규모와 운영 요구사항에 적응할 수 있는 확장성과 성능을 갖춘 AI 모델도 개발 필요\\n∙ 편향, 개인정보 침해와 인권 침해와 같은 다양한 윤리적·사회적 우려도 존재하며, 이를 해소하기\\n위해 데이터 편향을 제거하고 공공 안전과 개인정보 간 균형을 유지하며 AI 의사 결정 과정에\\n대한 투명성과 책임성을 보장 필요\\nn 보고서는 2024년 8월 발효된 EU AI 법이 법 집행기관에 미칠 영향도 분석\\n∙ EU AI 법은 공공장소에서 실시간 생체인식 식별과 같은 특정 애플리케이션의 사용을 금지하고\\n고위험 AI 시스템에 엄격한 감독을 부과하였으나 법 집행 활동의 특수성을 고려해 일부 예외를 설정\\n∙ 그러나 일부 예외에도 법 집행 역량 강화를 위한 AI 사용을 위해서는 기존에 도입한 AI\\n시스템에 대한 재평가와 수정이 필요한 만큼, 재정과 인력 측면의 상당한 부담 예상\\n☞ 출처: Europol, AI and policing-The benefits and challenges of artificial intelligence for law enforcement, 2024.09.24.\\n3\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 6, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2024-11월호\\nOECD, 공공 부문의 AI 도입을 위한 G7 툴킷 발표\\nKEY Contents\\nn OECD는 공공 부문에서 EU 및 G7 국가들의 AI 도입 모범사례와 거버넌스 프레임워크,\\n정책 옵션을 토대로 공공 부문의 AI 도입을 안내하는 보고서를 발표\\nn 보고서는 공공 부문의 AI 도입 시 프로토타입부터 시작해 시범 도입을 거쳐 본격적으로\\n구현하는 단계별 접근방식을 권고\\n£OECD, G7의 사례를 토대로 공공 부문의 AI 도입을 안내하는 지침 마련\\nn OECD가 2024년 10월 15일 안전하고 신뢰할 수 있는 AI의 원칙을 실행 가능한 정책으로 전환할\\n수 있도록 지원하는 ‘공공 부문의 AI를 위한 G7 툴킷’ 보고서를 발간\\n∙ OECD는 G7 회원국이 작성한 설문 응답 및 OECD와 UNESCO의 연구를 토대로 공공 부문에서 AI\\n활용 모범사례와 거버넌스 프레임워크, 정책 옵션과 관련된 종합적 지침 제공을 목표로 보고서를 작성\\nn G7과 EU의 AI 도입 추세를 분석한 결과, G7 회원국과 EU는 공공 부문의 AI 도입과 관련된\\n국가 전략 및 정책의 개발과 구현에서 차이가 존재\\n∙ EU·독일·미국·영국·일본은 국가 AI 전략에 공공 부문을 포함했고 프랑스는 국가 AI 전략에서는\\n공공 부문을 구체적으로 다루지 않으나 공공행정 혁신기금(FTAP)을 조성하여 60개 이상의 AI\\n프로젝트에 투자하는 등 별도의 정책을 수립\\n∙ 캐나다는 2025년 봄까지 공공 서비스를 위한 AI 전략을 개발할 계획이며, 이탈리아는 ‘공공\\n부문 디지털화를 위한 3개년 계획(2024~2026)’에 AI를 포함\\n∙ G7 회원국들은 접근방식의 차이에도 인재와 기술 개발, 조달 정책, 협력관계 구축, 윤리적이고\\n신뢰할 수 있으며 인간 중심적인 AI 관행 조성, 데이터 품질 보장 등에서 공통점을 보유\\nn AI 거버넌스 프레임워크 측면에서 G7 회원국 중 미국·캐나다·프랑스와 EU는 여러 기관이 AI를 관리하는\\n분산형 거버넌스 구조를 채택했으며 이탈리아·독일·영국은 단일 기관이 AI를 관리하는 중앙집중형\\n거버넌스를 채택\\nn G7 회원국들은 공공 부문의 운영 효율성 향상, 정책 결정 강화, 공공 서비스 개선, 정부의 투명성과\\n책임성 강화를 위해 AI를 활용하는 한편, 다양한 정책 옵션으로 AI 도입 시의 과제 해결을 모색\\n∙ AI 도입에 필수적인 인프라를 강화하기 위한 데이터 저장과 공유 솔루션 채택, AI에 적합한\\n혁신적이고 유연한 조달 절차의 수립 및 민간 파트너십 육성, 공공 부문의 AI 역량 강화, 데이터\\n거버넌스 프레임워크 구축 등이 대표적인 정책 옵션\\nn 보고서는 공공 부문의 AI 도입 시 각 단계를 신중히 관리하여 위험을 완화할 수 있도록, 문제를\\n명확히 정의하고 아이디어를 구상한 뒤 프로토타입부터 시작해 통제된 환경에서 AI를 시범 도입한\\n후 이를 개선해 본격적으로 구현하는 단계적 접근방식을 강조\\n☞ 출처: OECD, G7 Toolkit for Artificial Intelligence in the Public Sector, 2024.10.15.\\n4\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 7, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n세계경제포럼, 생성AI 시대의 거버넌스 프레임워크 제시\\nKEY Contents\\nn 세계경제포럼이 글로벌 정책입안자를 대상으로 생성AI의 공익적 활용과 경제·사회적\\n균형 달성, 위험 완화를 위한 거버넌스 프레임워크를 제안하는 백서를 발표\\nn 백서에 따르면 정부는 기존 규제를 평가해 생성AI로 인한 규제 격차를 해소하는 한편, 다양한\\n이해관계자 간 지식 공유를 촉진하고 미래의 AI 발전에 대비한 규제 민첩성을 갖출 필요\\n£생성AI 거버넌스, 과거-현재-미래를 아우르는 프레임워크 수립 필요\\nn 세계경제포럼(WEF)이 2024년 10월 8일 세계 각국의 정책입안자를 대상으로 생성AI 거버넌스\\n프레임워크를 제시한 백서를 발간\\n∙ 백서는 생성AI의 공익적 활용과 경제·사회적 균형 달성, 위험 완화라는 목표 달성을 위해 △과거\\n활용(Harness Past) △현재 구축(Build Present) △미래 계획(Plan Future)의 프레임워크를 제안\\nn (과거 활용) 기존 규제를 활용하고 생성AI로 인한 규제 격차를 해소하는 것으로, 정부는 새로운 AI\\n규제나 관할 당국을 수립하기에 앞서 다음 사항을 추진할 필요\\n∙ 생성AI로 인한 문제나 격차 발생에 관하여 기존 규제를 평가하고 다양한 규제 수단의 정책 목표를\\n고려해 규제를 조정하며, 규제 선례를 참고해 책임 할당을 명확히 하고 격차가 발견된 부분을 보완\\n∙ 기존 규제 당국이 생성AI 문제를 해결할 역량이 있는지 평가하고, AI 전담 기관을 설치하여 규제\\n권한을 집중하는 방안의 장단점을 고려\\nn (현재 구축) 사회 전반의 생성AI 거버넌스와 지식 공유의 증진을 의미하며, 생성AI의 거버넌스에는\\n정책입안자와 규제 당국 외에 산업계, 시민사회, 학계를 포함한 이해관계자 참여가 필수적\\n∙ 정부는 다양한 거버넌스 수단을 활용해 사회 전반의 생성AI 거버넌스에 참여하는 각 이해관계자\\n집단의 고유한 문제에 대응 필요\\n∙ 다양한 이해관계자 간 지식 공유를 촉진하고, 책임 있는 AI 관행으로 사회에 모범을 보일 필요성 존재\\nn (미래 계획) 생성AI 거버넌스에 대한 민첩한 준비와 함께 국제협력을 촉진하는 것으로, 정부는 빠\\n른 기술 발전과 한정된 자원, 글로벌 불확실성을 고려해 미래를 예견한 국가 전략을 개발하고 다음\\n의 활동을 추진\\n∙ 정부 내 AI 역량 향상과 AI 전문가 채용을 위한 투자를 시행하고 AI 전담 기관의 설립 필요성을 신중히 검토\\n∙ 생성AI와 인간 간 상호작용, 생성AI와 여타 기술의 융합, 생성AI 신기능과 관련된 혁신 및 이로 인한\\n새로운 위험을 탐색\\n∙ 기존 규제의 영향 평가 및 미래 AI 발전에 대비한 영향 평가로 규제 민첩성을 유지하며, 일례로\\n광범위한 도입에 앞서 규제 유예제도(샌드박스)를 시범 운영\\n∙ 지식과 인프라 공유와 AI 안전성 연구, AI 표준의 일관성 확보를 위한 국제협력 추진\\n☞ 출처: World Economic Forum, Governance in the Age of Generative AI: A 360° Approach for Resilient Policy and Regulation, 2024.10.08.\\n5\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 8, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2024-11월호\\nCB인사이츠 분석 결과, 2024년 3분기 벤처 투자 31%가 AI 스타트업에 집중\\nKEY Contents\\nn CB인사이츠에 따르면 2024년 3분기 AI 스타트업은 전체 벤처 투자의 31%를 유치했으며,\\nAI 스타트업의 투자금 회수 시점은 일반 기업보다 6년 빠른 것으로 확인\\nn 그러나 CB인사이츠는 투자자들의 낙관적 기대에도 불구하고 오픈AI와 같은 거대 기업도\\n비용 통제에 어려움을 겪고 있다며 상당수 AI 스타트업이 실패할 것으로 예상\\n£AI 스타트업, 벤처 투자의 최우선 고려 대상으로 부상\\nn 글로벌 리서치 기업 CB인사이츠(CB Insights)가 2024년 10월 3일 발표한 2024년 3분기 벤처\\n현황 보고서에 따르면 2024년 3분기 벤처 자금의 31%가 AI 스타트업에 투자된 것으로 분석\\n∙ AI 스타트업은 2024년 2분기에 전체 벤처 투자의 35%를 유치하며 역대 최고 비중을 차지했으며,\\n3분기에도 역대 두 번째로 높은 비중을 기록\\n∙ 오픈AI의 공동설립자 일리야 수츠케버(Ilya Sutskever)가 2024년 6월 설립한 스타트업 SSI(Safe\\nSuperintelligence Inc.)는 10억 달러를 유치하며 3분기 대표적인 AI 투자로 기록\\n∙ CB인사이츠가 전 세계 1만 5천 개 이상의 AI 스타트업을 추적한 결과, 전 세계 AI 스타트업의 43%가 미국\\n기업이며, 다음 순위는 중국이 9%, 영국이 7%, 인도와 캐나다가 각각 4%로 미국과 상당한 격차를 기록\\nn 기업가치 10억 달러 이상의 유니콘 기업은 2024년 3분기에 24개가 탄생했으며, 이중 절반 이상이\\nAI 기업인 것으로 확인\\n∙ 범용 로봇 개발기업 스킬드AI(Skild AI), 공간지능에 특화된 월드랩스(World Labs), 법률 AI\\n서비스 기업 하비(Harvey) 등이 유니콘 지위를 획득\\nn AI 스타트업은 투자금 회수(Exit) 시점도 일반 스타트업보다 훨씬 빨라 AI 기업이 엑시트하는 시점은\\n설립 후 7년에 불과했으나 여타 스타트업은 13년 소요되었으며, 이러한 경향은 M&A에서 가장 뚜렷해\\n2024년 AI 스타트업 엑시트는 대부분 M&A를 통해 달성\\n∙ 대기업들은 자사 제품군에 AI 도구를 신속히 도입하고자 AI 스타트업 인수에 적극적인 행보를 보이고\\n있으며, 일례로 엔비디아(Nvidia)는 2024년에 AI 스타트업 3곳을 인수했고, 세일즈포스(Salesforce)는\\n2024년 9월 AI 스타트업 2곳을 인수\\nn 그러나 CB인사이츠는 투자자들의 낙관적 기대에도 불구하고 현재의 AI스타트업 중 상당수는 기대에\\n부응하지 못하고 실패하게 될 것으로 예상\\n∙ CB인사이츠는 오픈AI와 같은 거대 AI 기업조차도 수익을 내지 못해 비용을 통제해야 하는 어려움을\\n겪고 있다며, 오픈AI의 2024년 손실 규모가 50억 달러에 달할 것으로 전망\\n☞ 출처 : CB Insights, State of Venture Q3’24 Report, 2024.10.03.\\n6\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 9, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개\\nKEY Contents\\nn 메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는\\n‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획\\nn 메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁\\n동영상 AI 모델보다 더 높은 점수를 기록\\n£메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개\\nn 메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타\\n무비 젠(Meta Movie Gen)’을 공개\\n∙ 메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을\\n반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과\\n같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침\\nn 메타 무비 젠은 △동영상 생성 △개인화 동영상 생성 △동영상 편집 △오디오 생성의 4가지 기능을 지원\\n∙ (동영상 생성) 300억 개 매개변수의 AI 모델을 통해 초당 16프레임의 속도로 1,080p 해상도의 최대\\n16초 길이 동영상 생성을 지원\\n∙ (개인화 동영상 생성) 사용자가 자신이나 타인의 이미지와 텍스트를 입력해 원래 인물의 고유한 특징을\\n반영한 개인화 동영상을 제작 가능\\n∙ (동영상 편집) 특정 요소의 추가나 제거, 변경과 같은 부분적 수정 및 동영상 배경 또는 스타일 변경과\\n같은 광범위한 수정도 지원\\n∙ (오디오 생성) 130억 개 매개변수의 오디오 생성 모델을 통합해 동영상과 텍스트 프롬프트 기반으로\\n최대 45초 길이의 배경음, 음향 효과 등 고품질 오디오를 생성\\n£메타 무비 젠, 인간 선호도 평가에서 오픈AI의 소라 능가\\nn 메타 무비 젠은 인간 선호도 평가에서 런웨이(Runway)의 젠(Gen) 3, 오픈AI의 소라(Sora)를\\n비롯한 경쟁 동영상 생성AI 모델보다 더 높은 점수를 기록\\n∙ 메타 무비 젠과 경쟁 모델에 대하여 세 명의 인간 평가자가 점수를 매겨 비교 후 순승률(Net Win\\nRate)*을 계산한 결과, 메타 무비 젠은 젠 3와 소라, 클링(Kling) 1.5를 모두 능가\\n* 두 모델(A와 B)에 대하여 3명의 인간 평가자가 A 선호 시 +1점, 동점이면 0점, B 선호 시 –1점을 매기는 식으로 계산해\\n승률(-100%~100% 값)을 구하며, 승률이 양수면 A 모델 선호, 음수면 B 모델 선호를 의미\\n<메타 무비 젠과 경쟁 AI 모델의 인간 선호도 평가 승률>\\n☞ 출처: Meta, How Meta Movie Gen could usher in a new AI-enabled era for content creators, 2024.10.04.\\n7\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 10, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2024-11월호\\n메타, 이미지와 텍스트 처리하는 첫 멀티모달 AI 모델 ‘라마 3.2’ 공개\\nKEY Contents\\nn 메타가 이미지와 텍스트를 모두 처리할 수 있는 모델과 모바일 기기에서 실행 가능한 경량\\n모델을 포함하는 라마 3.2 시리즈를 공개\\nn 비전 기능을 갖춘 라마 3.2 90B 모델은 다양한 이미지 인식과 시각적 이해 작업에서\\n앤스로픽의 ‘클로드3-하이쿠’ 및 오픈AI의 ‘GPT-4o-미니’와 대등한 수준의 성능 보유\\n£라마 3.2 90B 모델, 이미지 인식과 시각적 이해에서 GPT-4o-미니와 대등한 성능\\nn 메타가 2024년 9월 25일 ‘라마(Llama)’ 시리즈 최초로 이미지와 텍스트를 모두 처리하는 ‘라마 3.2’를 공개\\n∙ 라마 3.2 시리즈는 이미지를 처리하는 비전(Vision) 기능을 갖춘 매개변수 110억 개(11B)와 900억\\n개(90B)의 모델 및 모바일 기기에 적합한 매개변수 10억 개(1B)와 30억 개(3B)의 경량 모델로 구성\\n∙ 2024년 7월 공개된 라마 3.1과 비교해 라마 3.2는 전반적 성능 향상 외 비전 기능이 추가되어 이미지\\n추론을 지원하며 모바일 기기에서 실행 가능한 경량 모델이 추가되어 접근성을 향상\\nn 라마 3.2 시리즈 중 11B와 90B 모델은 차트와 그래프를 포함한 문서 이해, 이미지 캡션, 이미지\\n안의 물체 식별과 같은 이미지 추론을 지원\\n∙ 라마 3.2는 이미지에서 세부 정보를 추출하고 장면을 이해하여 이미지 캡션으로 사용할 수 있도록\\n내용을 전달하는 문장을 생성 가능\\n∙ 이미지 인식과 시각적 이해 관련 90B 모델의 벤치마크 평가 결과는 앤스로픽(Anthropic)의 ‘클로드\\n3-하이쿠’나 오픈AI의 ‘GPT-4o-미니’와 대등한 수준으로, 일례로 시각적 수학 추론(MathVista)에서\\n57.3점으로 클로드 3-하이쿠(46.4점)와 GPT-4o-미니(56.7점)를 능가\\nn 라마 3.2 시리즈 중 1B와 3B 경량 모델은 12만 8천 개 토큰의 컨텍스트 창을 지원하고 다국어\\n텍스트 생성과 도구 호출 기능을 제공하며, 데이터를 기기 내에 보관하는 온디바이스 앱 개발에 특화\\n∙ 모델 평가 결과, 3B 모델은 지시 이행, 요약, 신속한 재작성 및 도구 사용과 같은 작업에서\\n구글(Google)의 ‘젬마 2 2.6B’ 및 마이크로소프트(Microsoft)의 ‘파이 3.5-미니’보다 성능이 우수\\n∙ 일례로 텍스트 재작성(Open-rewrite eval) 평가에서 3B 모델은 40.1점으로 젬마 2 2.6B(31.2점)\\n및 파이-3.5-미니(34.5점)를 앞섰으며, 텍스트 요약 능력(TLDR9+)에서는 19.0점으로 젬마 2\\n2.6B(13.9점) 및 파이-3.5-미니(12.8점)를 능가\\nn 메타는 라마 3.2 출시와 함께 개발자들이 라마 모델을 더욱 쉽고 효율적으로 사용할 수 있도록\\n지원하는 표준화 인터페이스인 ‘라마 스택(Llama Stack)’도 공개\\n∙ 개발자들은 라마 스택을 통해 온프레미스*, 클라우드, 온디바이스 등 다양한 환경에서 일관적이고\\n간소화된 방식으로 라마 모델을 구축 가능\\n* 기업이 자체 시설에서 보유하고 직접 유지 관리하는 데이터센터\\n☞ 출처: Meta, Llama 3.2: Revolutionizing edge AI and vision with open, customizable models, 2024.09.25.\\n8\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 11, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n앨런AI연구소, 벤치마크 평가에서 GPT-4o 능가하는 성능의 오픈소스 LLM ‘몰모’ 공개\\nKEY Contents\\nn 앨런AI연구소가 공개한 멀티모달 LLM 제품군 몰모는 벤치마크 평가에서 GPT-4o를\\n능가하는 성능의 72B 모델과 전문가혼합 모델, 온디바이스 모델 등 4개 모델로 구성\\nn 몰모-72B 모델은 시각적 이해 능력이 뛰어나며 벤치마크 평가 및 인간 선호도 평가에서 첨단\\n폐쇄형 모델을 능가하는 점수를 기록\\n£몰모-72B 모델, 벤치마크 평가에서 GPT-4o와 제미나이 1.5 Pro 능가\\nn 미국 비영리 연구기관 앨런AI연구소(Allen Institute for AI, 이하 AI2)가 2024년 9월 25일 오픈\\n소스 멀티모달 LLM 제품군 ‘몰모(Molmo)’를 공개\\n∙ 몰모는 가장 규모가 크고 성능이 뛰어난 72B와 데모 모델 7B-D, 개방성이 가장 높은 7B-O, 70억 개의\\n전체 매개변수 중 10억 개만 활성화하는 전문가혼합(MoE) 모델 E-1B의 4개 모델로 구성되며, 이 중\\nE-1B 모델은 온디바이스 실행 가능\\n∙ 몰모는 데이터 규모보다 품질을 중시하는 학습 방식으로 데이터 효율성이 뛰어나 컴퓨팅 자원이 한정된\\n환경에서도 사용 가능한 것이 장점\\n∙ 몰모는 일상 사물과 표지판, 복잡한 차트, 시계, 메뉴판 등 다양한 시각 자료를 이해하고 이미지를\\n구성하는 요소를 정확히 지목할 수 있어, 화면과 현실 세계 간 복잡한 상호작용(예: 비행기 표 예약)이\\n필요한 웹 에이전트나 로봇 개발에도 유리\\n∙ AI2는 몰모의 언어와 시각 훈련 데이터, 미세조정 데이터, 모델 가중치, 소스코드를 모두 공개하고\\n연구와 상업적 목적의 활용을 허용\\nn AI2에 따르면 몰모-72B 모델은 주요 벤치마크와 인간 선호도 평가*에서 첨단 폐쇄형 모델을 능가\\n* 870명의 인간 평가자에게 다양한 이미지와 텍스트 프롬프트 쌍에 대한 모델 간 응답을 비교해 선호도 평가를 요청해 순위를 산정\\n∙ 몰모-72B는 11개 벤치마크 평균 점수 81.2점으로 ‘GPT-4o’(78.5점), ‘제미나이 1.5 Pro’(78.3점),\\n‘클로드-3.5 소네트’(76.7점)를 넘는 최고 점수를 기록했으며 인간 선호도 평가에서는 1077점으로\\nGPT-4o(1079점)에 이어 2위\\n∙ 전문가혼합 모델인 몰모E-1B는 벤치마크 평균 점수에서 68.6점, 인간 선호도 평가는 1,032점으로\\n각각 71.1점과 1,041점을 받은 GPT-4V과 경쟁할 수 있는 수준\\n<몰모 제품군과 GPT-4o/GPT-4V의 벤치마크 평균(左)과 인간 선호도 평가(右) 점수 비교>\\n☞ 출처: Allen Institute for AI, Introducing Molmo, 2024.09.25.\\n9\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 12, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2024-11월호\\n미스트랄AI, 온디바이스용 AI 모델 ‘레 미니스트로’ 공개\\nKEY Contents\\nn 미스트랄AI가 네트워크 연결 없이 온디바이스로 사용할 수 있는 경량 모델 ‘레 미니스트로’를\\n미스트랄 3B와 미스트랄 8B 버전으로 공개\\nn 벤치마크 평가에서 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마 및 라마와\\n비교해 대부분 벤치마크에서 더 높은 평가를 획득\\n£미스트랄 AI, 네트워크 연결이 필요 없는 경량 모델 ‘레 미니스트로’ 출시\\nn 프랑스의 대표 AI 스타트업 미스트랄AI(Mistral AI)가 2024년 10월 16일 네트워크 연결 없이\\n작동하는 온디바이스용 AI 모델 ‘레 미니스트로(Les Ministraux)’를 발표\\n∙ ‘미스트랄 3B’와 ‘미스트랄 8B’ 버전으로 공개된 이 모델은 경량 모델이면서도 영어책 50쪽 분량에\\n해당하는 12만 8천 개 토큰의 컨텍스트 창을 지원\\n∙ 미스트랄AI는 레 미니스트로가 번역과 스마트 어시스턴트, 분석, 자율 로봇 같은 중요 애플리케이션에\\n대하여 네트워크 연결 없이 개인정보보호가 가능한 온디바이스 추론을 원하는 고객 수요에 맞게\\n지연시간이 짧고 효율적인 솔루션을 제공한다고 강조\\n∙ 미스트랄AI는 8B 버전만 연구용으로 다운로드를 허용했으며 향후 두 모델을 클라우드 플랫폼을 통해\\n제공할 계획으로, 사용 비용은 100만 입출력 토큰 당 8B 버전은 10센트, 3B 버전은 4센트로 책정\\n£레 미니스트로, 오픈소스 모델 ‘젬마’ 및 ‘라마’ 대비 대부분 벤치마크에서 우수한 평가\\nn 벤치마크 평가 결과, 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마(Gemma)와 라마\\n(Llama)보다 대부분 벤치마크에서 더 높은 평가를 획득\\n∙ MMLU* 평가에서 미스트랄 3B는 60.9점을 얻어 구글의 ‘젬마 2 2B’(52.4점)와 메타의 ‘라마 3.2\\n3B’(56.2점)를 앞섰고, 미스트랄 8B는 65.0점으로 ‘라마 3.1 8B’(64.7점)와 1년 전 출시된 자체\\n모델 ‘미스트랄 7B’(62.5점)를 능가\\n* 다양한 주제에 대한 모델의 광범위한 지식과 추론 능력을 평가하는 벤치마크\\n∙ 미스트랄 8B는 코딩 능력을 평가하는 HumanEval pass@1*에서만 34.8점으로 ‘라마 3.1\\n8B’(37.8점)보다 소폭 낮은 점수를 기록\\n* AI 모델이 한 번의 시도로 정확한 코드를 생성할 수 있는 능력을 평가하는 벤치마크\\n<미스트랄 3B/7B와 경쟁 모델의 벤치마크 평가 비교>\\n☞ 출처: Mistral AI, Un Ministral, des Ministraux-Introducing the world’s best edge models, 2024.10.16.\\n10\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 13, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n카카오, 통합 AI 브랜드 겸 신규 AI 서비스 ‘카나나’ 공개\\nKEY Contents\\nn 카카오가 대화의 맥락 속에서 주요 정보를 기억해 이용자에게 최적화된 답변을 제공하는 AI\\n메이트 서비스인 ‘카나나’를 공개 했으며 카카오톡과 별개의 앱으로 출시 예정\\nn 카카오는 자체 언어모델로 용량 별로 카나나 플래그, 카나나 에센스, 카나나 나노도 개발\\n중으로, 에센스와 나노를 중심으로 주요 서비스에 적용할 계획\\n£카카오의 신규 AI 서비스 ‘카나나’, 개인메이트 ‘나나’와 그룹메이트 ‘카나’로 구현\\nn 카카오가 2024년 10월 22~24일 열린 개발자 컨퍼런스 ‘if(kakaoAI)2024’에서 그룹 전체의\\nAI 비전과 방향성을 공개하고 통합 AI 브랜드 ‘카나나(Kanana)’를 발표\\n∙ 사명인 카카오와 함께, ‘나에게 배워 나처럼 생각하고 행동한다’는 의미의 네이티브(Native), ‘배우지\\n않아도 자연스럽게 사용 가능한 기술’이라는 의미의 내츄럴(Natural) 등의 단어를 조합한 카나나는\\n‘가장 나다운 AI’를 의미\\n∙ 카카오는 동 브랜드를 자사가 개발하는 주요 AI 모델과 신규 서비스의 이름에 두루 사용할 계획으로,\\nAI 메이트 서비스 ‘카나나’ 출시 계획도 공개\\nn 카나나는 대화의 맥락 속에서 주요 정보를 기억해 이용자에게 최적화된 답변을 제시하는 ‘AI 메이트’를\\n지향하며, 개인메이트 ‘나나(nana)’와 그룹메이트 ‘카나(kana)’로 구현\\n∙ 개인메이트 나나는 이용자와 일대일 대화 및 이용자가 참여한 그룹 대화도 기억해 최적화된 개인화\\n경험을 제공하며, 일례로 그룹대화에서 나눈 컨퍼런스 참석 일정과 준비물을 기억해 이를 잊지 않도록\\n메시지로 전송\\n∙ 카나는 상주하는 그룹대화 안에서의 대화 내용만 기억해 이용자를 지원하며, 가령 스터디 그룹대화에서\\n함께 읽은 논문 관련 퀴즈를 내주고 채점과 부연 설명을 제공\\n∙ 카카오는 카나나를 카카오톡과 별개의 앱으로 출시할 예정으로, 연내 사내 테스트 버전 출시를 통해\\n완성도를 높여갈 계획\\nn 카카오는 자체 생성AI 모델도 연구개발 중으로, 언어모델은 용량에 따라 △카나나 플래그 △카나나\\n에센스 △카나나 나노로 분류되며, 글로벌 수준의 성능을 갖춘 에센스와 나노를 중심으로 카카오의\\n주요 서비스에 적용할 계획\\nn 카카오는 이번 행사에서 내부의 AI 리스크 관리 체계인 ‘Kakao ASI(AI Safety Initiative)’도 강조\\n∙ Kakao ASI는 안전하고 윤리적인 AI 기술 개발 및 운영 시스템을 구축하기 위한 종합 지침으로서,\\n기술의 설계부터 개발, 테스트, 배포, 모니터링, 업데이트 등 AI 시스템의 전 생애주기에서 발생할 수\\n있는 리스크에 선제적 대응 추구\\n☞ 출처: Kakao, 카카오, ‘if(kakaoAI)2024’에서 그룹 AI 비전 공개…AI 메이트 ‘카나나’도 처음 선보여, 2024.10.22.\\n11\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 14, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2024-11월호\\n2024년 노벨 물리학상과 화학상, AI 관련 연구자들이 수상\\nKEY Contents\\nn 2024년 노벨 물리학상은 물리학 원리를 바탕으로 인공 신경망을 이용한 머신러닝의 토대가 되는\\n방법을 개발한 존 홉필드와 제프리 힌턴이 수상\\nn 2024년 노벨 화학상은 단백질 설계에 기여한 데이비드 베이커 및 단백질 구조를 예측하는 AI\\n모델을 개발한 딥마인드의 데미스 허사비스와 존 점퍼가 수상\\n£노벨 물리학상, 인공 신경망 연구한 존 홉필드 교수와 제프리 힌턴 교수가 수상\\nn 스웨덴 왕립과학원 노벨위원회는 2024년 10월 8일 존 홉필드(John Hopfield) 미국 프린스턴⼤\\n교수와 제프리 힌턴(Geoffrey Hinton) 캐나다 토론토⼤ 교수에게 인공 신경망을 이용한 머신러닝의\\n토대가 되는 방법을 개발한 공로로 노벨 물리학상을 수여\\n∙ 홉필드는 물리학의 원리를 이용해 왜곡되거나 불완전한 입력 패턴과 가장 유사하게 저장된 패턴을\\n찾아내고 재구성할 수 있는 초기 인공 신경망 모델인 ‘홉필드 네트워크(Hopfield Network)’를 개발\\n∙ 힌턴은 홉필드 네트워크를 토대로 ‘볼츠만 머신(Boltzmann Machine)’을 고안했으며, 이 모델은\\n통계물리학을 활용해 주어진 데이터에서 특징적 요소를 인식하여 인간의 개입 없이 학습된 패턴\\n유형을 활용해 새로운 예제를 생성 가능\\n∙ 힌턴은 인공 신경망이 데이터를 통해 학습할 수 있다는 개념으로 머신러닝의 폭발적 발전을\\n이끌었으며, 인공 신경망은 현재 신소재 발견을 비롯한 광범위한 물리학 연구에 활용되는 추세\\n£노벨 화학상, 단백질 구조 예측 AI 모델 개발한 딥마인드 연구진 등 3인이 수상\\nn 데이비드 베이커(David Baker) 미국 워싱턴⼤ 교수와 데미스 허사비스(Demis Hassabis) 구글\\n딥마인드 CEO, 존 점퍼(John Jumper) 구글 딥마인드 수석 연구원은 새로운 단백질 생성 및 AI를\\n활용한 단백질 구조 예측에 대한 공로로 2024년 10월 9일 노벨 화학상을 수상\\n∙ 베이커 교수는 90년대 말 단백질 구조를 예측하는 컴퓨터 소프트웨어 ‘로제타(Rosetta)’를 개발*했으며,\\n2003년에는 단백질의 기본 요소인 아미노산을 이용해 기존 단백질과 다른 새로운 단백질을 설계\\n* 로제타폴드를 소개한 2021년 Science 논문에는 로제타폴드의 핵심개발자이자 제1저자인 현 백민경 교수\\n∙ 허사비스와 점퍼는 1970년대부터 난제로 남아있던 단백질 구조 예측에 결정적 기여를 한 AI 모델\\n‘알파폴드(AlphaFold) 2’를 2020년 발표하고 오픈소스로 공개\\n∙ 2억 개에 달하는 단백질 구조를 예측한 알파폴드 2는 과거에는 몇 년이 걸리거나 불가능하던 단백질\\n구조 예측을 몇 분 만에 완료할 수 있으며, 2024년 10월까지 190개국 200만 명 이상에 의해 사용\\n∙ 노벨위원회에 따르면 단백질의 구조 예측과 새로운 단백질의 설계는 특정 질병이나 항생제 내성의\\n발생원인 이해 및 새로운 의약품이나 나노소재 개발 등으로 인류에게 막대한 이익을 가져올 전망\\n☞ 출처: The Nobel Prize, They used physics to find patterns in information, 2024.10.08.\\nThe Nobel Prize, They have revealed proteins’ secrets through computing and artificial intelligence, 2024.10.09.\\n12\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 15, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n미국 국무부, AI 연구에서 국제협력을 위한 ‘글로벌 AI 연구 의제’ 발표\\nKEY Contents\\nn 미국 국무부는 바이든 대통령의 AI 행정명령에 따라 국제협력을 통해 포괄적이고 조정된 AI\\nR&D 접근방식을 제시한 ‘글로벌 AI 연구 의제(GAIRA)’를 발표\\nn 국무부는 GAIRA를 통해 AI R&D 원칙과 안전하고 신뢰할 수 있는 AI 발전을 위한 연구\\n우선순위, 주요 이해관계자별 권장 사항을 제시\\n£국무부, AI 연구 우선순위로 포괄적 연구 인프라 조성과 글로벌 도전과제 해결 등 제시\\nn 미국 국무부(United States Department of State)가 2024년 9월 23일 국제협력을 통해 안전하고\\n신뢰할 수 있는 AI 시스템을 개발하기 위한 R&D 원칙과 우선순위, 권장 사항을 제시한 ‘글로벌\\nAI 연구 의제(Global AI Research Agenda, 이하 GAIRA)’를 발표\\n∙ 국무부는 2023년 10월 30일 바이든 대통령이 서명한 AI 행정명령에 따라 모든 사람에게 이로운\\n방식으로 개발·사용되는 AI R&D에 대한 포괄적이고 조정된 접근방식을 마련하고자 GAIRA를 작성하고,\\nAI 연구에서 3가지 권장 원칙으로 △포용성·형평성 △책임 있는 연구 수행 △파트너십과 협업을 제시\\nn 국무부는 GAIRA를 통해 안전하고 신뢰할 수 있는 AI를 발전시키기 위한 연구 우선순위를 제시\\n∙ (사회 기술 연구) 기술과 사회 간 상호작용에 대한 이해를 심화하고 인간 복지를 향상하는 AI 시스템의\\n설계와 배포에 관한 연구를 수행 우선\\n∙ (포용적 연구 인프라 조성) AI 기술과 시스템의 혁신을 지원하는 데이터와 컴퓨팅 성능, 연구 플랫폼에\\n대한 접근성을 향상해 AI 연구와 개발 생태계의 다양성을 촉진하고 편향을 완화\\n∙ (글로벌 도전과제 해결) 환경 문제, 경제 회복력, 사회복지 등 글로벌 도전 과제 해결에 도움이 되는 AI\\n애플리케이션을 우선 개발\\n∙ (AI 안전과 보안, 신뢰성을 포함한 AI 기초연구) AI는 아직 개발 초기 단계로 안전하고 신뢰할 수 있는\\nAI 시스템 개발을 위해 더 많은 기술 발전 필요\\n∙ (글로벌 노동 시장에서 AI의 영향 연구) AI가 노동 시장에 미치는 여러 측면을 다루는 연구를 수행하고\\n노동 시장에 미치는 AI의 부정적 영향을 완화하기 위한 전략을 수립\\nn 국무부는 GAIRA를 통해 연구 기금 제공자, 연구 생태계 허브, 연구팀과 같은 이해관계자별로 연구\\n의제의 목표 달성을 위한 권장 사항을 제시\\n∙ (연구 기금 제공자) 투명성을 증진하고 국제 AI 연구 협력을 지원하는 기금을 요청하며 다양한 지역에\\n서 연구 인프라 접근성을 증진하고 민관협력을 추진\\n∙ (연구 생태계 허브) 연구 재현성을 장려하고, AI 연구 가이드라인 관련 협력과 조정을 강화하며, 민간\\n분야에서 중시하는 연구 주제 이외의 연구를 지원\\n∙ (연구팀) 다학제적 팀을 우선 편성하고 지역 연구자들과 협력하며, 사회기술적 방법론과 연구 설계를\\n채택하고 위험 평가 절차를 통합\\n☞ 출처: U.S. Department of State, Global AI Research Agenda, 2024.09.23.\\n13\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 16, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2024-11월호\\n일본 AI안전연구소, AI 안전성에 대한 평가 관점 가이드 발간\\nKEY Contents\\nn 일본 AI안전연구소는 AI 개발자나 제공자가 안전성 평가에 참조할 수 있는 ‘AI 안전성에 대한\\n평가 관점 가이드’를 발표\\nn 가이드는 AI 안전성의 핵심 요소를 달성하기 위한 10가지 평가 관점과 함께, 평가를 통해\\n효과적 조치를 취했을 때의 기대 목표를 제시\\n£일본 AI안전연구소, AI 개발자나 제공자의 안전성 평가를 위한 가이드라인 제시\\nn 일본 AI안전연구소(Japan AI Safety Institute)가 2024년 9월 25일 AI 개발자나 제공자가 안전성\\n평가 시에 참조할 수 있는 기본 개념을 제시하는 ‘AI 안전성에 대한 평가 관점 가이드’를 발간\\n∙ 가이드는 AI 안전성의 핵심 요소로 △인간중심 △안전성 △공평성 △프라이버시 보호 △보안 △투명성을\\n제시하고, 이를 달성하기 위한 10가지 평가 관점 및 평가를 통한 효과적 조치 이후의 기대 목표를 수립\\n<AI 안전성의 핵심 요소를 고려한 AI 안전성 평가 관점>\\n평가 관점 관련 AI 안전성 요소 기대 목표\\n유해 정보의 출력 통제 인간중심, 안전성, 공정성 Ÿ LLM 시스템이 테러, 범죄, 불쾌한 표현 등 유해 정보의 출력을 통제 가능\\n허위 정보와 Ÿ LLM 시스템의 출력에 대한 사실 검증 메커니즘 구축\\n인간중심, 안전성, 투명성\\n조작 방지 Ÿ LLM 시스템의 출력에 의한 사용자 결정의 조작 방지\\nŸ LLM 시스템 출력에 유해한 편향이 없으며 개인이나 집단에 대한 불공정한\\n공정성과 포용성 인간중심, 공정성, 투명성 차별 부재\\nŸ LLM 시스템의 출력을 모든 최종 사용자가 이해 가능\\n고위험 사용 및 Ÿ LLM 시스템이 본래 목적과 다르게 부적절하게 사용되어도 피해나 불이익\\n인간중심, 안전성\\n비의도적 사용 대처 미발생\\n개인정보 보호 프라이버시 보호 Ÿ LLM 시스템이 정보의 중요성에 따라 프라이버시를 적절히 보호\\nŸ LLM 시스템의 허가되지 않은 운영 및 비의도적 수정 또는 중단으로 인한\\n보안 보안\\n기밀정보의 유출 방지\\nŸ LLM 시스템 작동에 대한 증거 제시 등을 목적으로 출력의 근거를 기술적\\n설명 가능성 투명성\\n으로 합리적인 범위에서 확인 가능\\nŸ LLM 시스템이 적대적 프롬프트, 왜곡된 데이터 및 잘못된 입력 등 예상치\\n견고성 안전성, 투명성\\n않은 입력에 대해 안정적 출력을 제공\\nŸ LLM 시스템 학습을 위한 데이터가 적절한 상태로 유지되고 데이터 이력이\\n데이터 품질 안전성, 공정성, 투명성\\n적절히 관리되는 상태\\nŸ LLM 시스템에 대한 다양한 유형의 검증이 모델 학습 단계에서 시스템 사용\\n검증 가능성 투명성\\n시점까지 제공되는 상태\\nn AI 안전성 평가는 기본적으로 AI 시스템의 개발자 및 제공자에 의해 실시되며, AI 시스템 개발, 배포,\\n사용 단계에서 적절한 간격으로 시행될 필요\\n∙ AI 안전성 평가 범위는 개발 단계에서는 데이터, 배포와 사용 단계에서는 전체 LLM 시스템 등으로 달라질 수 있으며,\\n평가는 한 차례가 아니라 반복적으로 실시\\n☞ 출처: Japan AI Safety Institute, AIセーフティに関する評価観点ガイドの公開, 2024.09.25.\\n14\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 17, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n구글 딥마인드, 반도체 칩 레이아웃 설계하는 AI 모델 ‘알파칩’ 발표\\nKEY Contents\\nn 구글 딥마인드가 강화학습 방식으로 반도체 칩 레이아웃을 설계하여 사람이 몇 주에서 몇 달이\\n걸리는 수준의 칩 레이아웃을 몇 시간 만에 생성하는 AI 모델 ‘알파칩’을 공개\\nn 구글은 2020년 처음 알파칩에 관한 연구 논문을 발표한 뒤, 자체 AI 칩 TPU 개발 시\\n알파칩을 활용해 칩 성능을 개선하고 개발 주기를 단축\\n£알파칩, 구글의 자체 AI 칩 TPU의 레이아웃 설계에도 기여\\nn 구글 딥마인드가 2024년 9월 26일 반도체 칩의 레이아웃을 설계할 수 있는 AI 모델 ‘알파칩\\n(AlphaChip)’을 공개\\n∙ 2020년 연구 프로젝트로 시작된 알파칩은 강화학습 방식을 사용하여 반도체 칩 레이아웃을 설계하며,\\n사람이 완료하는데 몇 주에서 몇 달이 걸리는 수준의 칩 레이아웃을 몇 시간 만에 생성 가능\\nn 구글은 2020년 알파칩에 대한 연구 논문을 처음 발표했으며, 자체 AI 칩 TPU(Tensor Processing\\nUnit) 개발 시 알파칩을 활용해 칩 레이아웃을 설계\\n∙ TPU는 제미나이(Gemini)뿐 아니라 이마젠(Imagen), 비오(Veo) 등의 이미지 및 동영상 생성 모델과\\n같은 구글 AI 시스템의 핵심 요소를 형성\\n∙ 알파칩은 최신 6세대 TPU를 포함한 새로운 세대마다 칩 레이아웃 설계를 개선해 설계주기를\\n단축하고 더 높은 성능의 칩 생산에 기여\\nn 알파칩은 바둑에 특화된 알파고(AlphaGo) 및 바둑, 체스, 쇼기(일본 장기)를 마스터한 알파제로\\n(AlphaZero)와 비슷하게 칩 레이아웃 설계를 게임처럼 접근\\n∙ 알파칩은 모든 부품을 배치할 때까지 한 번에 하나의 회로 부품을 배치하고 최종 레이아웃의 품질에 따라\\n보상을 받게 되며, 상호 연결된 부품 간 관계를 학습하고 칩 전체로 확장해 레이아웃을 개선\\nn 구글은 자체 AI 칩 TPU뿐 아니라 영국 반도체 기업 ARM과 협력해 개발한 데이터센터용 CPU인\\n액시온(Axion) 프로세서도 알파칩으로 레이아웃을 생성했으며, 타사에도 알파칩을 제공\\n∙ 대만의 반도체 기업 미디어텍(MediaTek)은 삼성 스마트폰에 사용되는 ‘다이멘시티 플래그십(Dimensity\\nFlagship) 5G’와 같은 첨단 칩 개발에 알파칩을 활용해 개발을 가속화하고 칩 성능을 개선\\nn 구글 딥마인드는 현재 알파칩의 차기 버전을 개발 중으로, 향후 알파칩이 칩 설계주기의 전 단계를\\n최적화하고 스마트폰, 의료 장비, 농업 센서 등에 사용되는 맞춤형 하드웨어의 칩 설계에 혁신을\\n가져올 것으로 기대\\n☞ 출처: Google Deepmind, How AlphaChip transformed computer chip design, 2024.09.26.\\n15\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 18, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전트에 트랜스포머 아키텍처의 대안 필요성 강조\\nKEY Contents\\nn 이스라엘 AI 스타트업 AI21의 오리 고센 CEO는 AI 모델 개발에 주로 활용되는 트랜스포머\\n아키텍처가 느린 속도와 과도한 연산 비용으로 인해 AI 에이전트에 부적합하다고 지적\\nn 고센 CEO는 AI 에이전트를 활성화하려면 메모리 사용을 최적화하여 효율적 연산과 비용\\n절감을 지원하는 맘바나 잠바와 같은 대체 아키텍처에 주목해야 한다고 주장\\n£AI 에이전트 활성화를 위해 향상된 메모리 성능을 갖춘 대체 아키텍처 채택 필요\\nn 이스라엘의 AI 스타트업 AI21의 오리 고센(Ori Goshen) CEO가 AI 에이전트를 활성화하려면\\n트랜스포머(Transformer)* 이외의 새로운 아키텍처**가 필요하다고 주장\\n* 문장 속 단어와 같은 순차 데이터 내의 관계를 추적해 맥락과 의미를 학습하는 신경망\\n** AI 시스템이 데이터를 처리하고 학습하기 위한 신경망의 전체적인 구조와 설계 방식을 의미\\n∙ 트랜스포머는 현재 AI 모델 개발에서 가장 많이 사용되는 아키텍처이지만, 다중 에이전트 생태계 조성\\n측면에서는 한계를 내포\\n∙ 트랜스포머 아키텍처는 처리하는 컨텍스트가 길수록 속도가 느리고 연산 비용이 많이 드는데, AI\\n에이전트는 LLM을 여러 차례 호출해야 하고 각 단계에서 광범위한 컨텍스트를 사용하는 경우가 많아\\n처리 과정에서 지연이 발생\\nn 고센 CEO는 ‘맘바(Mamba)’와 ‘잠바(Jamba)’와 같은 대체 아키텍처를 활용하면 AI 에이전트를 더\\n효율적이고 저렴하게 만들 수 있다고 강조\\n∙ 카네기멜론⼤와 프린스턴⼤ 연구진이 개발한 맘바는 트랜스포머 모델의 핵심인 어텐션(Attention)*\\n메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사용을 최적화\\n* 입력된 데이터 간 연관성을 파악해 상호작용을 계산하는 메커니즘\\n∙ 미스트랄이 2024년 7월 ‘코드스트랄(Codestral) 맘바 7B’를, UAE의 AI 기업 팔콘(Falcon)이 8월\\n‘팔콘 맘바 7B’를 출시하는 등, 최근 오픈소스 AI 개발자 사이에서 맘바의 인기가 높아지는 추세\\n∙ AI21 역시 맘바 아키텍처를 토대로 더 빠른 추론 시간과 더 긴 컨텍스트를 지원하는 잠바 아키텍처를\\n활용해 기반모델을 개발\\nn 고센 CEO는 AI 에이전트가 최근 들어서야 부상하고 있으며 대다수 AI 에이전트가 아직 상용화되지 않은\\n이유가 트랜스포머로 구축된 LLM의 한계 때문이라고 지적\\n∙ AI 에이전트가 상용화되려면 데이터 간 연관성을 파악해 확률적으로 가장 그럴듯한 답변을 생성하는\\nLLM의 신뢰성을 높여야 하며, 필요한 수준의 신뢰성 보장을 위해서는 추가적인 요소의 통합이 필요\\n∙ 최근 서비스나우(ServiceNow), 세일즈포스 등 여러 기업이 AI 에이전트나 에이전트 구축을 지원하는\\n플랫폼을 출시하는 추세로, 고센 CEO는 이러한 추세가 적절한 기반모델과 아키텍처를 조합함으로써 더욱\\n확산될 것으로 예상\\n☞ 출처: Venturebeat, AI21 CEO says transformers not right for AI agents due to error perpetuation, 2024.10.11.\\n16\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 19, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\nMIT 산업성과센터, 근로자 관점에서 자동화 기술의 영향 조사\\nKEY Contents\\nn MIT 산업성과센터가 설문조사를 통해 근로자 관점의 자동화 기술의 영향을 조사한 결과,\\n근로자들은 직장 내 안전, 임금, 업무 자율성 등에서 자동화를 긍정적으로 평가\\nn 복잡한 문제 해결이 필요한 작업을 수행하는 근로자 및 자신의 직무에 만족하는 근로자일수록\\n자동화의 영향에 긍정적인 것으로 확인\\n£근로자들, 직장 내 안전, 임금, 업무 자율성 등에서 자동화의 영향에 긍정적\\nn MIT 산업성과센터(IPC)는 2024년 9월 30일 9개국* 9천 명 이상의 근로자에 대한 설문조사를\\n바탕으로 근로자 관점에서 자동화 기술을 평가한 연구 결과를 공개\\n* 독일, 미국, 스페인, 영국, 이탈리아, 일본, 폴란드, 프랑스, 호주\\n∙ 연구진은 설문조사를 통해 업무 환경, 직장에서 사용되는 자동화 기술(로봇 및 AI 등), 업무와 기술에\\n대한 태도, 기술이 업무에 미치는 영향을 조사\\nn 조사 결과, 근로자들 사이에서는 직장 내 안전이나 임금, 업무 자율성 등의 측면에서 자동화가\\n긍정적 영향을 미칠 것이란 응답이 우세\\n∙ 자동화가 직장 내 안전에 미치는 영향에 대하여 응답자 44.9%는 긍정적으로 평가했으며 부정적\\n응답은 12.5%에 불과\\n∙ 자동화가 임금에 미치는 영향은 28.8%가 긍정적, 24.8%는 부정적으로 답했으며, 업무 자율성에\\n미치는 영향은 37.9%는 긍정적, 19.9%가 부정적이라고 응답\\nn 자동화 기술에 대한 근로자들의 인식은 대체로 긍정적으로 나타났으나, 국가 별 차이가 존재하며\\n미국 근로자들이 가장 비관적 태도를 보유\\n∙ 9개국 중 미국에서만 자동화가 임금 및 직업 안정성에 부정적이라는 응답이 긍정적이라는 응답보다\\n우세(임금: –0.6%, 직업 안정성: -4.6%)*\\n* 긍정적 응답에서 부정적 응답 비율을 뺀 수치\\nn 직무 유형에서는 복잡한 문제 해결이나 새로운 아이디어가 필요한 작업을 수행하는 사무직 근로자가\\n자동화에 더 긍정적이며, 직장 내 처우도 자동화에 대한 근로자의 인식에 영향을 발휘\\n∙ 고용주가 근로자를 적절히 대우하고 안전에 투자하는 직장에서 일하는 근로자는 직장 내 자동화의\\n영향에 긍정적이며, 직무 만족도와 신뢰도도 자동화에 대한 긍정적 인식에 영향을 미치는 요인으로 확인\\nn 연구진은 조사 결과를 바탕으로 직장 내 원활한 자동화 기술 도입을 위해 직무 설계를 통해 근로자가\\n복잡한 문제를 해결할 수 있는 역할을 만들 것을 권고\\n∙ 근로자들은 신기술 사용과 관련된 보너스가 제공되면 자동화에 더 긍정적인 것으로 나타나, 생산성\\n향상을 위한 자동화 기술 사용에 금전적 보상을 제공하는 방안도 고려 필요\\n☞ 출처: MIT IPC, Automation from the Worker’s Perspective, 2024.09.30.\\n17\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 20, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2024-11월호\\n다이스 조사, AI 전문가의 73%는 2025년 중 이직 고려\\nKEY Contents\\nn 다이스에 따르면, AI 전문가의 73%는 2025년 이직을 계획 중이며, 58%는 2024년 중 현재보다\\n더 나은 일자리를 찾을 자신이 있다고 응답해 여타 기술 전문가 대비 직업 전망을 낙관\\nn AI 전문가들은 여타 기술 전문가 대비 AI 도구 사용에도 적극적이며, 업무에 생성AI가 상당한\\n영향을 미친다는 응답도 36%로 여타 기술 전문가(22%) 대비 높은 수치를 기록\\n£AI 전문가들, 일반적인 기술 전문가보다 직업 전망에 낙관적\\nn 미국 기술직 채용 플랫폼 다이스(Dice)의 조사에 따르면, AI 기술 전문가는 일반적인 기술 전문가\\n대비 기술 산업의 미래와 자기 경력에 대하여 낙관적\\n∙ 이번 조사는 520명의 미국 정규직 기술 전문가와 390명의 인사 전문가의 응답을 토대로 기술 분야의\\n일자리 시장 환경을 분석\\n∙ 2024년 동안 주요 빅테크가 기술직에 대한 정리해고를 단행하고 기술직 채용도 2021~2022년 대비\\n대폭 감소하는 등 일자리 시장의 침체에도 2024년 기술과 사업의 핵심 요소로 부상한 AI 분야의\\n전문가들은 직업 전망을 낙관\\nn AI 전문가의 73%는 2025년에 이직을 계획 중이며, 58%는 2024년 중 현재보다 더 나은 새로운 일자리를\\n찾을 자신이 있다고 응답\\n∙ 일반적인 기술 전문가의 경우 65%가 2025년 중 이직을 계획 중이며, 2024년 더 나은 신규 일자리를\\n찾을 수 있다고 자신하는 비율은 36%에 불과\\n∙ AI 전문가는 빅테크를 선호하는 비율이 29%로 일반적인 기술 전문가(18%) 대비 더 높게 나타났으며,\\n이는 예산 규모가 더 크고 중요한 AI 프로젝트에 관심이 있거나 빅테크의 채용 가능성에 자신 있기\\n때문으로 추측\\nn 그러나 AI 전문가들은 기업에서 자신이 맡은 업무에 대하여 엇갈린 감정을 표시했으며, 자신의 업무가\\n가치 있다고 느끼는 전문가일수록 현재 역할에 만족할 가능성도 증대할 것으로 추론\\n∙ AI 전문가의 51%는 자신의 프로젝트가 기업에 전략적 가치가 있다고 답했으나, 36%는 투자자나\\n이사회, 외부 관계자에게 기업이 AI로 뭔가를 하고 있음을 보여주기 위한 목적이라고 응답\\nn AI 전문가들은 AI 도구 사용에도 적극적이지만, 일반적인 기술 전문가들은 업무에서 AI 도구 사용을\\n주저하는 편으로, AI 전문가들은 일주일에 1회 이상 AI를 사용하는 비율이 49%에 달했으나, 여타\\n기술 전문가들은 25%에 불과\\nn 생성AI가 미치는 영향에 대해서 AI 전문가 사이에서는 상당한 영향을 미친다는 응답이 36%, 약간의 영향을\\n미친다는 응답이 56%, 영향이 없다는 응답은 8%를 기록했으나, 여타 기술 전문가들은 22%가 상당한 영향,\\n53%는 약간의 영향, 26%는 영향이 없다고 응답\\n☞ 출처: Dice, 3 Key Lessons about the AI Tech Talent Market, 2024.09.05.\\n18\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 21, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n가트너 예측, AI로 인해 엔지니어링 인력의 80%가 역량 향상 필요\\nKEY Contents\\nn 가트너에 따르면 생성AI의 도입으로 중장기적으로 소프트웨어 엔지니어링에서 데이터 과학 및\\nAI/ML 역량의 중요성이 커지면서 AI 엔지니어의 수요가 늘어날 전망\\nn 기업들은 AI 엔지니어를 지원하고 기업 내 AI 통합을 촉진하기 위해 AI 개발자 플랫폼에 대한\\n투자를 강화할 필요\\n£생성AI로 소프트웨어 엔지니어링에서 데이터과학과 AI/ML 역량의 중요성 증대\\nn 시장조사기관 가트너(Gartner)에 따르면 2027년까지 생성AI로 인해 소프트웨어 엔지니어링\\n인력의 80%가 역량 향상이 필요할 전망\\n∙ AI로 인해 인간 엔지니어에 대한 수요가 감소하거나 심지어 AI가 인간을 대체할 것이라는 예상과\\n달리, 가트너는 AI가 향후 소프트웨어 엔지니어의 역할을 변화시키더라도 인간의 전문성과 창의성은\\n여전히 중요하다고 강조\\nn 가트너에 따르면 생성AI는 소프트웨어 엔지니어의 역할에 단기, 중기, 장기적으로 영향을 미칠 전망\\n∙ 단기적으로는 AI가 기존 개발자의 작업 패턴과 업무를 보완하며 소폭의 생산성 향상 효과를 가져오며,\\nAI의 생산성 향상 효과는 성숙한 엔지니어링 관행을 갖춘 기업의 상급 개발자에게 집중될 전망\\n∙ 중기적으로는 AI 에이전트를 통해 더 많은 업무가 자동화되어 개발자의 작업 패턴의 변화가 예상되며,\\n이는 코드 대부분이 인간이 아닌 AI로 생성되는 AI 네이티브 소프트웨어 엔지니어링의 출현을 의미해\\n자연어 프롬프트 엔지니어링과 검색 증강 생성(RAG)* 기술이 엔지니어링의 필수 역량이 될 전망\\n* 외부 데이터를 활용하여 LLM의 출력 정확성을 향상하는 기술\\n∙ 장기적으로는 기업 내 AI 기반 소프트웨어 수요가 증가하면서 이를 충족하기 위해 소프트웨어\\n엔지니어링, 데이터 과학, AI/ML(머신러닝) 분야의 고유한 기술을 갖춘 훨씬 숙련된 AI 엔지니어가\\n부상할 전망\\n£AI 엔지니어를 지원하기 위해 기업의 AI 개발자 플랫폼 투자 필요\\nn 가트너가 2023년 4분기에 미국과 영국 기업 300개를 대상으로 실시한 설문조사에 따르면 소프트웨어\\n엔지니어링 책임자의 56%가 AI/ML 엔지니어를 2024년 가장 수요가 많은 직업으로 평가\\n∙ 기업들은 AI 엔지니어를 지원하기 위해 AI 개발자 플랫폼에 투자해야 하며, AI 개발자 플랫폼은\\n기업이 AI 역량을 더욱 효율적으로 구축하고 AI를 기업 솔루션에 대규모로 통합하는 데 도움이 될 전망\\n∙ 기업들은 AI 개발자 플랫폼 투자를 통해 소프트웨어 엔지니어링팀의 역량을 강화하고 지속적인 AI\\n통합과 개발을 추진하는 도구와 프로세스를 채택 필요\\n☞ 출처: Gartner, Gartner Says Generative AI will Require 80% of Engineering Workforce to Upskill Through 2027,\\n2024.10.03.\\n19\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 22, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='SPRi AI Brief |\\n2024-11월호\\n인디드 조사 결과, 생성AI가 인간 근로자 대체할 가능성은 희박\\nKEY Contents\\nn 인디드가 2,800개 이상의 직무 기술에 대한 생성AI의 수행 능력을 분석해 인간을 대체할\\n가능성을 평가한 결과, 생성AI로 대체될 가능성이 “매우 높은” 것으로 평가된 기술은 전무\\nn 생성AI의 최대 강점은 직무 기술과 관련된 이론적 지식을 제공하는 능력이며, 물리적 작업\\n수행이 필요한 직무 기술에서는 인간 근로자를 대체할 가능성이 희박\\n£생성AI, 문제 해결 역량 및 물리적 작업 수행 역량의 부족으로 인간 근로자 대체에 한계\\nn 미국의 채용 플랫폼 인디드(Indeed) 산하 연구소 하이어링랩(Hiring Lab)이 2024년 9월 25일\\n발표한 연구 결과에 따르면 생성AI가 인간 근로자를 대체할 가능성은 희박\\n∙ 인디드 하이어링랩은 오픈AI의 GPT-4o로 2,800개 이상의 고유한 직무 기술에 대한 생성AI의 수행\\n능력을 분석해 생성AI가 인간을 대체할 가능성을 평가\\n∙ 연구진은 오픈AI의 GPT-4o가 △기술과 관련된 이론적 지식의 제공 역량 △기술을 사용한 문제 해결 역량\\n△기술 활용 시 물리적 작업의 중요성에 관한 판단 능력의 3개 차원에서 자체 수행 능력을 평가하도록 진행\\n∙ 다섯 가지 선택지(매우 낮음, 낮음, 보통, 높음, 매우 높음)로 평가 결과, 인디드가 평가 대상으로 삼은\\n2,800개 이상의 직무 기술 중 68.7%는 생성AI로 대체될 가능성이 “매우 낮음” 또는 “낮음”으로\\n평가됐으며, “매우 높음”으로 평가된 기술은 전무\\nn 생성AI는 직무 기술의 이론적 지식을 제공하는 자체 능력을 다소 높게 평가했으나, 문제 해결\\n능력 및 물리적 작업의 중요성에 관한 판단 능력은 상대적으로 낮게 평가\\n∙ 생성AI는 직무 기술 중 79.7%에 이론적 지식의 제공 능력을 4점(높음)으로, 기술 중 70.7%에 문제\\n해결 역량을 3점(보통)으로 평가했으며, 기술 중 54%에 대하여 물리적 작업의 필요성이 “높음” 또는\\n“매우 높음”이라고 평가*\\n* 매우 낮음(very unlikely 1점), 낮음(unlikely, 2점), 보통(possible, 3점), 높음(likely, 4점), 매우 높음(very likely, 5점)\\n∙ 생성AI는 물리적 작업을 수행할 몸체가 없어 실제 작업 수행이 필요한 직무 기술에서는 인간 근로자를\\n대체할 가능성이 제한적\\n∙ 일례로 생성AI는 디지털 기술 비중이 큰 소프트웨어 개발 직종의 구인 공고에서 통상 제시되는 직무\\n기술의 71%에 대하여 인간을 대체할 가능성이 “보통” 또는 “높음”으로 평가했으나, 간호사 직종의\\n구인 공고에 제시되는 기술의 약 32.9%만 생성AI로 대체될 가능성이 “보통” 또는 “높음”으로 평가\\nn 인디드는 현재 생성AI의 최대 강점은 직무 기술과 관련된 이론적 지식을 제공하는 능력이라고 강조\\n∙ 생성AI는 직원 생산성을 극대화하여 노동 시장의 경색을 완화할 수 있으며, 물리적 작업 수행이 필요한\\n직업에서도 근로자가 핵심 업무에 집중할 수 있도록 지원 가능\\n∙ 그러나 생성AI는 논리적 오류나 사실과 다른 내용 또는 편향이나 차별과 같은 비윤리적 응답을 출력할\\n가능성도 있으므로 인간의 신중한 검토 필요\\n☞ 출처: Indeed Hiring Lab, AI at Work: Why GenAI Is More Likely To Support Workers Than Replace Them, 2024.09.25.\\n20\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 23, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4'}, page_content='Ⅱ\\n. 주요 행사 일정\\n행사명 행사 주요 개요\\n- 신경정보처리시스템재단은 인공지능과 머신러닝 분야의 연구 성과\\n교환을 촉진하는 것을 목적으로 하는 비영리 법인으로 매년 학제간\\n학술대회(NeurIPS)를 주최\\n- 이번 제38회 연례학술대회는 AI 연구자를 위한 실험 설계,\\nNeurIPS\\nLLM을 위한 메타 생성 알고리즘, 정렬에 대한 학제 간 통찰력\\n2024\\n등을 다룰 예정\\n기간 장소 홈페이지\\n2024.12.10~15 캐나다 밴쿠버 https://neurips.cc/\\n- GenAI Summit Maroc 2024는 인공지능과 데이터 분석에\\n초점을 맞춘 최고의 이벤트로, 250명 이상의 업계 리더, 정책\\nGenAI\\n입안자, 전문가가 모여 AI 발전을 탐구\\nSummit\\n- 이번 행사에는 오픈소스 AI, AI 주도 사이버 보안, 우수한\\nMaroc\\n의사결정을 위한 생성AI와 예측 AI 결합 등을 다룰 예정\\n2024\\n기간 장소 홈페이지\\n2024.12.10~11 모로코 https://genaimaroc.com/\\n- AI Summit Seoul 행사는 2018년 개최를 시작으로 금년도는\\n7회 행사로 개최\\n- 이번 행사는 AI와 산업의 융합에 초점을 두고 다양한 글로벌\\n기업과 기관, 학계 전문가 등 전문가들이 한자리에 모여 AI\\nAI Summit\\n및 산업 트렌드 등에 대한 주제 발표 및 워크샵 진행\\nSeoul 2024\\n기간 장소 홈페이지\\n2024.12.10~11 서울(코엑스 그랜드볼룸) https://aisummit.co.kr/\\n21\\n')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc.metadata['filename'] = doc.metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 3, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n미국 민권위원회, 연방정부의 얼굴인식 기술 사용에 따른 민권 영향 분석\\nKEY Contents\\nn 미국 민권위원회에 따르면 연방정부와 법 집행기관에서 얼굴인식 기술이 빠르게 도입되고\\n있으나 이를 관리할 지침과 감독의 부재로 민권 문제를 초래할 위험 존재\\nn 미국 민권위원회는 연방정부의 책임 있는 얼굴인식 기술 사용을 위해 운영 프로토콜 개발과\\n실제 사용 상황의 얼굴인식 기술 평가 및 불평등 완화, 지역사회의 의견 수렴 등을 권고\\n£연방정부의 얼굴인식 기술 도입에 대한 지침과 감독 부재로 민권 문제를 초래할 위험 존재\\nn 미국 민권위원회(U.S. Commission on Civil Rights)가 2024년 9월 19일 연방정부의 얼굴인식\\n기술 사용이 민권에 미치는 영향을 분석한 보고서를 발간\\n∙ AI 기술의 일종인 얼굴인식 기술은 연방정부와 법 집행기관에서 빠르게 도입되고 있으며, 일례로\\n법무부 연방수사국(FBI)은 범죄 수사 및 용의자 수색용 단서 확보를 위해 얼굴인식 기술을 가장 빈번히 사용\\n∙ 그러나 얼굴인식 기술의 책임 있는 사용을 위한 연방 지침과 감독은 실제 활용 사례보다 뒤처졌으며,\\n현재 연방정부의 얼굴인식 기술이나 여타 AI 기술 사용을 명시적으로 규제하는 법률도 부재\\nn 보고서에 따르면 얼굴인식 기술의 무분별한 사용은 편향, 개인정보 침해, 적법 절차의 미준수\\n및 차별적 영향과 같은 민권 문제를 초래할 위험 보유\\n∙ 얼굴인식 기술의 정확도는 인종, 성별, 연령 등 인구통계학적 요인에 따라 달라질 수 있으며, 이는 식별\\n오류 및 부정확한 체포로 이어져 유색인종을 비롯한 특정 집단에 차별적 결과를 초래할 위험 존재\\n∙ 정부 기관이 사전 영장이나 정당한 이유 없이 얼굴인식 기술을 광범위하게 사용할 경우 개인을\\n지속적으로 추적하고 감시함으로써 개인정보 보호 권리에 심각한 영향을 미칠 위험 존재\\n∙ 법 집행기관의 얼굴인식 기술 사용 시 부정확한 식별 및 편향으로 인해 개인이 법의 보호를 받아\\n공정하고 올바르게 대우받을 권리를 침해할 가능성도 존재\\n£민권위원회, 연방정부의 책임 있는 얼굴인식 기술 사용을 위한 권고사항 제시\\nn 민권위원회는 연방정부의 얼굴인식 기술 사용과 관련해 다음과 같은 권고사항을 제시\\n∙ 국립표준기술연구소(NIST)는 정부 기관의 얼굴인식 기술 시스템 도입 시의 효과와 공평성, 정확성\\n평가에 사용할 수 있는 운영 테스트 프로토콜의 개발 필요\\n∙ 각 연방정부 기관의 최고AI책임자는 실제 사용 상황에서 얼굴인식 기술을 평가하고 차별이나 편견으로\\n인한 불평등을 완화하며, 얼굴인식 기술의 사용으로 영향을 받는 지역사회의 의견을 수렴 필요\\n∙ 얼굴인식 기술 제공업체는 다양한 인구통계 집단에 대한 높은 정확도를 보장하기 위해 지속적인 교육과\\n지원, 업데이트를 제공 필요\\n☞ 출처: U.S. Commission on Civil Rights, The Civil Rights Implications of the Federal Use of Facial Recognition Technology, 2024.09.19.\\n1\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 4, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='SPRi AI Brief |\\n2024-11월호\\n미국 백악관 예산관리국, 정부의 책임 있는 AI 조달을 위한 지침 발표\\nKEY Contents\\nn 미국 백악관 예산관리국이 바이든 대통령의 AI 행정명령에 따라 연방정부의 책임 있는 AI 조달을\\n지원하기 위한 지침을 발표\\nn 지침은 정부 기관의 AI 조달 시 AI의 위험과 성과를 관리할 수 있는 모범 관행의 수립 및 최상의 AI\\n솔루션을 사용하기 위한 공급업체 시장의 경쟁 보장, 정부 기관 간 협업을 요구\\n£백악관 예산관리국, 연방정부의 AI 조달 시 책임성을 증진하기 위한 모범 관행 제시\\nn 미국 백악관 예산관리국(OMB)이 바이든 대통령의 AI 행정명령에 따른 후속 조치로 2024년 10월 3일\\n‘정부의 책임 있는 AI 조달 지침(M-24-18)’을 발표\\n∙ 미국 연방정부는 2023년 1,000억 달러 이상의 IT 제품과 서비스를 구매한 미국 경제 최대 규모의 단일\\n구매자로서 구매력을 활용해 책임 있는 AI의 발전을 뒷받침할 계획\\n∙ 이번 지침은 △AI 위험과 성과 관리 △AI 시장의 경쟁 촉진 △연방정부 전반의 협업 보장이라는 3개\\n전략적 목표에 대하여 권고사항을 제시\\nn (AI 위험과 성과 관리) 예산관리국의 지침은 AI 시스템의 구축, 훈련, 배포 방식의 복잡성을 고려해\\nAI의 위험과 성과를 관리하기 위한 모범 관행을 다음과 같이 제시\\n∙ 정부 기관의 개인정보 보호 담당자가 AI 조달 프로세스에 조기에 지속적으로 참여해 개인정보 보호\\n위험을 식별 및 관리하고 법률과 정책 준수를 보장\\n∙ 정부 기관과 공급업체와 간 협력으로 AI 솔루션이 조달되는 시기와 해당 조달로 인해 시민 권리와\\n안전에 영향을 미치는 AI에 대하여 추가로 위험관리가 필요한 시점을 파악\\n∙ 성과 기반의 혁신적 조달 기법을 활용해 정부 기관이 위험을 효과적으로 관리 및 완화하고 성과를 향상할\\n수 있도록 장려하는 한편, 정부 데이터와 지식재산권을 보호하는 방식으로 계약 조건을 협상\\nn (AI 시장의 경쟁 촉진) 지침은 정부 기관이 최상의 AI 솔루션을 사용할 수 있도록 공급업체 시장에서\\n강력한 경쟁을 보장할 것을 요구\\n∙ 계약 요건 수립 시 공급업체 의존성을 최소화할 수 있는 인수 원칙을 적용하고, 시장 조사와 요구사항\\n개발, 공급업체 평가 절차에서 상호운용성과 투명성을 고려하며, 혁신적 조달 관행을 활용해 우수한\\n계약업체 성과와 정부 기관의 임무 성과를 보장\\nn (연방정부 전반의 협업 보장) 빠르게 발전하는 AI 기술환경의 위험관리를 위해 AI 전문지식을 갖춘\\n공무원과 조달, 개인정보보호, 사이버보안 전문가를 포함하는 협업 팀을 구성해 전략적 조달을 지원\\n∙ 각 정부 기관은 기관 간 협의회를 구성해 효과적이고 책임 있는 AI 조달을 지원하고, 협업 시 기관 목표에\\n가장 적합한 AI 투자 식별 및 우선순위 지정, AI 배포 역량 개발, AI 모범 활용 사례 채택 증진 등을 고려\\n☞ 출처: The White House, FACT SHEET: OMB Issues Guidance to Advance the Responsible Acquisition of AI in\\nGovernment, 2024.10.03.\\n2\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 5, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간\\nKEY Contents\\nn 유로폴의 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오\\n분석 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\nn 그러나 AI 도입을 위해서는 기술적 과제 해결 및 다양한 윤리적·사회적 이슈 대응이\\n필요하며, EU AI 법에 부합하도록 기존 AI 시스템에 대한 평가와 수정도 필요\\n£유로폴, 법 집행에서 AI 기술의 윤리적이고 투명한 구현을 위한 고려사항 제시\\nn EU 사법기관 유로폴(Europol)이 2024년 9월 24일 법 집행에서 효과적 범죄 퇴치를 위한 AI의\\n활용 가능성을 탐색한 보고서를 발간\\n∙ 보고서는 법 집행에서 AI 기술을 윤리적이고 투명하게 구현하기 위한 지침 역할을 하며, AI의 이점과\\n과제를 함께 다룸으로써 법 집행에서 AI 사용 시 윤리적 고려 사항에 대한 인식 제고를 추구\\nn 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오 분석, 생체인식\\n시스템 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\n∙ 법 집행기관은 AI 기반 데이터 분석을 활용해 범죄 활동에 대한 탐지와 대응 능력을 강화하고, AI\\n도구로 구조화되지 않은 데이터를 신속히 분석해 비상 상황의 의사결정을 위한 통찰력 확보 가능\\n∙ 기계번역과 같은 AI 기반 도구는 여러 국가가 참여하는 조사에서 원활한 국제협력을 위해서도 필수적\\nn 그러나 법 집행에서 AI 도구의 효과적이고 책임 있는 활용을 위해 해결되어야 할 기술적 과제 및\\n다양한 윤리적·사회적 우려도 존재\\n∙ 일례로 관할권 간 데이터 수집과 보관 관행의 차이에 따른 데이터셋의 편향으로 인해 AI 산출물의\\n무결성(無缺性)이 손상될 수 있어 표준화된 데이터 수집 규약 필요\\n∙ 데이터 규모나 활용 사례의 복잡성과 관계없이 AI 도구를 효과적으로 사용하려면 다양한 데이터\\n규모와 운영 요구사항에 적응할 수 있는 확장성과 성능을 갖춘 AI 모델도 개발 필요\\n∙ 편향, 개인정보 침해와 인권 침해와 같은 다양한 윤리적·사회적 우려도 존재하며, 이를 해소하기\\n위해 데이터 편향을 제거하고 공공 안전과 개인정보 간 균형을 유지하며 AI 의사 결정 과정에\\n대한 투명성과 책임성을 보장 필요\\nn 보고서는 2024년 8월 발효된 EU AI 법이 법 집행기관에 미칠 영향도 분석\\n∙ EU AI 법은 공공장소에서 실시간 생체인식 식별과 같은 특정 애플리케이션의 사용을 금지하고\\n고위험 AI 시스템에 엄격한 감독을 부과하였으나 법 집행 활동의 특수성을 고려해 일부 예외를 설정\\n∙ 그러나 일부 예외에도 법 집행 역량 강화를 위한 AI 사용을 위해서는 기존에 도입한 AI\\n시스템에 대한 재평가와 수정이 필요한 만큼, 재정과 인력 측면의 상당한 부담 예상\\n☞ 출처: Europol, AI and policing-The benefits and challenges of artificial intelligence for law enforcement, 2024.09.24.\\n3\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 6, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='SPRi AI Brief |\\n2024-11월호\\nOECD, 공공 부문의 AI 도입을 위한 G7 툴킷 발표\\nKEY Contents\\nn OECD는 공공 부문에서 EU 및 G7 국가들의 AI 도입 모범사례와 거버넌스 프레임워크,\\n정책 옵션을 토대로 공공 부문의 AI 도입을 안내하는 보고서를 발표\\nn 보고서는 공공 부문의 AI 도입 시 프로토타입부터 시작해 시범 도입을 거쳐 본격적으로\\n구현하는 단계별 접근방식을 권고\\n£OECD, G7의 사례를 토대로 공공 부문의 AI 도입을 안내하는 지침 마련\\nn OECD가 2024년 10월 15일 안전하고 신뢰할 수 있는 AI의 원칙을 실행 가능한 정책으로 전환할\\n수 있도록 지원하는 ‘공공 부문의 AI를 위한 G7 툴킷’ 보고서를 발간\\n∙ OECD는 G7 회원국이 작성한 설문 응답 및 OECD와 UNESCO의 연구를 토대로 공공 부문에서 AI\\n활용 모범사례와 거버넌스 프레임워크, 정책 옵션과 관련된 종합적 지침 제공을 목표로 보고서를 작성\\nn G7과 EU의 AI 도입 추세를 분석한 결과, G7 회원국과 EU는 공공 부문의 AI 도입과 관련된\\n국가 전략 및 정책의 개발과 구현에서 차이가 존재\\n∙ EU·독일·미국·영국·일본은 국가 AI 전략에 공공 부문을 포함했고 프랑스는 국가 AI 전략에서는\\n공공 부문을 구체적으로 다루지 않으나 공공행정 혁신기금(FTAP)을 조성하여 60개 이상의 AI\\n프로젝트에 투자하는 등 별도의 정책을 수립\\n∙ 캐나다는 2025년 봄까지 공공 서비스를 위한 AI 전략을 개발할 계획이며, 이탈리아는 ‘공공\\n부문 디지털화를 위한 3개년 계획(2024~2026)’에 AI를 포함\\n∙ G7 회원국들은 접근방식의 차이에도 인재와 기술 개발, 조달 정책, 협력관계 구축, 윤리적이고\\n신뢰할 수 있으며 인간 중심적인 AI 관행 조성, 데이터 품질 보장 등에서 공통점을 보유\\nn AI 거버넌스 프레임워크 측면에서 G7 회원국 중 미국·캐나다·프랑스와 EU는 여러 기관이 AI를 관리하는\\n분산형 거버넌스 구조를 채택했으며 이탈리아·독일·영국은 단일 기관이 AI를 관리하는 중앙집중형\\n거버넌스를 채택\\nn G7 회원국들은 공공 부문의 운영 효율성 향상, 정책 결정 강화, 공공 서비스 개선, 정부의 투명성과\\n책임성 강화를 위해 AI를 활용하는 한편, 다양한 정책 옵션으로 AI 도입 시의 과제 해결을 모색\\n∙ AI 도입에 필수적인 인프라를 강화하기 위한 데이터 저장과 공유 솔루션 채택, AI에 적합한\\n혁신적이고 유연한 조달 절차의 수립 및 민간 파트너십 육성, 공공 부문의 AI 역량 강화, 데이터\\n거버넌스 프레임워크 구축 등이 대표적인 정책 옵션\\nn 보고서는 공공 부문의 AI 도입 시 각 단계를 신중히 관리하여 위험을 완화할 수 있도록, 문제를\\n명확히 정의하고 아이디어를 구상한 뒤 프로토타입부터 시작해 통제된 환경에서 AI를 시범 도입한\\n후 이를 개선해 본격적으로 구현하는 단계적 접근방식을 강조\\n☞ 출처: OECD, G7 Toolkit for Artificial Intelligence in the Public Sector, 2024.10.15.\\n4\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 7, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n세계경제포럼, 생성AI 시대의 거버넌스 프레임워크 제시\\nKEY Contents\\nn 세계경제포럼이 글로벌 정책입안자를 대상으로 생성AI의 공익적 활용과 경제·사회적\\n균형 달성, 위험 완화를 위한 거버넌스 프레임워크를 제안하는 백서를 발표\\nn 백서에 따르면 정부는 기존 규제를 평가해 생성AI로 인한 규제 격차를 해소하는 한편, 다양한\\n이해관계자 간 지식 공유를 촉진하고 미래의 AI 발전에 대비한 규제 민첩성을 갖출 필요\\n£생성AI 거버넌스, 과거-현재-미래를 아우르는 프레임워크 수립 필요\\nn 세계경제포럼(WEF)이 2024년 10월 8일 세계 각국의 정책입안자를 대상으로 생성AI 거버넌스\\n프레임워크를 제시한 백서를 발간\\n∙ 백서는 생성AI의 공익적 활용과 경제·사회적 균형 달성, 위험 완화라는 목표 달성을 위해 △과거\\n활용(Harness Past) △현재 구축(Build Present) △미래 계획(Plan Future)의 프레임워크를 제안\\nn (과거 활용) 기존 규제를 활용하고 생성AI로 인한 규제 격차를 해소하는 것으로, 정부는 새로운 AI\\n규제나 관할 당국을 수립하기에 앞서 다음 사항을 추진할 필요\\n∙ 생성AI로 인한 문제나 격차 발생에 관하여 기존 규제를 평가하고 다양한 규제 수단의 정책 목표를\\n고려해 규제를 조정하며, 규제 선례를 참고해 책임 할당을 명확히 하고 격차가 발견된 부분을 보완\\n∙ 기존 규제 당국이 생성AI 문제를 해결할 역량이 있는지 평가하고, AI 전담 기관을 설치하여 규제\\n권한을 집중하는 방안의 장단점을 고려\\nn (현재 구축) 사회 전반의 생성AI 거버넌스와 지식 공유의 증진을 의미하며, 생성AI의 거버넌스에는\\n정책입안자와 규제 당국 외에 산업계, 시민사회, 학계를 포함한 이해관계자 참여가 필수적\\n∙ 정부는 다양한 거버넌스 수단을 활용해 사회 전반의 생성AI 거버넌스에 참여하는 각 이해관계자\\n집단의 고유한 문제에 대응 필요\\n∙ 다양한 이해관계자 간 지식 공유를 촉진하고, 책임 있는 AI 관행으로 사회에 모범을 보일 필요성 존재\\nn (미래 계획) 생성AI 거버넌스에 대한 민첩한 준비와 함께 국제협력을 촉진하는 것으로, 정부는 빠\\n른 기술 발전과 한정된 자원, 글로벌 불확실성을 고려해 미래를 예견한 국가 전략을 개발하고 다음\\n의 활동을 추진\\n∙ 정부 내 AI 역량 향상과 AI 전문가 채용을 위한 투자를 시행하고 AI 전담 기관의 설립 필요성을 신중히 검토\\n∙ 생성AI와 인간 간 상호작용, 생성AI와 여타 기술의 융합, 생성AI 신기능과 관련된 혁신 및 이로 인한\\n새로운 위험을 탐색\\n∙ 기존 규제의 영향 평가 및 미래 AI 발전에 대비한 영향 평가로 규제 민첩성을 유지하며, 일례로\\n광범위한 도입에 앞서 규제 유예제도(샌드박스)를 시범 운영\\n∙ 지식과 인프라 공유와 AI 안전성 연구, AI 표준의 일관성 확보를 위한 국제협력 추진\\n☞ 출처: World Economic Forum, Governance in the Age of Generative AI: A 360° Approach for Resilient Policy and Regulation, 2024.10.08.\\n5\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 8, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='SPRi AI Brief |\\n2024-11월호\\nCB인사이츠 분석 결과, 2024년 3분기 벤처 투자 31%가 AI 스타트업에 집중\\nKEY Contents\\nn CB인사이츠에 따르면 2024년 3분기 AI 스타트업은 전체 벤처 투자의 31%를 유치했으며,\\nAI 스타트업의 투자금 회수 시점은 일반 기업보다 6년 빠른 것으로 확인\\nn 그러나 CB인사이츠는 투자자들의 낙관적 기대에도 불구하고 오픈AI와 같은 거대 기업도\\n비용 통제에 어려움을 겪고 있다며 상당수 AI 스타트업이 실패할 것으로 예상\\n£AI 스타트업, 벤처 투자의 최우선 고려 대상으로 부상\\nn 글로벌 리서치 기업 CB인사이츠(CB Insights)가 2024년 10월 3일 발표한 2024년 3분기 벤처\\n현황 보고서에 따르면 2024년 3분기 벤처 자금의 31%가 AI 스타트업에 투자된 것으로 분석\\n∙ AI 스타트업은 2024년 2분기에 전체 벤처 투자의 35%를 유치하며 역대 최고 비중을 차지했으며,\\n3분기에도 역대 두 번째로 높은 비중을 기록\\n∙ 오픈AI의 공동설립자 일리야 수츠케버(Ilya Sutskever)가 2024년 6월 설립한 스타트업 SSI(Safe\\nSuperintelligence Inc.)는 10억 달러를 유치하며 3분기 대표적인 AI 투자로 기록\\n∙ CB인사이츠가 전 세계 1만 5천 개 이상의 AI 스타트업을 추적한 결과, 전 세계 AI 스타트업의 43%가 미국\\n기업이며, 다음 순위는 중국이 9%, 영국이 7%, 인도와 캐나다가 각각 4%로 미국과 상당한 격차를 기록\\nn 기업가치 10억 달러 이상의 유니콘 기업은 2024년 3분기에 24개가 탄생했으며, 이중 절반 이상이\\nAI 기업인 것으로 확인\\n∙ 범용 로봇 개발기업 스킬드AI(Skild AI), 공간지능에 특화된 월드랩스(World Labs), 법률 AI\\n서비스 기업 하비(Harvey) 등이 유니콘 지위를 획득\\nn AI 스타트업은 투자금 회수(Exit) 시점도 일반 스타트업보다 훨씬 빨라 AI 기업이 엑시트하는 시점은\\n설립 후 7년에 불과했으나 여타 스타트업은 13년 소요되었으며, 이러한 경향은 M&A에서 가장 뚜렷해\\n2024년 AI 스타트업 엑시트는 대부분 M&A를 통해 달성\\n∙ 대기업들은 자사 제품군에 AI 도구를 신속히 도입하고자 AI 스타트업 인수에 적극적인 행보를 보이고\\n있으며, 일례로 엔비디아(Nvidia)는 2024년에 AI 스타트업 3곳을 인수했고, 세일즈포스(Salesforce)는\\n2024년 9월 AI 스타트업 2곳을 인수\\nn 그러나 CB인사이츠는 투자자들의 낙관적 기대에도 불구하고 현재의 AI스타트업 중 상당수는 기대에\\n부응하지 못하고 실패하게 될 것으로 예상\\n∙ CB인사이츠는 오픈AI와 같은 거대 AI 기업조차도 수익을 내지 못해 비용을 통제해야 하는 어려움을\\n겪고 있다며, 오픈AI의 2024년 손실 규모가 50억 달러에 달할 것으로 전망\\n☞ 출처 : CB Insights, State of Venture Q3’24 Report, 2024.10.03.\\n6\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 9, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n메타, 동영상 생성AI 도구 ‘메타 무비 젠’ 공개\\nKEY Contents\\nn 메타가 동영상 생성, 개인화 동영상 제작, 동영상 편집, 오디오 생성과 같은 기능을 지원하는\\n‘메타 무비 젠’을 공개하고 2025년 중 인스타그램 등 자사 플랫폼에 통합할 계획\\nn 메타 무비 젠은 인간 선호도 평가에서 런웨이의 젠 3, 오픈AI의 소라, 클링 1.5와 같은 경쟁\\n동영상 AI 모델보다 더 높은 점수를 기록\\n£메타, 동영상 제작과 편집, 오디오 생성을 지원하는 메타 무비 젠을 공개\\nn 메타(Meta)가 2024년 10월 4일 텍스트 입력을 통해 고해상도 동영상을 생성하는 AI 도구 ‘메타\\n무비 젠(Meta Movie Gen)’을 공개\\n∙ 메타는 크리에이터와 영화 제작자 등 소수의 외부 파트너에게 메타 무비 젠을 우선 제공 후 피드백을\\n반영해 기능을 개선할 계획으로, 단독 서비스로 출시하는 대신 2025년 중 인스타그램(Instagram)과\\n같은 자사 소셜미디어 플랫폼에 통합하여 제공할 방침\\nn 메타 무비 젠은 △동영상 생성 △개인화 동영상 생성 △동영상 편집 △오디오 생성의 4가지 기능을 지원\\n∙ (동영상 생성) 300억 개 매개변수의 AI 모델을 통해 초당 16프레임의 속도로 1,080p 해상도의 최대\\n16초 길이 동영상 생성을 지원\\n∙ (개인화 동영상 생성) 사용자가 자신이나 타인의 이미지와 텍스트를 입력해 원래 인물의 고유한 특징을\\n반영한 개인화 동영상을 제작 가능\\n∙ (동영상 편집) 특정 요소의 추가나 제거, 변경과 같은 부분적 수정 및 동영상 배경 또는 스타일 변경과\\n같은 광범위한 수정도 지원\\n∙ (오디오 생성) 130억 개 매개변수의 오디오 생성 모델을 통합해 동영상과 텍스트 프롬프트 기반으로\\n최대 45초 길이의 배경음, 음향 효과 등 고품질 오디오를 생성\\n£메타 무비 젠, 인간 선호도 평가에서 오픈AI의 소라 능가\\nn 메타 무비 젠은 인간 선호도 평가에서 런웨이(Runway)의 젠(Gen) 3, 오픈AI의 소라(Sora)를\\n비롯한 경쟁 동영상 생성AI 모델보다 더 높은 점수를 기록\\n∙ 메타 무비 젠과 경쟁 모델에 대하여 세 명의 인간 평가자가 점수를 매겨 비교 후 순승률(Net Win\\nRate)*을 계산한 결과, 메타 무비 젠은 젠 3와 소라, 클링(Kling) 1.5를 모두 능가\\n* 두 모델(A와 B)에 대하여 3명의 인간 평가자가 A 선호 시 +1점, 동점이면 0점, B 선호 시 –1점을 매기는 식으로 계산해\\n승률(-100%~100% 값)을 구하며, 승률이 양수면 A 모델 선호, 음수면 B 모델 선호를 의미\\n<메타 무비 젠과 경쟁 AI 모델의 인간 선호도 평가 승률>\\n☞ 출처: Meta, How Meta Movie Gen could usher in a new AI-enabled era for content creators, 2024.10.04.\\n7\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 10, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='SPRi AI Brief |\\n2024-11월호\\n메타, 이미지와 텍스트 처리하는 첫 멀티모달 AI 모델 ‘라마 3.2’ 공개\\nKEY Contents\\nn 메타가 이미지와 텍스트를 모두 처리할 수 있는 모델과 모바일 기기에서 실행 가능한 경량\\n모델을 포함하는 라마 3.2 시리즈를 공개\\nn 비전 기능을 갖춘 라마 3.2 90B 모델은 다양한 이미지 인식과 시각적 이해 작업에서\\n앤스로픽의 ‘클로드3-하이쿠’ 및 오픈AI의 ‘GPT-4o-미니’와 대등한 수준의 성능 보유\\n£라마 3.2 90B 모델, 이미지 인식과 시각적 이해에서 GPT-4o-미니와 대등한 성능\\nn 메타가 2024년 9월 25일 ‘라마(Llama)’ 시리즈 최초로 이미지와 텍스트를 모두 처리하는 ‘라마 3.2’를 공개\\n∙ 라마 3.2 시리즈는 이미지를 처리하는 비전(Vision) 기능을 갖춘 매개변수 110억 개(11B)와 900억\\n개(90B)의 모델 및 모바일 기기에 적합한 매개변수 10억 개(1B)와 30억 개(3B)의 경량 모델로 구성\\n∙ 2024년 7월 공개된 라마 3.1과 비교해 라마 3.2는 전반적 성능 향상 외 비전 기능이 추가되어 이미지\\n추론을 지원하며 모바일 기기에서 실행 가능한 경량 모델이 추가되어 접근성을 향상\\nn 라마 3.2 시리즈 중 11B와 90B 모델은 차트와 그래프를 포함한 문서 이해, 이미지 캡션, 이미지\\n안의 물체 식별과 같은 이미지 추론을 지원\\n∙ 라마 3.2는 이미지에서 세부 정보를 추출하고 장면을 이해하여 이미지 캡션으로 사용할 수 있도록\\n내용을 전달하는 문장을 생성 가능\\n∙ 이미지 인식과 시각적 이해 관련 90B 모델의 벤치마크 평가 결과는 앤스로픽(Anthropic)의 ‘클로드\\n3-하이쿠’나 오픈AI의 ‘GPT-4o-미니’와 대등한 수준으로, 일례로 시각적 수학 추론(MathVista)에서\\n57.3점으로 클로드 3-하이쿠(46.4점)와 GPT-4o-미니(56.7점)를 능가\\nn 라마 3.2 시리즈 중 1B와 3B 경량 모델은 12만 8천 개 토큰의 컨텍스트 창을 지원하고 다국어\\n텍스트 생성과 도구 호출 기능을 제공하며, 데이터를 기기 내에 보관하는 온디바이스 앱 개발에 특화\\n∙ 모델 평가 결과, 3B 모델은 지시 이행, 요약, 신속한 재작성 및 도구 사용과 같은 작업에서\\n구글(Google)의 ‘젬마 2 2.6B’ 및 마이크로소프트(Microsoft)의 ‘파이 3.5-미니’보다 성능이 우수\\n∙ 일례로 텍스트 재작성(Open-rewrite eval) 평가에서 3B 모델은 40.1점으로 젬마 2 2.6B(31.2점)\\n및 파이-3.5-미니(34.5점)를 앞섰으며, 텍스트 요약 능력(TLDR9+)에서는 19.0점으로 젬마 2\\n2.6B(13.9점) 및 파이-3.5-미니(12.8점)를 능가\\nn 메타는 라마 3.2 출시와 함께 개발자들이 라마 모델을 더욱 쉽고 효율적으로 사용할 수 있도록\\n지원하는 표준화 인터페이스인 ‘라마 스택(Llama Stack)’도 공개\\n∙ 개발자들은 라마 스택을 통해 온프레미스*, 클라우드, 온디바이스 등 다양한 환경에서 일관적이고\\n간소화된 방식으로 라마 모델을 구축 가능\\n* 기업이 자체 시설에서 보유하고 직접 유지 관리하는 데이터센터\\n☞ 출처: Meta, Llama 3.2: Revolutionizing edge AI and vision with open, customizable models, 2024.09.25.\\n8\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 11, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n앨런AI연구소, 벤치마크 평가에서 GPT-4o 능가하는 성능의 오픈소스 LLM ‘몰모’ 공개\\nKEY Contents\\nn 앨런AI연구소가 공개한 멀티모달 LLM 제품군 몰모는 벤치마크 평가에서 GPT-4o를\\n능가하는 성능의 72B 모델과 전문가혼합 모델, 온디바이스 모델 등 4개 모델로 구성\\nn 몰모-72B 모델은 시각적 이해 능력이 뛰어나며 벤치마크 평가 및 인간 선호도 평가에서 첨단\\n폐쇄형 모델을 능가하는 점수를 기록\\n£몰모-72B 모델, 벤치마크 평가에서 GPT-4o와 제미나이 1.5 Pro 능가\\nn 미국 비영리 연구기관 앨런AI연구소(Allen Institute for AI, 이하 AI2)가 2024년 9월 25일 오픈\\n소스 멀티모달 LLM 제품군 ‘몰모(Molmo)’를 공개\\n∙ 몰모는 가장 규모가 크고 성능이 뛰어난 72B와 데모 모델 7B-D, 개방성이 가장 높은 7B-O, 70억 개의\\n전체 매개변수 중 10억 개만 활성화하는 전문가혼합(MoE) 모델 E-1B의 4개 모델로 구성되며, 이 중\\nE-1B 모델은 온디바이스 실행 가능\\n∙ 몰모는 데이터 규모보다 품질을 중시하는 학습 방식으로 데이터 효율성이 뛰어나 컴퓨팅 자원이 한정된\\n환경에서도 사용 가능한 것이 장점\\n∙ 몰모는 일상 사물과 표지판, 복잡한 차트, 시계, 메뉴판 등 다양한 시각 자료를 이해하고 이미지를\\n구성하는 요소를 정확히 지목할 수 있어, 화면과 현실 세계 간 복잡한 상호작용(예: 비행기 표 예약)이\\n필요한 웹 에이전트나 로봇 개발에도 유리\\n∙ AI2는 몰모의 언어와 시각 훈련 데이터, 미세조정 데이터, 모델 가중치, 소스코드를 모두 공개하고\\n연구와 상업적 목적의 활용을 허용\\nn AI2에 따르면 몰모-72B 모델은 주요 벤치마크와 인간 선호도 평가*에서 첨단 폐쇄형 모델을 능가\\n* 870명의 인간 평가자에게 다양한 이미지와 텍스트 프롬프트 쌍에 대한 모델 간 응답을 비교해 선호도 평가를 요청해 순위를 산정\\n∙ 몰모-72B는 11개 벤치마크 평균 점수 81.2점으로 ‘GPT-4o’(78.5점), ‘제미나이 1.5 Pro’(78.3점),\\n‘클로드-3.5 소네트’(76.7점)를 넘는 최고 점수를 기록했으며 인간 선호도 평가에서는 1077점으로\\nGPT-4o(1079점)에 이어 2위\\n∙ 전문가혼합 모델인 몰모E-1B는 벤치마크 평균 점수에서 68.6점, 인간 선호도 평가는 1,032점으로\\n각각 71.1점과 1,041점을 받은 GPT-4V과 경쟁할 수 있는 수준\\n<몰모 제품군과 GPT-4o/GPT-4V의 벤치마크 평균(左)과 인간 선호도 평가(右) 점수 비교>\\n☞ 출처: Allen Institute for AI, Introducing Molmo, 2024.09.25.\\n9\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 12, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='SPRi AI Brief |\\n2024-11월호\\n미스트랄AI, 온디바이스용 AI 모델 ‘레 미니스트로’ 공개\\nKEY Contents\\nn 미스트랄AI가 네트워크 연결 없이 온디바이스로 사용할 수 있는 경량 모델 ‘레 미니스트로’를\\n미스트랄 3B와 미스트랄 8B 버전으로 공개\\nn 벤치마크 평가에서 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마 및 라마와\\n비교해 대부분 벤치마크에서 더 높은 평가를 획득\\n£미스트랄 AI, 네트워크 연결이 필요 없는 경량 모델 ‘레 미니스트로’ 출시\\nn 프랑스의 대표 AI 스타트업 미스트랄AI(Mistral AI)가 2024년 10월 16일 네트워크 연결 없이\\n작동하는 온디바이스용 AI 모델 ‘레 미니스트로(Les Ministraux)’를 발표\\n∙ ‘미스트랄 3B’와 ‘미스트랄 8B’ 버전으로 공개된 이 모델은 경량 모델이면서도 영어책 50쪽 분량에\\n해당하는 12만 8천 개 토큰의 컨텍스트 창을 지원\\n∙ 미스트랄AI는 레 미니스트로가 번역과 스마트 어시스턴트, 분석, 자율 로봇 같은 중요 애플리케이션에\\n대하여 네트워크 연결 없이 개인정보보호가 가능한 온디바이스 추론을 원하는 고객 수요에 맞게\\n지연시간이 짧고 효율적인 솔루션을 제공한다고 강조\\n∙ 미스트랄AI는 8B 버전만 연구용으로 다운로드를 허용했으며 향후 두 모델을 클라우드 플랫폼을 통해\\n제공할 계획으로, 사용 비용은 100만 입출력 토큰 당 8B 버전은 10센트, 3B 버전은 4센트로 책정\\n£레 미니스트로, 오픈소스 모델 ‘젬마’ 및 ‘라마’ 대비 대부분 벤치마크에서 우수한 평가\\nn 벤치마크 평가 결과, 레 미니스트로는 비슷한 매개변수를 가진 오픈소스 모델 젬마(Gemma)와 라마\\n(Llama)보다 대부분 벤치마크에서 더 높은 평가를 획득\\n∙ MMLU* 평가에서 미스트랄 3B는 60.9점을 얻어 구글의 ‘젬마 2 2B’(52.4점)와 메타의 ‘라마 3.2\\n3B’(56.2점)를 앞섰고, 미스트랄 8B는 65.0점으로 ‘라마 3.1 8B’(64.7점)와 1년 전 출시된 자체\\n모델 ‘미스트랄 7B’(62.5점)를 능가\\n* 다양한 주제에 대한 모델의 광범위한 지식과 추론 능력을 평가하는 벤치마크\\n∙ 미스트랄 8B는 코딩 능력을 평가하는 HumanEval pass@1*에서만 34.8점으로 ‘라마 3.1\\n8B’(37.8점)보다 소폭 낮은 점수를 기록\\n* AI 모델이 한 번의 시도로 정확한 코드를 생성할 수 있는 능력을 평가하는 벤치마크\\n<미스트랄 3B/7B와 경쟁 모델의 벤치마크 평가 비교>\\n☞ 출처: Mistral AI, Un Ministral, des Ministraux-Introducing the world’s best edge models, 2024.10.16.\\n10\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 13, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n카카오, 통합 AI 브랜드 겸 신규 AI 서비스 ‘카나나’ 공개\\nKEY Contents\\nn 카카오가 대화의 맥락 속에서 주요 정보를 기억해 이용자에게 최적화된 답변을 제공하는 AI\\n메이트 서비스인 ‘카나나’를 공개 했으며 카카오톡과 별개의 앱으로 출시 예정\\nn 카카오는 자체 언어모델로 용량 별로 카나나 플래그, 카나나 에센스, 카나나 나노도 개발\\n중으로, 에센스와 나노를 중심으로 주요 서비스에 적용할 계획\\n£카카오의 신규 AI 서비스 ‘카나나’, 개인메이트 ‘나나’와 그룹메이트 ‘카나’로 구현\\nn 카카오가 2024년 10월 22~24일 열린 개발자 컨퍼런스 ‘if(kakaoAI)2024’에서 그룹 전체의\\nAI 비전과 방향성을 공개하고 통합 AI 브랜드 ‘카나나(Kanana)’를 발표\\n∙ 사명인 카카오와 함께, ‘나에게 배워 나처럼 생각하고 행동한다’는 의미의 네이티브(Native), ‘배우지\\n않아도 자연스럽게 사용 가능한 기술’이라는 의미의 내츄럴(Natural) 등의 단어를 조합한 카나나는\\n‘가장 나다운 AI’를 의미\\n∙ 카카오는 동 브랜드를 자사가 개발하는 주요 AI 모델과 신규 서비스의 이름에 두루 사용할 계획으로,\\nAI 메이트 서비스 ‘카나나’ 출시 계획도 공개\\nn 카나나는 대화의 맥락 속에서 주요 정보를 기억해 이용자에게 최적화된 답변을 제시하는 ‘AI 메이트’를\\n지향하며, 개인메이트 ‘나나(nana)’와 그룹메이트 ‘카나(kana)’로 구현\\n∙ 개인메이트 나나는 이용자와 일대일 대화 및 이용자가 참여한 그룹 대화도 기억해 최적화된 개인화\\n경험을 제공하며, 일례로 그룹대화에서 나눈 컨퍼런스 참석 일정과 준비물을 기억해 이를 잊지 않도록\\n메시지로 전송\\n∙ 카나는 상주하는 그룹대화 안에서의 대화 내용만 기억해 이용자를 지원하며, 가령 스터디 그룹대화에서\\n함께 읽은 논문 관련 퀴즈를 내주고 채점과 부연 설명을 제공\\n∙ 카카오는 카나나를 카카오톡과 별개의 앱으로 출시할 예정으로, 연내 사내 테스트 버전 출시를 통해\\n완성도를 높여갈 계획\\nn 카카오는 자체 생성AI 모델도 연구개발 중으로, 언어모델은 용량에 따라 △카나나 플래그 △카나나\\n에센스 △카나나 나노로 분류되며, 글로벌 수준의 성능을 갖춘 에센스와 나노를 중심으로 카카오의\\n주요 서비스에 적용할 계획\\nn 카카오는 이번 행사에서 내부의 AI 리스크 관리 체계인 ‘Kakao ASI(AI Safety Initiative)’도 강조\\n∙ Kakao ASI는 안전하고 윤리적인 AI 기술 개발 및 운영 시스템을 구축하기 위한 종합 지침으로서,\\n기술의 설계부터 개발, 테스트, 배포, 모니터링, 업데이트 등 AI 시스템의 전 생애주기에서 발생할 수\\n있는 리스크에 선제적 대응 추구\\n☞ 출처: Kakao, 카카오, ‘if(kakaoAI)2024’에서 그룹 AI 비전 공개…AI 메이트 ‘카나나’도 처음 선보여, 2024.10.22.\\n11\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 14, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='SPRi AI Brief |\\n2024-11월호\\n2024년 노벨 물리학상과 화학상, AI 관련 연구자들이 수상\\nKEY Contents\\nn 2024년 노벨 물리학상은 물리학 원리를 바탕으로 인공 신경망을 이용한 머신러닝의 토대가 되는\\n방법을 개발한 존 홉필드와 제프리 힌턴이 수상\\nn 2024년 노벨 화학상은 단백질 설계에 기여한 데이비드 베이커 및 단백질 구조를 예측하는 AI\\n모델을 개발한 딥마인드의 데미스 허사비스와 존 점퍼가 수상\\n£노벨 물리학상, 인공 신경망 연구한 존 홉필드 교수와 제프리 힌턴 교수가 수상\\nn 스웨덴 왕립과학원 노벨위원회는 2024년 10월 8일 존 홉필드(John Hopfield) 미국 프린스턴⼤\\n교수와 제프리 힌턴(Geoffrey Hinton) 캐나다 토론토⼤ 교수에게 인공 신경망을 이용한 머신러닝의\\n토대가 되는 방법을 개발한 공로로 노벨 물리학상을 수여\\n∙ 홉필드는 물리학의 원리를 이용해 왜곡되거나 불완전한 입력 패턴과 가장 유사하게 저장된 패턴을\\n찾아내고 재구성할 수 있는 초기 인공 신경망 모델인 ‘홉필드 네트워크(Hopfield Network)’를 개발\\n∙ 힌턴은 홉필드 네트워크를 토대로 ‘볼츠만 머신(Boltzmann Machine)’을 고안했으며, 이 모델은\\n통계물리학을 활용해 주어진 데이터에서 특징적 요소를 인식하여 인간의 개입 없이 학습된 패턴\\n유형을 활용해 새로운 예제를 생성 가능\\n∙ 힌턴은 인공 신경망이 데이터를 통해 학습할 수 있다는 개념으로 머신러닝의 폭발적 발전을\\n이끌었으며, 인공 신경망은 현재 신소재 발견을 비롯한 광범위한 물리학 연구에 활용되는 추세\\n£노벨 화학상, 단백질 구조 예측 AI 모델 개발한 딥마인드 연구진 등 3인이 수상\\nn 데이비드 베이커(David Baker) 미국 워싱턴⼤ 교수와 데미스 허사비스(Demis Hassabis) 구글\\n딥마인드 CEO, 존 점퍼(John Jumper) 구글 딥마인드 수석 연구원은 새로운 단백질 생성 및 AI를\\n활용한 단백질 구조 예측에 대한 공로로 2024년 10월 9일 노벨 화학상을 수상\\n∙ 베이커 교수는 90년대 말 단백질 구조를 예측하는 컴퓨터 소프트웨어 ‘로제타(Rosetta)’를 개발*했으며,\\n2003년에는 단백질의 기본 요소인 아미노산을 이용해 기존 단백질과 다른 새로운 단백질을 설계\\n* 로제타폴드를 소개한 2021년 Science 논문에는 로제타폴드의 핵심개발자이자 제1저자인 현 백민경 교수\\n∙ 허사비스와 점퍼는 1970년대부터 난제로 남아있던 단백질 구조 예측에 결정적 기여를 한 AI 모델\\n‘알파폴드(AlphaFold) 2’를 2020년 발표하고 오픈소스로 공개\\n∙ 2억 개에 달하는 단백질 구조를 예측한 알파폴드 2는 과거에는 몇 년이 걸리거나 불가능하던 단백질\\n구조 예측을 몇 분 만에 완료할 수 있으며, 2024년 10월까지 190개국 200만 명 이상에 의해 사용\\n∙ 노벨위원회에 따르면 단백질의 구조 예측과 새로운 단백질의 설계는 특정 질병이나 항생제 내성의\\n발생원인 이해 및 새로운 의약품이나 나노소재 개발 등으로 인류에게 막대한 이익을 가져올 전망\\n☞ 출처: The Nobel Prize, They used physics to find patterns in information, 2024.10.08.\\nThe Nobel Prize, They have revealed proteins’ secrets through computing and artificial intelligence, 2024.10.09.\\n12\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 15, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n미국 국무부, AI 연구에서 국제협력을 위한 ‘글로벌 AI 연구 의제’ 발표\\nKEY Contents\\nn 미국 국무부는 바이든 대통령의 AI 행정명령에 따라 국제협력을 통해 포괄적이고 조정된 AI\\nR&D 접근방식을 제시한 ‘글로벌 AI 연구 의제(GAIRA)’를 발표\\nn 국무부는 GAIRA를 통해 AI R&D 원칙과 안전하고 신뢰할 수 있는 AI 발전을 위한 연구\\n우선순위, 주요 이해관계자별 권장 사항을 제시\\n£국무부, AI 연구 우선순위로 포괄적 연구 인프라 조성과 글로벌 도전과제 해결 등 제시\\nn 미국 국무부(United States Department of State)가 2024년 9월 23일 국제협력을 통해 안전하고\\n신뢰할 수 있는 AI 시스템을 개발하기 위한 R&D 원칙과 우선순위, 권장 사항을 제시한 ‘글로벌\\nAI 연구 의제(Global AI Research Agenda, 이하 GAIRA)’를 발표\\n∙ 국무부는 2023년 10월 30일 바이든 대통령이 서명한 AI 행정명령에 따라 모든 사람에게 이로운\\n방식으로 개발·사용되는 AI R&D에 대한 포괄적이고 조정된 접근방식을 마련하고자 GAIRA를 작성하고,\\nAI 연구에서 3가지 권장 원칙으로 △포용성·형평성 △책임 있는 연구 수행 △파트너십과 협업을 제시\\nn 국무부는 GAIRA를 통해 안전하고 신뢰할 수 있는 AI를 발전시키기 위한 연구 우선순위를 제시\\n∙ (사회 기술 연구) 기술과 사회 간 상호작용에 대한 이해를 심화하고 인간 복지를 향상하는 AI 시스템의\\n설계와 배포에 관한 연구를 수행 우선\\n∙ (포용적 연구 인프라 조성) AI 기술과 시스템의 혁신을 지원하는 데이터와 컴퓨팅 성능, 연구 플랫폼에\\n대한 접근성을 향상해 AI 연구와 개발 생태계의 다양성을 촉진하고 편향을 완화\\n∙ (글로벌 도전과제 해결) 환경 문제, 경제 회복력, 사회복지 등 글로벌 도전 과제 해결에 도움이 되는 AI\\n애플리케이션을 우선 개발\\n∙ (AI 안전과 보안, 신뢰성을 포함한 AI 기초연구) AI는 아직 개발 초기 단계로 안전하고 신뢰할 수 있는\\nAI 시스템 개발을 위해 더 많은 기술 발전 필요\\n∙ (글로벌 노동 시장에서 AI의 영향 연구) AI가 노동 시장에 미치는 여러 측면을 다루는 연구를 수행하고\\n노동 시장에 미치는 AI의 부정적 영향을 완화하기 위한 전략을 수립\\nn 국무부는 GAIRA를 통해 연구 기금 제공자, 연구 생태계 허브, 연구팀과 같은 이해관계자별로 연구\\n의제의 목표 달성을 위한 권장 사항을 제시\\n∙ (연구 기금 제공자) 투명성을 증진하고 국제 AI 연구 협력을 지원하는 기금을 요청하며 다양한 지역에\\n서 연구 인프라 접근성을 증진하고 민관협력을 추진\\n∙ (연구 생태계 허브) 연구 재현성을 장려하고, AI 연구 가이드라인 관련 협력과 조정을 강화하며, 민간\\n분야에서 중시하는 연구 주제 이외의 연구를 지원\\n∙ (연구팀) 다학제적 팀을 우선 편성하고 지역 연구자들과 협력하며, 사회기술적 방법론과 연구 설계를\\n채택하고 위험 평가 절차를 통합\\n☞ 출처: U.S. Department of State, Global AI Research Agenda, 2024.09.23.\\n13\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 16, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='SPRi AI Brief |\\n2024-11월호\\n일본 AI안전연구소, AI 안전성에 대한 평가 관점 가이드 발간\\nKEY Contents\\nn 일본 AI안전연구소는 AI 개발자나 제공자가 안전성 평가에 참조할 수 있는 ‘AI 안전성에 대한\\n평가 관점 가이드’를 발표\\nn 가이드는 AI 안전성의 핵심 요소를 달성하기 위한 10가지 평가 관점과 함께, 평가를 통해\\n효과적 조치를 취했을 때의 기대 목표를 제시\\n£일본 AI안전연구소, AI 개발자나 제공자의 안전성 평가를 위한 가이드라인 제시\\nn 일본 AI안전연구소(Japan AI Safety Institute)가 2024년 9월 25일 AI 개발자나 제공자가 안전성\\n평가 시에 참조할 수 있는 기본 개념을 제시하는 ‘AI 안전성에 대한 평가 관점 가이드’를 발간\\n∙ 가이드는 AI 안전성의 핵심 요소로 △인간중심 △안전성 △공평성 △프라이버시 보호 △보안 △투명성을\\n제시하고, 이를 달성하기 위한 10가지 평가 관점 및 평가를 통한 효과적 조치 이후의 기대 목표를 수립\\n<AI 안전성의 핵심 요소를 고려한 AI 안전성 평가 관점>\\n평가 관점 관련 AI 안전성 요소 기대 목표\\n유해 정보의 출력 통제 인간중심, 안전성, 공정성 Ÿ LLM 시스템이 테러, 범죄, 불쾌한 표현 등 유해 정보의 출력을 통제 가능\\n허위 정보와 Ÿ LLM 시스템의 출력에 대한 사실 검증 메커니즘 구축\\n인간중심, 안전성, 투명성\\n조작 방지 Ÿ LLM 시스템의 출력에 의한 사용자 결정의 조작 방지\\nŸ LLM 시스템 출력에 유해한 편향이 없으며 개인이나 집단에 대한 불공정한\\n공정성과 포용성 인간중심, 공정성, 투명성 차별 부재\\nŸ LLM 시스템의 출력을 모든 최종 사용자가 이해 가능\\n고위험 사용 및 Ÿ LLM 시스템이 본래 목적과 다르게 부적절하게 사용되어도 피해나 불이익\\n인간중심, 안전성\\n비의도적 사용 대처 미발생\\n개인정보 보호 프라이버시 보호 Ÿ LLM 시스템이 정보의 중요성에 따라 프라이버시를 적절히 보호\\nŸ LLM 시스템의 허가되지 않은 운영 및 비의도적 수정 또는 중단으로 인한\\n보안 보안\\n기밀정보의 유출 방지\\nŸ LLM 시스템 작동에 대한 증거 제시 등을 목적으로 출력의 근거를 기술적\\n설명 가능성 투명성\\n으로 합리적인 범위에서 확인 가능\\nŸ LLM 시스템이 적대적 프롬프트, 왜곡된 데이터 및 잘못된 입력 등 예상치\\n견고성 안전성, 투명성\\n않은 입력에 대해 안정적 출력을 제공\\nŸ LLM 시스템 학습을 위한 데이터가 적절한 상태로 유지되고 데이터 이력이\\n데이터 품질 안전성, 공정성, 투명성\\n적절히 관리되는 상태\\nŸ LLM 시스템에 대한 다양한 유형의 검증이 모델 학습 단계에서 시스템 사용\\n검증 가능성 투명성\\n시점까지 제공되는 상태\\nn AI 안전성 평가는 기본적으로 AI 시스템의 개발자 및 제공자에 의해 실시되며, AI 시스템 개발, 배포,\\n사용 단계에서 적절한 간격으로 시행될 필요\\n∙ AI 안전성 평가 범위는 개발 단계에서는 데이터, 배포와 사용 단계에서는 전체 LLM 시스템 등으로 달라질 수 있으며,\\n평가는 한 차례가 아니라 반복적으로 실시\\n☞ 출처: Japan AI Safety Institute, AIセーフティに関する評価観点ガイドの公開, 2024.09.25.\\n14\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 17, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n구글 딥마인드, 반도체 칩 레이아웃 설계하는 AI 모델 ‘알파칩’ 발표\\nKEY Contents\\nn 구글 딥마인드가 강화학습 방식으로 반도체 칩 레이아웃을 설계하여 사람이 몇 주에서 몇 달이\\n걸리는 수준의 칩 레이아웃을 몇 시간 만에 생성하는 AI 모델 ‘알파칩’을 공개\\nn 구글은 2020년 처음 알파칩에 관한 연구 논문을 발표한 뒤, 자체 AI 칩 TPU 개발 시\\n알파칩을 활용해 칩 성능을 개선하고 개발 주기를 단축\\n£알파칩, 구글의 자체 AI 칩 TPU의 레이아웃 설계에도 기여\\nn 구글 딥마인드가 2024년 9월 26일 반도체 칩의 레이아웃을 설계할 수 있는 AI 모델 ‘알파칩\\n(AlphaChip)’을 공개\\n∙ 2020년 연구 프로젝트로 시작된 알파칩은 강화학습 방식을 사용하여 반도체 칩 레이아웃을 설계하며,\\n사람이 완료하는데 몇 주에서 몇 달이 걸리는 수준의 칩 레이아웃을 몇 시간 만에 생성 가능\\nn 구글은 2020년 알파칩에 대한 연구 논문을 처음 발표했으며, 자체 AI 칩 TPU(Tensor Processing\\nUnit) 개발 시 알파칩을 활용해 칩 레이아웃을 설계\\n∙ TPU는 제미나이(Gemini)뿐 아니라 이마젠(Imagen), 비오(Veo) 등의 이미지 및 동영상 생성 모델과\\n같은 구글 AI 시스템의 핵심 요소를 형성\\n∙ 알파칩은 최신 6세대 TPU를 포함한 새로운 세대마다 칩 레이아웃 설계를 개선해 설계주기를\\n단축하고 더 높은 성능의 칩 생산에 기여\\nn 알파칩은 바둑에 특화된 알파고(AlphaGo) 및 바둑, 체스, 쇼기(일본 장기)를 마스터한 알파제로\\n(AlphaZero)와 비슷하게 칩 레이아웃 설계를 게임처럼 접근\\n∙ 알파칩은 모든 부품을 배치할 때까지 한 번에 하나의 회로 부품을 배치하고 최종 레이아웃의 품질에 따라\\n보상을 받게 되며, 상호 연결된 부품 간 관계를 학습하고 칩 전체로 확장해 레이아웃을 개선\\nn 구글은 자체 AI 칩 TPU뿐 아니라 영국 반도체 기업 ARM과 협력해 개발한 데이터센터용 CPU인\\n액시온(Axion) 프로세서도 알파칩으로 레이아웃을 생성했으며, 타사에도 알파칩을 제공\\n∙ 대만의 반도체 기업 미디어텍(MediaTek)은 삼성 스마트폰에 사용되는 ‘다이멘시티 플래그십(Dimensity\\nFlagship) 5G’와 같은 첨단 칩 개발에 알파칩을 활용해 개발을 가속화하고 칩 성능을 개선\\nn 구글 딥마인드는 현재 알파칩의 차기 버전을 개발 중으로, 향후 알파칩이 칩 설계주기의 전 단계를\\n최적화하고 스마트폰, 의료 장비, 농업 센서 등에 사용되는 맞춤형 하드웨어의 칩 설계에 혁신을\\n가져올 것으로 기대\\n☞ 출처: Google Deepmind, How AlphaChip transformed computer chip design, 2024.09.26.\\n15\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 18, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전트에 트랜스포머 아키텍처의 대안 필요성 강조\\nKEY Contents\\nn 이스라엘 AI 스타트업 AI21의 오리 고센 CEO는 AI 모델 개발에 주로 활용되는 트랜스포머\\n아키텍처가 느린 속도와 과도한 연산 비용으로 인해 AI 에이전트에 부적합하다고 지적\\nn 고센 CEO는 AI 에이전트를 활성화하려면 메모리 사용을 최적화하여 효율적 연산과 비용\\n절감을 지원하는 맘바나 잠바와 같은 대체 아키텍처에 주목해야 한다고 주장\\n£AI 에이전트 활성화를 위해 향상된 메모리 성능을 갖춘 대체 아키텍처 채택 필요\\nn 이스라엘의 AI 스타트업 AI21의 오리 고센(Ori Goshen) CEO가 AI 에이전트를 활성화하려면\\n트랜스포머(Transformer)* 이외의 새로운 아키텍처**가 필요하다고 주장\\n* 문장 속 단어와 같은 순차 데이터 내의 관계를 추적해 맥락과 의미를 학습하는 신경망\\n** AI 시스템이 데이터를 처리하고 학습하기 위한 신경망의 전체적인 구조와 설계 방식을 의미\\n∙ 트랜스포머는 현재 AI 모델 개발에서 가장 많이 사용되는 아키텍처이지만, 다중 에이전트 생태계 조성\\n측면에서는 한계를 내포\\n∙ 트랜스포머 아키텍처는 처리하는 컨텍스트가 길수록 속도가 느리고 연산 비용이 많이 드는데, AI\\n에이전트는 LLM을 여러 차례 호출해야 하고 각 단계에서 광범위한 컨텍스트를 사용하는 경우가 많아\\n처리 과정에서 지연이 발생\\nn 고센 CEO는 ‘맘바(Mamba)’와 ‘잠바(Jamba)’와 같은 대체 아키텍처를 활용하면 AI 에이전트를 더\\n효율적이고 저렴하게 만들 수 있다고 강조\\n∙ 카네기멜론⼤와 프린스턴⼤ 연구진이 개발한 맘바는 트랜스포머 모델의 핵심인 어텐션(Attention)*\\n메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사용을 최적화\\n* 입력된 데이터 간 연관성을 파악해 상호작용을 계산하는 메커니즘\\n∙ 미스트랄이 2024년 7월 ‘코드스트랄(Codestral) 맘바 7B’를, UAE의 AI 기업 팔콘(Falcon)이 8월\\n‘팔콘 맘바 7B’를 출시하는 등, 최근 오픈소스 AI 개발자 사이에서 맘바의 인기가 높아지는 추세\\n∙ AI21 역시 맘바 아키텍처를 토대로 더 빠른 추론 시간과 더 긴 컨텍스트를 지원하는 잠바 아키텍처를\\n활용해 기반모델을 개발\\nn 고센 CEO는 AI 에이전트가 최근 들어서야 부상하고 있으며 대다수 AI 에이전트가 아직 상용화되지 않은\\n이유가 트랜스포머로 구축된 LLM의 한계 때문이라고 지적\\n∙ AI 에이전트가 상용화되려면 데이터 간 연관성을 파악해 확률적으로 가장 그럴듯한 답변을 생성하는\\nLLM의 신뢰성을 높여야 하며, 필요한 수준의 신뢰성 보장을 위해서는 추가적인 요소의 통합이 필요\\n∙ 최근 서비스나우(ServiceNow), 세일즈포스 등 여러 기업이 AI 에이전트나 에이전트 구축을 지원하는\\n플랫폼을 출시하는 추세로, 고센 CEO는 이러한 추세가 적절한 기반모델과 아키텍처를 조합함으로써 더욱\\n확산될 것으로 예상\\n☞ 출처: Venturebeat, AI21 CEO says transformers not right for AI agents due to error perpetuation, 2024.10.11.\\n16\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 19, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\nMIT 산업성과센터, 근로자 관점에서 자동화 기술의 영향 조사\\nKEY Contents\\nn MIT 산업성과센터가 설문조사를 통해 근로자 관점의 자동화 기술의 영향을 조사한 결과,\\n근로자들은 직장 내 안전, 임금, 업무 자율성 등에서 자동화를 긍정적으로 평가\\nn 복잡한 문제 해결이 필요한 작업을 수행하는 근로자 및 자신의 직무에 만족하는 근로자일수록\\n자동화의 영향에 긍정적인 것으로 확인\\n£근로자들, 직장 내 안전, 임금, 업무 자율성 등에서 자동화의 영향에 긍정적\\nn MIT 산업성과센터(IPC)는 2024년 9월 30일 9개국* 9천 명 이상의 근로자에 대한 설문조사를\\n바탕으로 근로자 관점에서 자동화 기술을 평가한 연구 결과를 공개\\n* 독일, 미국, 스페인, 영국, 이탈리아, 일본, 폴란드, 프랑스, 호주\\n∙ 연구진은 설문조사를 통해 업무 환경, 직장에서 사용되는 자동화 기술(로봇 및 AI 등), 업무와 기술에\\n대한 태도, 기술이 업무에 미치는 영향을 조사\\nn 조사 결과, 근로자들 사이에서는 직장 내 안전이나 임금, 업무 자율성 등의 측면에서 자동화가\\n긍정적 영향을 미칠 것이란 응답이 우세\\n∙ 자동화가 직장 내 안전에 미치는 영향에 대하여 응답자 44.9%는 긍정적으로 평가했으며 부정적\\n응답은 12.5%에 불과\\n∙ 자동화가 임금에 미치는 영향은 28.8%가 긍정적, 24.8%는 부정적으로 답했으며, 업무 자율성에\\n미치는 영향은 37.9%는 긍정적, 19.9%가 부정적이라고 응답\\nn 자동화 기술에 대한 근로자들의 인식은 대체로 긍정적으로 나타났으나, 국가 별 차이가 존재하며\\n미국 근로자들이 가장 비관적 태도를 보유\\n∙ 9개국 중 미국에서만 자동화가 임금 및 직업 안정성에 부정적이라는 응답이 긍정적이라는 응답보다\\n우세(임금: –0.6%, 직업 안정성: -4.6%)*\\n* 긍정적 응답에서 부정적 응답 비율을 뺀 수치\\nn 직무 유형에서는 복잡한 문제 해결이나 새로운 아이디어가 필요한 작업을 수행하는 사무직 근로자가\\n자동화에 더 긍정적이며, 직장 내 처우도 자동화에 대한 근로자의 인식에 영향을 발휘\\n∙ 고용주가 근로자를 적절히 대우하고 안전에 투자하는 직장에서 일하는 근로자는 직장 내 자동화의\\n영향에 긍정적이며, 직무 만족도와 신뢰도도 자동화에 대한 긍정적 인식에 영향을 미치는 요인으로 확인\\nn 연구진은 조사 결과를 바탕으로 직장 내 원활한 자동화 기술 도입을 위해 직무 설계를 통해 근로자가\\n복잡한 문제를 해결할 수 있는 역할을 만들 것을 권고\\n∙ 근로자들은 신기술 사용과 관련된 보너스가 제공되면 자동화에 더 긍정적인 것으로 나타나, 생산성\\n향상을 위한 자동화 기술 사용에 금전적 보상을 제공하는 방안도 고려 필요\\n☞ 출처: MIT IPC, Automation from the Worker’s Perspective, 2024.09.30.\\n17\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 20, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='SPRi AI Brief |\\n2024-11월호\\n다이스 조사, AI 전문가의 73%는 2025년 중 이직 고려\\nKEY Contents\\nn 다이스에 따르면, AI 전문가의 73%는 2025년 이직을 계획 중이며, 58%는 2024년 중 현재보다\\n더 나은 일자리를 찾을 자신이 있다고 응답해 여타 기술 전문가 대비 직업 전망을 낙관\\nn AI 전문가들은 여타 기술 전문가 대비 AI 도구 사용에도 적극적이며, 업무에 생성AI가 상당한\\n영향을 미친다는 응답도 36%로 여타 기술 전문가(22%) 대비 높은 수치를 기록\\n£AI 전문가들, 일반적인 기술 전문가보다 직업 전망에 낙관적\\nn 미국 기술직 채용 플랫폼 다이스(Dice)의 조사에 따르면, AI 기술 전문가는 일반적인 기술 전문가\\n대비 기술 산업의 미래와 자기 경력에 대하여 낙관적\\n∙ 이번 조사는 520명의 미국 정규직 기술 전문가와 390명의 인사 전문가의 응답을 토대로 기술 분야의\\n일자리 시장 환경을 분석\\n∙ 2024년 동안 주요 빅테크가 기술직에 대한 정리해고를 단행하고 기술직 채용도 2021~2022년 대비\\n대폭 감소하는 등 일자리 시장의 침체에도 2024년 기술과 사업의 핵심 요소로 부상한 AI 분야의\\n전문가들은 직업 전망을 낙관\\nn AI 전문가의 73%는 2025년에 이직을 계획 중이며, 58%는 2024년 중 현재보다 더 나은 새로운 일자리를\\n찾을 자신이 있다고 응답\\n∙ 일반적인 기술 전문가의 경우 65%가 2025년 중 이직을 계획 중이며, 2024년 더 나은 신규 일자리를\\n찾을 수 있다고 자신하는 비율은 36%에 불과\\n∙ AI 전문가는 빅테크를 선호하는 비율이 29%로 일반적인 기술 전문가(18%) 대비 더 높게 나타났으며,\\n이는 예산 규모가 더 크고 중요한 AI 프로젝트에 관심이 있거나 빅테크의 채용 가능성에 자신 있기\\n때문으로 추측\\nn 그러나 AI 전문가들은 기업에서 자신이 맡은 업무에 대하여 엇갈린 감정을 표시했으며, 자신의 업무가\\n가치 있다고 느끼는 전문가일수록 현재 역할에 만족할 가능성도 증대할 것으로 추론\\n∙ AI 전문가의 51%는 자신의 프로젝트가 기업에 전략적 가치가 있다고 답했으나, 36%는 투자자나\\n이사회, 외부 관계자에게 기업이 AI로 뭔가를 하고 있음을 보여주기 위한 목적이라고 응답\\nn AI 전문가들은 AI 도구 사용에도 적극적이지만, 일반적인 기술 전문가들은 업무에서 AI 도구 사용을\\n주저하는 편으로, AI 전문가들은 일주일에 1회 이상 AI를 사용하는 비율이 49%에 달했으나, 여타\\n기술 전문가들은 25%에 불과\\nn 생성AI가 미치는 영향에 대해서 AI 전문가 사이에서는 상당한 영향을 미친다는 응답이 36%, 약간의 영향을\\n미친다는 응답이 56%, 영향이 없다는 응답은 8%를 기록했으나, 여타 기술 전문가들은 22%가 상당한 영향,\\n53%는 약간의 영향, 26%는 영향이 없다고 응답\\n☞ 출처: Dice, 3 Key Lessons about the AI Tech Talent Market, 2024.09.05.\\n18\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 21, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n가트너 예측, AI로 인해 엔지니어링 인력의 80%가 역량 향상 필요\\nKEY Contents\\nn 가트너에 따르면 생성AI의 도입으로 중장기적으로 소프트웨어 엔지니어링에서 데이터 과학 및\\nAI/ML 역량의 중요성이 커지면서 AI 엔지니어의 수요가 늘어날 전망\\nn 기업들은 AI 엔지니어를 지원하고 기업 내 AI 통합을 촉진하기 위해 AI 개발자 플랫폼에 대한\\n투자를 강화할 필요\\n£생성AI로 소프트웨어 엔지니어링에서 데이터과학과 AI/ML 역량의 중요성 증대\\nn 시장조사기관 가트너(Gartner)에 따르면 2027년까지 생성AI로 인해 소프트웨어 엔지니어링\\n인력의 80%가 역량 향상이 필요할 전망\\n∙ AI로 인해 인간 엔지니어에 대한 수요가 감소하거나 심지어 AI가 인간을 대체할 것이라는 예상과\\n달리, 가트너는 AI가 향후 소프트웨어 엔지니어의 역할을 변화시키더라도 인간의 전문성과 창의성은\\n여전히 중요하다고 강조\\nn 가트너에 따르면 생성AI는 소프트웨어 엔지니어의 역할에 단기, 중기, 장기적으로 영향을 미칠 전망\\n∙ 단기적으로는 AI가 기존 개발자의 작업 패턴과 업무를 보완하며 소폭의 생산성 향상 효과를 가져오며,\\nAI의 생산성 향상 효과는 성숙한 엔지니어링 관행을 갖춘 기업의 상급 개발자에게 집중될 전망\\n∙ 중기적으로는 AI 에이전트를 통해 더 많은 업무가 자동화되어 개발자의 작업 패턴의 변화가 예상되며,\\n이는 코드 대부분이 인간이 아닌 AI로 생성되는 AI 네이티브 소프트웨어 엔지니어링의 출현을 의미해\\n자연어 프롬프트 엔지니어링과 검색 증강 생성(RAG)* 기술이 엔지니어링의 필수 역량이 될 전망\\n* 외부 데이터를 활용하여 LLM의 출력 정확성을 향상하는 기술\\n∙ 장기적으로는 기업 내 AI 기반 소프트웨어 수요가 증가하면서 이를 충족하기 위해 소프트웨어\\n엔지니어링, 데이터 과학, AI/ML(머신러닝) 분야의 고유한 기술을 갖춘 훨씬 숙련된 AI 엔지니어가\\n부상할 전망\\n£AI 엔지니어를 지원하기 위해 기업의 AI 개발자 플랫폼 투자 필요\\nn 가트너가 2023년 4분기에 미국과 영국 기업 300개를 대상으로 실시한 설문조사에 따르면 소프트웨어\\n엔지니어링 책임자의 56%가 AI/ML 엔지니어를 2024년 가장 수요가 많은 직업으로 평가\\n∙ 기업들은 AI 엔지니어를 지원하기 위해 AI 개발자 플랫폼에 투자해야 하며, AI 개발자 플랫폼은\\n기업이 AI 역량을 더욱 효율적으로 구축하고 AI를 기업 솔루션에 대규모로 통합하는 데 도움이 될 전망\\n∙ 기업들은 AI 개발자 플랫폼 투자를 통해 소프트웨어 엔지니어링팀의 역량을 강화하고 지속적인 AI\\n통합과 개발을 추진하는 도구와 프로세스를 채택 필요\\n☞ 출처: Gartner, Gartner Says Generative AI will Require 80% of Engineering Workforce to Upskill Through 2027,\\n2024.10.03.\\n19\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 22, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='SPRi AI Brief |\\n2024-11월호\\n인디드 조사 결과, 생성AI가 인간 근로자 대체할 가능성은 희박\\nKEY Contents\\nn 인디드가 2,800개 이상의 직무 기술에 대한 생성AI의 수행 능력을 분석해 인간을 대체할\\n가능성을 평가한 결과, 생성AI로 대체될 가능성이 “매우 높은” 것으로 평가된 기술은 전무\\nn 생성AI의 최대 강점은 직무 기술과 관련된 이론적 지식을 제공하는 능력이며, 물리적 작업\\n수행이 필요한 직무 기술에서는 인간 근로자를 대체할 가능성이 희박\\n£생성AI, 문제 해결 역량 및 물리적 작업 수행 역량의 부족으로 인간 근로자 대체에 한계\\nn 미국의 채용 플랫폼 인디드(Indeed) 산하 연구소 하이어링랩(Hiring Lab)이 2024년 9월 25일\\n발표한 연구 결과에 따르면 생성AI가 인간 근로자를 대체할 가능성은 희박\\n∙ 인디드 하이어링랩은 오픈AI의 GPT-4o로 2,800개 이상의 고유한 직무 기술에 대한 생성AI의 수행\\n능력을 분석해 생성AI가 인간을 대체할 가능성을 평가\\n∙ 연구진은 오픈AI의 GPT-4o가 △기술과 관련된 이론적 지식의 제공 역량 △기술을 사용한 문제 해결 역량\\n△기술 활용 시 물리적 작업의 중요성에 관한 판단 능력의 3개 차원에서 자체 수행 능력을 평가하도록 진행\\n∙ 다섯 가지 선택지(매우 낮음, 낮음, 보통, 높음, 매우 높음)로 평가 결과, 인디드가 평가 대상으로 삼은\\n2,800개 이상의 직무 기술 중 68.7%는 생성AI로 대체될 가능성이 “매우 낮음” 또는 “낮음”으로\\n평가됐으며, “매우 높음”으로 평가된 기술은 전무\\nn 생성AI는 직무 기술의 이론적 지식을 제공하는 자체 능력을 다소 높게 평가했으나, 문제 해결\\n능력 및 물리적 작업의 중요성에 관한 판단 능력은 상대적으로 낮게 평가\\n∙ 생성AI는 직무 기술 중 79.7%에 이론적 지식의 제공 능력을 4점(높음)으로, 기술 중 70.7%에 문제\\n해결 역량을 3점(보통)으로 평가했으며, 기술 중 54%에 대하여 물리적 작업의 필요성이 “높음” 또는\\n“매우 높음”이라고 평가*\\n* 매우 낮음(very unlikely 1점), 낮음(unlikely, 2점), 보통(possible, 3점), 높음(likely, 4점), 매우 높음(very likely, 5점)\\n∙ 생성AI는 물리적 작업을 수행할 몸체가 없어 실제 작업 수행이 필요한 직무 기술에서는 인간 근로자를\\n대체할 가능성이 제한적\\n∙ 일례로 생성AI는 디지털 기술 비중이 큰 소프트웨어 개발 직종의 구인 공고에서 통상 제시되는 직무\\n기술의 71%에 대하여 인간을 대체할 가능성이 “보통” 또는 “높음”으로 평가했으나, 간호사 직종의\\n구인 공고에 제시되는 기술의 약 32.9%만 생성AI로 대체될 가능성이 “보통” 또는 “높음”으로 평가\\nn 인디드는 현재 생성AI의 최대 강점은 직무 기술과 관련된 이론적 지식을 제공하는 능력이라고 강조\\n∙ 생성AI는 직원 생산성을 극대화하여 노동 시장의 경색을 완화할 수 있으며, 물리적 작업 수행이 필요한\\n직업에서도 근로자가 핵심 업무에 집중할 수 있도록 지원 가능\\n∙ 그러나 생성AI는 논리적 오류나 사실과 다른 내용 또는 편향이나 차별과 같은 비윤리적 응답을 출력할\\n가능성도 있으므로 인간의 신중한 검토 필요\\n☞ 출처: Indeed Hiring Lab, AI at Work: Why GenAI Is More Likely To Support Workers Than Replace Them, 2024.09.25.\\n20\\n'),\n",
       " Document(metadata={'source': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'file_path': './data/SPRi AI Brief_11월호_산업동향_F.pdf', 'page': 23, 'total_pages': 25, 'Author': 'dj', 'Creator': 'Hwp 2018 10.0.0.13947', 'Producer': 'Hancom PDF 1.3.0.547', 'CreationDate': \"D:20241105150426+09'00'\", 'ModDate': \"D:20241105150426+09'00'\", 'PDFVersion': '1.4', 'filename': './data/SPRi AI Brief_11월호_산업동향_F.pdf'}, page_content='Ⅱ\\n. 주요 행사 일정\\n행사명 행사 주요 개요\\n- 신경정보처리시스템재단은 인공지능과 머신러닝 분야의 연구 성과\\n교환을 촉진하는 것을 목적으로 하는 비영리 법인으로 매년 학제간\\n학술대회(NeurIPS)를 주최\\n- 이번 제38회 연례학술대회는 AI 연구자를 위한 실험 설계,\\nNeurIPS\\nLLM을 위한 메타 생성 알고리즘, 정렬에 대한 학제 간 통찰력\\n2024\\n등을 다룰 예정\\n기간 장소 홈페이지\\n2024.12.10~15 캐나다 밴쿠버 https://neurips.cc/\\n- GenAI Summit Maroc 2024는 인공지능과 데이터 분석에\\n초점을 맞춘 최고의 이벤트로, 250명 이상의 업계 리더, 정책\\nGenAI\\n입안자, 전문가가 모여 AI 발전을 탐구\\nSummit\\n- 이번 행사에는 오픈소스 AI, AI 주도 사이버 보안, 우수한\\nMaroc\\n의사결정을 위한 생성AI와 예측 AI 결합 등을 다룰 예정\\n2024\\n기간 장소 홈페이지\\n2024.12.10~11 모로코 https://genaimaroc.com/\\n- AI Summit Seoul 행사는 2018년 개최를 시작으로 금년도는\\n7회 행사로 개최\\n- 이번 행사는 AI와 산업의 융합에 초점을 두고 다양한 글로벌\\n기업과 기관, 학계 전문가 등 전문가들이 한자리에 모여 AI\\nAI Summit\\n및 산업 트렌드 등에 대한 주제 발표 및 워크샵 진행\\nSeoul 2024\\n기간 장소 홈페이지\\n2024.12.10~11 서울(코엑스 그랜드볼룸) https://aisummit.co.kr/\\n21\\n')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q ragas==0.1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context, conditional\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset.extractor import KeyphraseExtractor\n",
    "from ragas.testset.docstore import InMemoryDocumentStore\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성기\n",
    "generator_llm = ChatOpenAI(model = 'gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가를 할 수 있는 비평기\n",
    "critic_llm = ChatOpenAI(model = 'gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델\n",
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-3-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할기\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_llm = LangchainLLMWrapper(ChatOpenAI(model = 'gpt-4o'))\n",
    "# 구문추출기 생성을 위한 모델 호출(RAGAS와의 호환을 위한 Wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구문추출기 : 문서 핵심 정보 식별 및 추출 역할\n",
    "Keyphrase_extractor = KeyphraseExtractor(llm = langchain_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 모델도 RAGAS와 호환을 위해 Wrapper 적용\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장소, docstore 만들기\n",
    "docstore = InMemoryDocumentStore(\n",
    "    splitter = splitter,\n",
    "    embeddings = ragas_embeddings,\n",
    "    extractor = Keyphrase_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성기 만들기\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm, # 생성기\n",
    "    critic_llm,  # 판별기\n",
    "    ragas_embeddings, # 임베딩모델\n",
    "    docstore=docstore # 문서저장소\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 질문 유형 분포를 설정하는 딕셔너리입니다.\n",
    "# # 각 질문 유형과 그에 해당하는 비율(0~1)을 정의합니다.\n",
    "# distributions = {\n",
    "#     \"simple\": 0.4,          # 단순한 질문 유형의 비율 (전체의 40%)\n",
    "#     \"reasoning\": 0.2,       # 논리적 사고가 필요한 질문 유형의 비율 (전체의 20%)\n",
    "#     \"multi_context\": 0.2,   # 여러 문맥을 고려해야 하는 질문 유형의 비율 (전체의 20%)\n",
    "#     \"conditional\": 0.2      # 조건에 따라 답변이 달라지는 질문 유형의 비율 (전체의 20%)\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 유형 분포 결정\n",
    "distributions = {simple : 0.4, reasoning : 0.2, multi_context : 0.2, conditional : 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "embedding nodes:   9%|▊         | 7/82 [00:18<00:05, 12.64it/s][ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Molmo-72B', 'Benchmark scores', 'GPT-4o', 'MolmoE-1B', 'Allen Institute for AI']}\n",
      "embedding nodes:  11%|█         | 9/82 [00:24<05:08,  4.22s/it][ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Negative response rate', 'Automation in the workplace', 'Employee perception', 'Workplace satisfaction', 'Technological adaptation']}\n",
      "embedding nodes:  16%|█▌        | 13/82 [00:29<02:38,  2.29s/it]Task exception was never retrieved\n",
      "future: <Task finished name='Task-12249' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12245' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12246' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12242' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12253' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12258' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12256' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12255' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12264' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12270' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12274' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12266' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12268' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12263' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12271' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12278' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12277' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12279' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12284' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12287' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12281' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['AlphaChip', 'Google Deepmind', 'ARM collaboration', 'MediaTek Dimensity Flagship 5G', 'Smartphone and medical equipment applications']}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12283' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "embedding nodes:  17%|█▋        | 14/82 [00:30<02:12,  1.95s/it]Task exception was never retrieved\n",
      "future: <Task finished name='Task-12289' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12290' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12293' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12299' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12298' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12306' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12305' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12310' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12311' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12314' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12317' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12319' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12320' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12321' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12322' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12323' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12296' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-12308' coro=<as_completed.<locals>.sema_coro() done, defined at c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:32> exception=NotFoundError(\"Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 34, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 61, in wrapped_callable_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py\", line 55, in wrapped_callable_async\n",
      "    result = await callable(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 98, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py\", line 398, in <lambda>\n",
      "    self._add_action_func(lambda rs: rs.outcome.result())\n",
      "                                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py\", line 180, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 787, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 747, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py\", line 277, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 923, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 843, in _agenerate\n",
      "    response = await self.async_client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1661, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1843, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1537, in request\n",
      "    return await self._request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py\", line 1638, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-3.5` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "embedding nodes:  18%|█▊        | 15/82 [00:30<01:44,  1.55s/it][ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['AI 전문가', '기업에서의 AI 도구 사용', 'AI 프로젝트의 전략적 가치', 'AI의 영향', '기술 전문가의 AI 사용 빈도']}\n",
      "embedding nodes:  20%|█▉        | 16/82 [00:35<02:44,  2.49s/it][ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Kakao group chat', 'KakaoTalk app', 'AI model development', 'Kakao ASI (AI Safety Initiative)', 'AI technology and operation']}\n",
      "embedding nodes:  22%|██▏       | 18/82 [00:46<03:36,  3.38s/it][ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['AI 시장의 경쟁 촉진', '정부 데이터와 지식재산권 보호', '공급업체 시장에서 강력한 경쟁', 'AI 기술환경의 위험관리', 'AI 투자 식별 및 우선순위 지정']}\n",
      "embedding nodes:  26%|██▌       | 21/82 [00:53<02:17,  2.26s/it][ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['엔지니어링', '데이터 과학', 'AI/ML(머신러닝)', 'AI 엔지니어', 'AI 개발자 플랫폼 투자']}\n",
      "embedding nodes:  27%|██▋       | 22/82 [00:54<02:02,  2.04s/it][ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Mistral 8B', 'Llama 3.1 8B', 'AI model evaluation', 'HumanEval pass@1', 'Benchmark comparison']}\n",
      "embedding nodes:  33%|███▎      | 27/82 [00:59<00:46,  1.18it/s][ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['메커니즘 데이터 정리', '입력 데이터 간 연관성', 'AI 에이전트', 'LLM 신뢰성', 'AI 개발자 사이 인기']}\n",
      "embedding nodes:  34%|███▍      | 28/82 [01:28<08:35,  9.54s/it][ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Global AI challenges', 'AI development priorities', 'AI safety and reliability', 'AI impact on labor market', 'International AI research collaboration']}\n",
      "                                                                \r"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 30000, Used 30000, Requested 936. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_debugging_logs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 로그 활성화\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 예외가 발생해도 계속 진행, 예외 발생 시 문서가 처리 되지 않거나 문서에 기록됨\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\generator.py:206\u001b[0m, in \u001b[0;36mTestsetGenerator.generate_with_langchain_docs\u001b[1;34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[0m\n\u001b[0;32m    204\u001b[0m distributions \u001b[38;5;241m=\u001b[39m distributions \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_langchain_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    211\u001b[0m     test_size\u001b[38;5;241m=\u001b[39mtest_size,\n\u001b[0;32m    212\u001b[0m     distributions\u001b[38;5;241m=\u001b[39mdistributions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mrun_config,\n\u001b[0;32m    217\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\docstore.py:214\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_documents\u001b[1;34m(self, docs, show_progress)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[0;32m    210\u001b[0m nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    211\u001b[0m     Node\u001b[38;5;241m.\u001b[39mfrom_langchain_document(d)\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39mtransform_documents(docs)\n\u001b[0;32m    213\u001b[0m ]\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\docstore.py:251\u001b[0m, in \u001b[0;36mInMemoryDocumentStore.add_nodes\u001b[1;34m(self, nodes, show_progress)\u001b[0m\n\u001b[0;32m    244\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    245\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mextract,\n\u001b[0;32m    246\u001b[0m             n,\n\u001b[0;32m    247\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyphrase-extraction[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    248\u001b[0m         )\n\u001b[0;32m    249\u001b[0m         result_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 251\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:118\u001b[0m, in \u001b[0;36mExecutor.results\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 118\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_aresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m sorted_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(results, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:113\u001b[0m, in \u001b[0;36mExecutor.results.<locals>._aresults\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m    106\u001b[0m     futures_as_they_finish,\n\u001b[0;32m    107\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress,\n\u001b[0;32m    112\u001b[0m ):\n\u001b[1;32m--> 113\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[0;32m    114\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py:615\u001b[0m, in \u001b[0;36mas_completed.<locals>._wait_for_one\u001b[1;34m()\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTimeoutError\n\u001b[1;32m--> 615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:34\u001b[0m, in \u001b[0;36mas_completed.<locals>.sema_coro\u001b[1;34m(coro)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msema_coro\u001b[39m(coro):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[1;32m---> 34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:61\u001b[0m, in \u001b[0;36mExecutor.wrap_callable_with_index.<locals>.wrapped_callable_async\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraise_exceptions:\n\u001b[1;32m---> 61\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     63\u001b[0m         exec_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\executor.py:55\u001b[0m, in \u001b[0;36mExecutor.wrap_callable_with_index.<locals>.wrapped_callable_async\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetriesExceeded \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# this only for testset generation v2\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax retries exceeded for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mevolution\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\testset\\extractor.py:49\u001b[0m, in \u001b[0;36mKeyphraseExtractor.extract\u001b[1;34m(self, node, is_async)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract\u001b[39m(\u001b[38;5;28mself\u001b[39m, node: Node, is_async: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mList[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m     48\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor_prompt\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mnode\u001b[38;5;241m.\u001b[39mpage_content)\n\u001b[1;32m---> 49\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate(prompt\u001b[38;5;241m=\u001b[39mprompt, is_async\u001b[38;5;241m=\u001b[39mis_async)\n\u001b[0;32m     50\u001b[0m     keyphrases \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m json_loader\u001b[38;5;241m.\u001b[39msafe_load(\n\u001b[0;32m     51\u001b[0m         results\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip(), llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, is_async\u001b[38;5;241m=\u001b[39mis_async\n\u001b[0;32m     52\u001b[0m     )\n\u001b[0;32m     53\u001b[0m     keyphrases \u001b[38;5;241m=\u001b[39m keyphrases \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(keyphrases, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py:98\u001b[0m, in \u001b[0;36mBaseRagasLLM.generate\u001b[1;34m(self, prompt, n, temperature, stop, callbacks, is_async)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_async:\n\u001b[0;32m     95\u001b[0m     agenerate_text_with_retry \u001b[38;5;241m=\u001b[39m add_async_retry(\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_config\n\u001b[0;32m     97\u001b[0m     )\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m agenerate_text_with_retry(\n\u001b[0;32m     99\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    100\u001b[0m         n\u001b[38;5;241m=\u001b[39mn,\n\u001b[0;32m    101\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    102\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    103\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    104\u001b[0m     )\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189\u001b[0m, in \u001b[0;36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    188\u001b[0m async_wrapped\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153\u001b[0m, in \u001b[0;36mAsyncRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    151\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[1;32m--> 153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\_utils.py:99\u001b[0m, in \u001b[0;36mwrap_to_async_func.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs: typing\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: typing\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py:418\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[1;34m(rs)\u001b[0m\n\u001b[0;32m    416\u001b[0m retry_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_error_cls(fut)\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\__init__.py:185\u001b[0m, in \u001b[0;36mRetryError.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mNoReturn:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_attempt\u001b[38;5;241m.\u001b[39mfailed:\n\u001b[1;32m--> 185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_attempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114\u001b[0m, in \u001b[0;36mAsyncRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    116\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\ragas\\llms\\base.py:180\u001b[0m, in \u001b[0;36mLangchainLLMWrapper.agenerate_text\u001b[1;34m(self, prompt, n, temperature, stop, callbacks)\u001b[0m\n\u001b[0;32m    177\u001b[0m     temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_temperature(n\u001b[38;5;241m=\u001b[39mn)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_multiple_completion_supported(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangchain_llm):\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangchain_llm\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[0;32m    181\u001b[0m         prompts\u001b[38;5;241m=\u001b[39m[prompt],\n\u001b[0;32m    182\u001b[0m         n\u001b[38;5;241m=\u001b[39mn,\n\u001b[0;32m    183\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    184\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    185\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlangchain_llm\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[0;32m    189\u001b[0m         prompts\u001b[38;5;241m=\u001b[39m[prompt] \u001b[38;5;241m*\u001b[39m n,\n\u001b[0;32m    190\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    191\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    192\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    193\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:787\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    781\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    785\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    786\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[0;32m    788\u001b[0m         prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    789\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:747\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    735\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    736\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m    737\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m             ]\n\u001b[0;32m    746\u001b[0m         )\n\u001b[1;32m--> 747\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    748\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    749\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    751\u001b[0m ]\n\u001b[0;32m    752\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\asyncio\\tasks.py:277\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:923\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 923\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[0;32m    924\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    925\u001b[0m         )\n\u001b[0;32m    926\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    927\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:843\u001b[0m, in \u001b[0;36mBaseChatOpenAI._agenerate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    841\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 843\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result, response, generation_info\n\u001b[0;32m    846\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:1661\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m   1620\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1658\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m   1659\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[0;32m   1660\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m-> 1661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m   1662\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1663\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m   1664\u001b[0m             {\n\u001b[0;32m   1665\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m   1666\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m   1667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[0;32m   1668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m   1669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m   1670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m   1671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m   1672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m   1673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[0;32m   1674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m   1675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m   1676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[0;32m   1677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m   1678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m   1679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[0;32m   1680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m   1681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m   1682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m   1683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m   1684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m   1685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[0;32m   1686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m   1687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m   1688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m   1689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m   1690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m   1691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m   1692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m   1693\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m   1694\u001b[0m             },\n\u001b[0;32m   1695\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m   1696\u001b[0m         ),\n\u001b[0;32m   1697\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   1698\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m   1699\u001b[0m         ),\n\u001b[0;32m   1700\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m   1701\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1702\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m   1703\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py:1843\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1829\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1830\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1831\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1838\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1839\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[0;32m   1840\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1841\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1842\u001b[0m     )\n\u001b[1;32m-> 1843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py:1537\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1535\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1538\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1539\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1540\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1541\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1542\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1543\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py:1623\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m-> 1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1624\u001b[0m         input_options,\n\u001b[0;32m   1625\u001b[0m         cast_to,\n\u001b[0;32m   1626\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1627\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1628\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1629\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1630\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1666\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1676\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py:1623\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1622\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maclose()\n\u001b[1;32m-> 1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry_request(\n\u001b[0;32m   1624\u001b[0m         input_options,\n\u001b[0;32m   1625\u001b[0m         cast_to,\n\u001b[0;32m   1626\u001b[0m         retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1627\u001b[0m         response_headers\u001b[38;5;241m=\u001b[39merr\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m   1628\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1629\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1630\u001b[0m     )\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py:1670\u001b[0m, in \u001b[0;36mAsyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1666\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1671\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1672\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1673\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1674\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1675\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1676\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py:1638\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[0;32m   1635\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[0;32m   1637\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1638\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1641\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1642\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1646\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1647\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 30000, Used 30000, Requested 936. Please try again in 1.872s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['해결 역량', '물리적 작업', '생성AI', '인간 근로자 대체 가능성', '디지털 기술']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['NeurIPS 2024', 'GenAI Summit Maroc 2024', 'AI Summit Seoul', 'Artificial Intelligence', 'Data Analysis']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Generative AI governance', 'Diverse stakeholders', 'AI accountability', 'International cooperation', 'AI safety research']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Data collection standardization', 'AI model development', 'Data privacy and human rights', 'EU AI law', 'High-risk AI systems']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['DeepMind CEO John Jumper', 'AI-based protein structure prediction', 'AlphaFold 2', 'Nobel Prize in Chemistry 2024', 'Protein folding problem']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['AI 거버넌스 프레임워크', 'G7 회원국', 'AI 관리 구조', '공공 부문 AI 도입', '데이터 저장과 공유 솔루션']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['AI startups', 'M&A activity', 'Harvey', 'Nvidia', 'Salesforce', 'CB Insights']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Meta Movie Gen', 'AI-enabled era', 'Content creators', \"Runway's Gen 3\", 'Sora AI']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Llama 3.2 series', 'GPT-4o-mini', 'Open-rewrite eval', 'Llama Stack', 'Edge AI and vision']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['AI 안전성 평가', '개인정보 보호', 'LLM 시스템', '데이터 유출 방지', '기술적 설명 가능성']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Global AI Research Agenda (GAIRA)', 'United States Department of State', 'AI R&D principles', 'International cooperation', 'AI system development']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['AlphaChip', 'Google Brain', 'AI model', 'Chip layout design', 'Tensor Processing Unit (TPU)']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['AI 전문가', '기술 전문가', '직업 전망', '미국 기술직 채용 플랫폼', 'AI 도구 사용']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['AI governance', 'U.S. Office of Management and Budget', 'AI safety and performance management', 'AI market competition', 'Government collaboration with public enterprises']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['생성AI 거버넌스 프레임워크', '세계경제포럼', '경제·사회적 균형 달성', '기존 규제 평가', '다양한 이해관계자 참여']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['OECD AI initiatives', 'G7 toolkit for AI', 'Public sector AI adoption', 'AI governance framework', 'AI policy development']}\n",
      "[ragas.testset.extractor.DEBUG] topics: {'keyphrases': ['Japan AI Safety Institute', 'AI safety evaluation', 'AI safety guidelines', 'Core elements of AI safety', 'Human-centered AI']}\n"
     ]
    }
   ],
   "source": [
    "testset = generator.generate_with_langchain_docs(\n",
    "    documents = docs,\n",
    "    test_size = 10,\n",
    "    distributions = distributions,\n",
    "    with_debugging_logs = True,\n",
    "    # 로그 활성화\n",
    "    raise_exceptions= False\n",
    "    # 예외가 발생해도 계속 진행, 예외 발생 시 문서가 처리 되지 않거나 문서에 기록됨\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = testset.to_pandas()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 셋이 나오지 않았기 때문에 우선 csv 파일을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\RMARKET\\workspace\\5주차\\data\\ragas_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What specific aspects of data collection stand...</td>\n",
       "      <td>['1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n유로폴, 법 ...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/SPRi AI Brief_11월호_산업동향_F.pd...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What factors are contributing to the recent in...</td>\n",
       "      <td>['SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전...</td>\n",
       "      <td>The recent increase in popularity of the Mamba...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/SPRi AI Brief_11월호_산업동향_F.pd...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the MIT Industrial Performance Center...</td>\n",
       "      <td>['1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\nMIT 산업성...</td>\n",
       "      <td>The MIT Industrial Performance Center assesses...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/SPRi AI Brief_11월호_산업동향_F.pd...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What role does AI play in addressing global ch...</td>\n",
       "      <td>['∙ (글로벌 도전과제 해결) 환경 문제, 경제 회복력, 사회복지 등 글로벌 도전...</td>\n",
       "      <td>AI helps in solving global challenges such as ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'data/SPRi AI Brief_11월호_산업동향_F.pd...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do Transformers boost AI21's model efficie...</td>\n",
       "      <td>['SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전...</td>\n",
       "      <td>Transformers are used extensively in AI model ...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'data/SPRi AI Brief_11월호_산업동향_F.pd...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What specific aspects of data collection stand...   \n",
       "1  What factors are contributing to the recent in...   \n",
       "2  How does the MIT Industrial Performance Center...   \n",
       "3  What role does AI play in addressing global ch...   \n",
       "4  How do Transformers boost AI21's model efficie...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  ['1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n유로폴, 법 ...   \n",
       "1  ['SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전...   \n",
       "2  ['1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\nMIT 산업성...   \n",
       "3  ['∙ (글로벌 도전과제 해결) 환경 문제, 경제 회복력, 사회복지 등 글로벌 도전...   \n",
       "4  ['SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  The answer to given question is not present in...         simple   \n",
       "1  The recent increase in popularity of the Mamba...         simple   \n",
       "2  The MIT Industrial Performance Center assesses...         simple   \n",
       "3  AI helps in solving global challenges such as ...         simple   \n",
       "4  Transformers are used extensively in AI model ...      reasoning   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': 'data/SPRi AI Brief_11월호_산업동향_F.pd...          True  \n",
       "1  [{'source': 'data/SPRi AI Brief_11월호_산업동향_F.pd...          True  \n",
       "2  [{'source': 'data/SPRi AI Brief_11월호_산업동향_F.pd...          True  \n",
       "3  [{'source': 'data/SPRi AI Brief_11월호_산업동향_F.pd...          True  \n",
       "4  [{'source': 'data/SPRi AI Brief_11월호_산업동향_F.pd...          True  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'contexts', 'ground_truth', 'evolution_type', 'metadata', 'episode_done'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\\\n유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간\\\\nKEY Contents\\\\nn 유로폴의 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오\\\\n분석 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\\\nn 그러나 AI 도입을 위해서는 기술적 과제 해결 및 다양한 윤리적·사회적 이슈 대응이\\\\n필요하며, EU AI 법에 부합하도록 기존 AI 시스템에 대한 평가와 수정도 필요\\\\n£유로폴, 법 집행에서 AI 기술의 윤리적이고 투명한 구현을 위한 고려사항 제시\\\\nn EU 사법기관 유로폴(Europol)이 2024년 9월 24일 법 집행에서 효과적 범죄 퇴치를 위한 AI의\\\\n활용 가능성을 탐색한 보고서를 발간\\\\n∙ 보고서는 법 집행에서 AI 기술을 윤리적이고 투명하게 구현하기 위한 지침 역할을 하며, AI의 이점과\\\\n과제를 함께 다룸으로써 법 집행에서 AI 사용 시 윤리적 고려 사항에 대한 인식 제고를 추구\\\\nn 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오 분석, 생체인식\\\\n시스템 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\\\n∙ 법 집행기관은 AI 기반 데이터 분석을 활용해 범죄 활동에 대한 탐지와 대응 능력을 강화하고, AI\\\\n도구로 구조화되지 않은 데이터를 신속히 분석해 비상 상황의 의사결정을 위한 통찰력 확보 가능\\\\n∙ 기계번역과 같은 AI 기반 도구는 여러 국가가 참여하는 조사에서 원활한 국제협력을 위해서도 필수적\\\\nn 그러나 법 집행에서 AI 도구의 효과적이고 책임 있는 활용을 위해 해결되어야 할 기술적 과제 및\\\\n다양한 윤리적·사회적 우려도 존재\\\\n∙ 일례로 관할권 간 데이터 수집과 보관 관행의 차이에 따른 데이터셋의 편향으로 인해 AI 산출물의\\\\n무결성(無缺性)이 손상될 수 있어 표준화된 데이터 수집 규약 필요\\\\n∙ 데이터 규모나 활용 사례의 복잡성과 관계없이 AI 도구를 효과적으로 사용하려면 다양한 데이터', '무결성(無缺性)이 손상될 수 있어 표준화된 데이터 수집 규약 필요\\\\n∙ 데이터 규모나 활용 사례의 복잡성과 관계없이 AI 도구를 효과적으로 사용하려면 다양한 데이터\\\\n규모와 운영 요구사항에 적응할 수 있는 확장성과 성능을 갖춘 AI 모델도 개발 필요\\\\n∙ 편향, 개인정보 침해와 인권 침해와 같은 다양한 윤리적·사회적 우려도 존재하며, 이를 해소하기\\\\n위해 데이터 편향을 제거하고 공공 안전과 개인정보 간 균형을 유지하며 AI 의사 결정 과정에\\\\n대한 투명성과 책임성을 보장 필요\\\\nn 보고서는 2024년 8월 발효된 EU AI 법이 법 집행기관에 미칠 영향도 분석\\\\n∙ EU AI 법은 공공장소에서 실시간 생체인식 식별과 같은 특정 애플리케이션의 사용을 금지하고\\\\n고위험 AI 시스템에 엄격한 감독을 부과하였으나 법 집행 활동의 특수성을 고려해 일부 예외를 설정\\\\n∙ 그러나 일부 예외에도 법 집행 역량 강화를 위한 AI 사용을 위해서는 기존에 도입한 AI\\\\n시스템에 대한 재평가와 수정이 필요한 만큼, 재정과 인력 측면의 상당한 부담 예상\\\\n☞ 출처: Europol, AI and policing-The benefits and challenges of artificial intelligence for law enforcement, 2024.09.24.\\\\n3', '1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\\\n유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간\\\\nKEY Contents\\\\nn 유로폴의 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오\\\\n분석 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\\\nn 그러나 AI 도입을 위해서는 기술적 과제 해결 및 다양한 윤리적·사회적 이슈 대응이\\\\n필요하며, EU AI 법에 부합하도록 기존 AI 시스템에 대한 평가와 수정도 필요\\\\n£유로폴, 법 집행에서 AI 기술의 윤리적이고 투명한 구현을 위한 고려사항 제시\\\\nn EU 사법기관 유로폴(Europol)이 2024년 9월 24일 법 집행에서 효과적 범죄 퇴치를 위한 AI의\\\\n활용 가능성을 탐색한 보고서를 발간\\\\n∙ 보고서는 법 집행에서 AI 기술을 윤리적이고 투명하게 구현하기 위한 지침 역할을 하며, AI의 이점과\\\\n과제를 함께 다룸으로써 법 집행에서 AI 사용 시 윤리적 고려 사항에 대한 인식 제고를 추구\\\\nn 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오 분석, 생체인식\\\\n시스템 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\\\n∙ 법 집행기관은 AI 기반 데이터 분석을 활용해 범죄 활동에 대한 탐지와 대응 능력을 강화하고, AI\\\\n도구로 구조화되지 않은 데이터를 신속히 분석해 비상 상황의 의사결정을 위한 통찰력 확보 가능\\\\n∙ 기계번역과 같은 AI 기반 도구는 여러 국가가 참여하는 조사에서 원활한 국제협력을 위해서도 필수적\\\\nn 그러나 법 집행에서 AI 도구의 효과적이고 책임 있는 활용을 위해 해결되어야 할 기술적 과제 및\\\\n다양한 윤리적·사회적 우려도 존재\\\\n∙ 일례로 관할권 간 데이터 수집과 보관 관행의 차이에 따른 데이터셋의 편향으로 인해 AI 산출물의\\\\n무결성(無缺性)이 손상될 수 있어 표준화된 데이터 수집 규약 필요\\\\n∙ 데이터 규모나 활용 사례의 복잡성과 관계없이 AI 도구를 효과적으로 사용하려면 다양한 데이터']\""
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['contexts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_to_list(example):\n",
    "    contexts = ast.literal_eval(example['contexts'])\n",
    "    return {'contexts':contexts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 303.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = test_dataset.map(convert_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간\\nKEY Contents\\nn 유로폴의 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오\\n분석 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\nn 그러나 AI 도입을 위해서는 기술적 과제 해결 및 다양한 윤리적·사회적 이슈 대응이\\n필요하며, EU AI 법에 부합하도록 기존 AI 시스템에 대한 평가와 수정도 필요\\n£유로폴, 법 집행에서 AI 기술의 윤리적이고 투명한 구현을 위한 고려사항 제시\\nn EU 사법기관 유로폴(Europol)이 2024년 9월 24일 법 집행에서 효과적 범죄 퇴치를 위한 AI의\\n활용 가능성을 탐색한 보고서를 발간\\n∙ 보고서는 법 집행에서 AI 기술을 윤리적이고 투명하게 구현하기 위한 지침 역할을 하며, AI의 이점과\\n과제를 함께 다룸으로써 법 집행에서 AI 사용 시 윤리적 고려 사항에 대한 인식 제고를 추구\\nn 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오 분석, 생체인식\\n시스템 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\n∙ 법 집행기관은 AI 기반 데이터 분석을 활용해 범죄 활동에 대한 탐지와 대응 능력을 강화하고, AI\\n도구로 구조화되지 않은 데이터를 신속히 분석해 비상 상황의 의사결정을 위한 통찰력 확보 가능\\n∙ 기계번역과 같은 AI 기반 도구는 여러 국가가 참여하는 조사에서 원활한 국제협력을 위해서도 필수적\\nn 그러나 법 집행에서 AI 도구의 효과적이고 책임 있는 활용을 위해 해결되어야 할 기술적 과제 및\\n다양한 윤리적·사회적 우려도 존재\\n∙ 일례로 관할권 간 데이터 수집과 보관 관행의 차이에 따른 데이터셋의 편향으로 인해 AI 산출물의\\n무결성(無缺性)이 손상될 수 있어 표준화된 데이터 수집 규약 필요\\n∙ 데이터 규모나 활용 사례의 복잡성과 관계없이 AI 도구를 효과적으로 사용하려면 다양한 데이터',\n",
       " '무결성(無缺性)이 손상될 수 있어 표준화된 데이터 수집 규약 필요\\n∙ 데이터 규모나 활용 사례의 복잡성과 관계없이 AI 도구를 효과적으로 사용하려면 다양한 데이터\\n규모와 운영 요구사항에 적응할 수 있는 확장성과 성능을 갖춘 AI 모델도 개발 필요\\n∙ 편향, 개인정보 침해와 인권 침해와 같은 다양한 윤리적·사회적 우려도 존재하며, 이를 해소하기\\n위해 데이터 편향을 제거하고 공공 안전과 개인정보 간 균형을 유지하며 AI 의사 결정 과정에\\n대한 투명성과 책임성을 보장 필요\\nn 보고서는 2024년 8월 발효된 EU AI 법이 법 집행기관에 미칠 영향도 분석\\n∙ EU AI 법은 공공장소에서 실시간 생체인식 식별과 같은 특정 애플리케이션의 사용을 금지하고\\n고위험 AI 시스템에 엄격한 감독을 부과하였으나 법 집행 활동의 특수성을 고려해 일부 예외를 설정\\n∙ 그러나 일부 예외에도 법 집행 역량 강화를 위한 AI 사용을 위해서는 기존에 도입한 AI\\n시스템에 대한 재평가와 수정이 필요한 만큼, 재정과 인력 측면의 상당한 부담 예상\\n☞ 출처: Europol, AI and policing-The benefits and challenges of artificial intelligence for law enforcement, 2024.09.24.\\n3',\n",
       " '1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n유로폴, 법 집행에서 AI의 이점과 과제를 다룬 보고서 발간\\nKEY Contents\\nn 유로폴의 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오\\n분석 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\nn 그러나 AI 도입을 위해서는 기술적 과제 해결 및 다양한 윤리적·사회적 이슈 대응이\\n필요하며, EU AI 법에 부합하도록 기존 AI 시스템에 대한 평가와 수정도 필요\\n£유로폴, 법 집행에서 AI 기술의 윤리적이고 투명한 구현을 위한 고려사항 제시\\nn EU 사법기관 유로폴(Europol)이 2024년 9월 24일 법 집행에서 효과적 범죄 퇴치를 위한 AI의\\n활용 가능성을 탐색한 보고서를 발간\\n∙ 보고서는 법 집행에서 AI 기술을 윤리적이고 투명하게 구현하기 위한 지침 역할을 하며, AI의 이점과\\n과제를 함께 다룸으로써 법 집행에서 AI 사용 시 윤리적 고려 사항에 대한 인식 제고를 추구\\nn 보고서에 따르면 AI는 고급 데이터 분석, 디지털 증거 수집, 이미지와 비디오 분석, 생체인식\\n시스템 등에 활용되어 법 집행 업무를 대폭 개선할 수 있는 잠재력 보유\\n∙ 법 집행기관은 AI 기반 데이터 분석을 활용해 범죄 활동에 대한 탐지와 대응 능력을 강화하고, AI\\n도구로 구조화되지 않은 데이터를 신속히 분석해 비상 상황의 의사결정을 위한 통찰력 확보 가능\\n∙ 기계번역과 같은 AI 기반 도구는 여러 국가가 참여하는 조사에서 원활한 국제협력을 위해서도 필수적\\nn 그러나 법 집행에서 AI 도구의 효과적이고 책임 있는 활용을 위해 해결되어야 할 기술적 과제 및\\n다양한 윤리적·사회적 우려도 존재\\n∙ 일례로 관할권 간 데이터 수집과 보관 관행의 차이에 따른 데이터셋의 편향으로 인해 AI 산출물의\\n무결성(無缺性)이 손상될 수 있어 표준화된 데이터 수집 규약 필요\\n∙ 데이터 규모나 활용 사례의 복잡성과 관계없이 AI 도구를 효과적으로 사용하려면 다양한 데이터']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]['contexts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader('data/SPRi AI Brief_11월호_산업동향_F.pdf')\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)\n",
    "split_documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"너는 주어진 질문에 대답하는 AI야. 다음 검색된 context를 사용해서 question에 대답해줘.\n",
    "    답을 모르면, '알 수 없습니다.'라고 대답해.\n",
    "    \n",
    "    \n",
    "    # Context : {context}\n",
    "    # Question : {question}\n",
    "    # Answer :\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = 'gpt-4o-mini', temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {'context' : retriever, 'question' : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dataset = []\n",
    "for question in test_dataset['question']:\n",
    "    batch_dataset.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What specific aspects of data collection standardization, such as data format, quality control, or metadata standards, are required to effectively use AI tools, while ensuring ease of implementation and simplicity, without considering data size and utilization scenarios?',\n",
       " 'What factors are contributing to the recent increase in popularity of the Mamba AI framework among developers?',\n",
       " 'How does the MIT Industrial Performance Center assess the impact of automation technology on workers?']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['알 수 없습니다.',\n",
       " '최근 Mamba AI 프레임워크의 인기가 높아지는 이유는 다음과 같습니다:\\n\\n1. **효율적인 메모리 사용**: Mamba는 트랜스포머 모델의 핵심인 어텐션 메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여하여 메모리 사용을 최적화합니다. 이는 AI 에이전트의 효율성을 높이는 데 기여합니다.\\n\\n2. **빠른 추론 시간**: Mamba 아키텍처는 더 빠른 추론 시간을 지원하여 개발자들이 더 효율적으로 작업할 수 있도록 합니다.\\n\\n3. **오픈소스 개발자 사이의 관심**: 최근 미스트랄과 UAE의 AI 기업 팔콘이 Mamba 기반의 모델을 출시하면서 오픈소스 AI 개발자들 사이에서 Mamba의 인기가 높아지고 있습니다.\\n\\n이러한 요소들이 결합되어 Mamba AI 프레임워크의 인기를 증가시키고 있습니다.',\n",
       " 'MIT 산업성과센터는 설문조사를 통해 근로자 관점에서 자동화 기술의 영향을 평가한 결과, 근로자들은 직장 내 안전, 임금, 업무 자율성 등에서 자동화를 긍정적으로 평가하고 있다고 밝혔습니다. 조사에 따르면, 복잡한 문제 해결이 필요한 작업을 수행하는 근로자 및 자신의 직무에 만족하는 근로자일수록 자동화의 영향에 긍정적인 것으로 나타났습니다. 또한, 자동화가 직장 내 안전에 긍정적인 영향을 미친다고 응답한 비율이 44.9%에 달하며, 임금과 업무 자율성에 대해서도 긍정적인 응답이 우세한 것으로 확인되었습니다. 그러나 국가별로 차이가 있으며, 미국 근로자들은 자동화가 임금 및 직업 안정성에 부정적이라는 응답이 더 많았습니다.']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = chain.batch(batch_dataset)\n",
    "answer[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'answer' in test_dataset.column_names:\n",
    "    test_dataset = test_dataset.remove_columns(['answer']).add_column('answer', answer)\n",
    "else:\n",
    "    test_dataset = test_dataset.add_column('answer', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'contexts', 'ground_truth', 'evolution_type', 'metadata', 'episode_done', 'answer'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy,faithfulness,context_recall,context_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]Exception raised in Job[12]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 199446, Requested 1490. Please try again in 280ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:   2%|▎         | 1/40 [01:29<58:05, 89.37s/it]Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:   5%|▌         | 2/40 [01:45<29:10, 46.05s/it]Exception raised in Job[7]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 198479, Requested 2177. Please try again in 196ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:   8%|▊         | 3/40 [01:48<16:17, 26.41s/it]Exception raised in Job[31]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 199691, Requested 2189. Please try again in 564ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  10%|█         | 4/40 [01:54<11:00, 18.34s/it]Exception raised in Job[25]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  12%|█▎        | 5/40 [02:00<08:06, 13.91s/it]Exception raised in Job[29]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 1542. Please try again in 462ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  15%|█▌        | 6/40 [02:01<05:28,  9.67s/it]Exception raised in Job[5]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 199809, Requested 1474. Please try again in 384ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  18%|█▊        | 7/40 [02:11<05:17,  9.62s/it]Exception raised in Job[27]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Exception raised in Job[4]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 2758. Please try again in 827ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  22%|██▎       | 9/40 [02:18<03:28,  6.74s/it]Exception raised in Job[35]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 1869. Please try again in 560ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  25%|██▌       | 10/40 [02:22<03:01,  6.04s/it]Exception raised in Job[21]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  28%|██▊       | 11/40 [02:31<03:22,  6.98s/it]Exception raised in Job[6]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  30%|███       | 12/40 [02:32<02:23,  5.12s/it]Exception raised in Job[23]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 199946, Requested 2178. Please try again in 637ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  32%|███▎      | 13/40 [02:39<02:34,  5.73s/it]Exception raised in Job[38]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  35%|███▌      | 14/40 [02:51<03:18,  7.63s/it]Exception raised in Job[39]: TimeoutError()\n",
      "Exception raised in Job[36]: TimeoutError()\n",
      "Evaluating:  38%|███▊      | 15/40 [03:00<03:14,  7.79s/it]Exception raised in Job[17]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 199728, Requested 697. Please try again in 127ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  42%|████▎     | 17/40 [03:29<04:11, 10.94s/it]Exception raised in Job[28]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 3762. Please try again in 1.128s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  45%|████▌     | 18/40 [03:47<04:37, 12.61s/it]Exception raised in Job[16]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  48%|████▊     | 19/40 [03:58<04:19, 12.36s/it]Exception raised in Job[8]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 198889, Requested 2059. Please try again in 284ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  50%|█████     | 20/40 [04:00<03:09,  9.49s/it]Exception raised in Job[11]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  52%|█████▎    | 21/40 [04:01<02:13,  7.02s/it]Exception raised in Job[26]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 3985. Please try again in 1.195s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  55%|█████▌    | 22/40 [04:16<02:50,  9.47s/it]Exception raised in Job[10]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 2227. Please try again in 668ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  57%|█████▊    | 23/40 [04:24<02:33,  9.05s/it]Exception raised in Job[34]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 1922. Please try again in 576ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  60%|██████    | 24/40 [04:28<01:57,  7.35s/it]Exception raised in Job[0]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 3390. Please try again in 1.017s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  62%|██████▎   | 25/40 [04:32<01:38,  6.59s/it]Exception raised in Job[24]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  65%|██████▌   | 26/40 [04:36<01:18,  5.59s/it]Exception raised in Job[33]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  68%|██████▊   | 27/40 [04:41<01:13,  5.67s/it]Exception raised in Job[9]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 199049, Requested 1501. Please try again in 165ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  70%|███████   | 28/40 [04:46<01:03,  5.27s/it]Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 750. Please try again in 225ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  72%|███████▎  | 29/40 [05:27<02:57, 16.12s/it]Exception raised in Job[15]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 1807. Please try again in 542ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  75%|███████▌  | 30/40 [05:28<01:55, 11.59s/it]Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 3930. Please try again in 1.179s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  78%|███████▊  | 31/40 [05:38<01:38, 10.97s/it]Exception raised in Job[18]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 199780, Requested 2212. Please try again in 597ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 32/40 [05:51<01:33, 11.66s/it]Exception raised in Job[22]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per min (RPM): Limit 500, Used 500, Requested 1. Please try again in 120ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  82%|████████▎ | 33/40 [05:56<01:07,  9.69s/it]Exception raised in Job[14]: TimeoutError()\n",
      "Evaluating:  85%|████████▌ | 34/40 [06:00<00:46,  7.78s/it]Exception raised in Job[32]: TimeoutError()\n",
      "Evaluating:  88%|████████▊ | 35/40 [06:29<01:11, 14.27s/it]Exception raised in Job[30]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 198049, Requested 3925. Please try again in 592ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  90%|█████████ | 36/40 [06:32<00:43, 10.81s/it]Exception raised in Job[13]: TimeoutError()\n",
      "Evaluating:  92%|█████████▎| 37/40 [06:47<00:36, 12.07s/it]Exception raised in Job[20]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on tokens per min (TPM): Limit 200000, Used 200000, Requested 1904. Please try again in 571ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  95%|█████████▌| 38/40 [07:08<00:29, 14.81s/it]Exception raised in Job[37]: TimeoutError()\n",
      "Evaluating:  98%|█████████▊| 39/40 [07:16<00:12, 12.91s/it]Exception raised in Job[19]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-nS8RoafnzeRRZRKTRiJUJQQC on requests per day (RPD): Limit 10000, Used 10000, Requested 1. Please try again in 8.64s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 40/40 [07:17<00:00, 10.93s/it]\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset = test_dataset,\n",
    "    metrics = [\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_recall,\n",
    "        context_precision\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# answer_relevancy : 모델이 문맥에서 중요한 정보를 잘 회상, 재현했는가\n",
    "# faithfulness : 모델의 답변이 문맥에 기반해서 사실적인가(factual)\n",
    "# context_recall : 전체 질문에 대해 모델의 답변이 질문과 얼마나 관련성이 있는가\n",
    "# context_precision : 모델이 문맥에서 필요한 정보를 정확히 활용했는가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': nan, 'faithfulness': nan, 'context_recall': nan, 'context_precision': nan}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What specific aspects of data collection stand...</td>\n",
       "      <td>[1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n유로폴, 법 집...</td>\n",
       "      <td>알 수 없습니다.</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What factors are contributing to the recent in...</td>\n",
       "      <td>[SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전트...</td>\n",
       "      <td>최근 Mamba AI 프레임워크의 인기가 높아지는 이유는 다음과 같습니다:\\n\\n1...</td>\n",
       "      <td>The recent increase in popularity of the Mamba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does the MIT Industrial Performance Center...</td>\n",
       "      <td>[1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\nMIT 산업성과...</td>\n",
       "      <td>MIT 산업성과센터는 설문조사를 통해 근로자 관점에서 자동화 기술의 영향을 평가한 ...</td>\n",
       "      <td>The MIT Industrial Performance Center assesses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What role does AI play in addressing global ch...</td>\n",
       "      <td>[∙ (글로벌 도전과제 해결) 환경 문제, 경제 회복력, 사회복지 등 글로벌 도전 ...</td>\n",
       "      <td>AI plays a significant role in addressing glob...</td>\n",
       "      <td>AI helps in solving global challenges such as ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do Transformers boost AI21's model efficie...</td>\n",
       "      <td>[SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전트...</td>\n",
       "      <td>알 수 없습니다.</td>\n",
       "      <td>Transformers are used extensively in AI model ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How can investment boost AI platforms in compa...</td>\n",
       "      <td>[1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n가트너 예측, ...</td>\n",
       "      <td>기업의 AI 개발자 플랫폼에 대한 투자는 AI 엔지니어를 지원하고, AI 역량을 더...</td>\n",
       "      <td>Companies need to strengthen investment in AI ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How might the EU AI law impact data use for cr...</td>\n",
       "      <td>[1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n유로폴, 법 집...</td>\n",
       "      <td>EU AI 법은 공공장소에서 실시간 생체인식 식별과 같은 특정 애플리케이션의 사용을...</td>\n",
       "      <td>The EU AI law, effective from August 2024, imp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What limits Transformers in AI that require me...</td>\n",
       "      <td>[SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전트...</td>\n",
       "      <td>트랜스포머 아키텍처는 처리하는 컨텍스트가 길어질수록 속도가 느리고 연산 비용이 많이...</td>\n",
       "      <td>Transformers are limited in AI applications th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>If LLMs don't become more reliable, how could ...</td>\n",
       "      <td>[메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사...</td>\n",
       "      <td>If LLMs don't become more reliable, it could h...</td>\n",
       "      <td>If LLMs don't become more reliable, it could a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How does AI safety shift with data privacy pri...</td>\n",
       "      <td>[인간중심, 안전성\\n비의도적 사용 대처 미발생\\n개인정보 보호 프라이버시 보호 Ÿ...</td>\n",
       "      <td>알 수 없습니다.</td>\n",
       "      <td>The context does not provide specific informat...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What specific aspects of data collection stand...   \n",
       "1  What factors are contributing to the recent in...   \n",
       "2  How does the MIT Industrial Performance Center...   \n",
       "3  What role does AI play in addressing global ch...   \n",
       "4  How do Transformers boost AI21's model efficie...   \n",
       "5  How can investment boost AI platforms in compa...   \n",
       "6  How might the EU AI law impact data use for cr...   \n",
       "7  What limits Transformers in AI that require me...   \n",
       "8  If LLMs don't become more reliable, how could ...   \n",
       "9  How does AI safety shift with data privacy pri...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n유로폴, 법 집...   \n",
       "1  [SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전트...   \n",
       "2  [1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\nMIT 산업성과...   \n",
       "3  [∙ (글로벌 도전과제 해결) 환경 문제, 경제 회복력, 사회복지 등 글로벌 도전 ...   \n",
       "4  [SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전트...   \n",
       "5  [1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n가트너 예측, ...   \n",
       "6  [1. 정책/법제 2. 기업/산업 3. 기술/연구 4. 인력/교육\\n유로폴, 법 집...   \n",
       "7  [SPRi AI Brief |\\n2024-11월호\\nAI21 CEO, AI 에이전트...   \n",
       "8  [메커니즘 대신 데이터를 우선순위에 따라 정리하고 입력에 가중치를 부여해 메모리 사...   \n",
       "9  [인간중심, 안전성\\n비의도적 사용 대처 미발생\\n개인정보 보호 프라이버시 보호 Ÿ...   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                          알 수 없습니다.   \n",
       "1  최근 Mamba AI 프레임워크의 인기가 높아지는 이유는 다음과 같습니다:\\n\\n1...   \n",
       "2  MIT 산업성과센터는 설문조사를 통해 근로자 관점에서 자동화 기술의 영향을 평가한 ...   \n",
       "3  AI plays a significant role in addressing glob...   \n",
       "4                                          알 수 없습니다.   \n",
       "5  기업의 AI 개발자 플랫폼에 대한 투자는 AI 엔지니어를 지원하고, AI 역량을 더...   \n",
       "6  EU AI 법은 공공장소에서 실시간 생체인식 식별과 같은 특정 애플리케이션의 사용을...   \n",
       "7  트랜스포머 아키텍처는 처리하는 컨텍스트가 길어질수록 속도가 느리고 연산 비용이 많이...   \n",
       "8  If LLMs don't become more reliable, it could h...   \n",
       "9                                          알 수 없습니다.   \n",
       "\n",
       "                                        ground_truth  answer_relevancy  \\\n",
       "0  The answer to given question is not present in...               NaN   \n",
       "1  The recent increase in popularity of the Mamba...               NaN   \n",
       "2  The MIT Industrial Performance Center assesses...               NaN   \n",
       "3  AI helps in solving global challenges such as ...               NaN   \n",
       "4  Transformers are used extensively in AI model ...               NaN   \n",
       "5  Companies need to strengthen investment in AI ...               NaN   \n",
       "6  The EU AI law, effective from August 2024, imp...               NaN   \n",
       "7  Transformers are limited in AI applications th...               NaN   \n",
       "8  If LLMs don't become more reliable, it could a...               NaN   \n",
       "9  The context does not provide specific informat...               NaN   \n",
       "\n",
       "   faithfulness  context_recall  context_precision  \n",
       "0           NaN             NaN                NaN  \n",
       "1           NaN             NaN                NaN  \n",
       "2           NaN             NaN                NaN  \n",
       "3           NaN             NaN                NaN  \n",
       "4           NaN             NaN                NaN  \n",
       "5           NaN             NaN                NaN  \n",
       "6           NaN             NaN                NaN  \n",
       "7           NaN             NaN                NaN  \n",
       "8           NaN             NaN                NaN  \n",
       "9           NaN             NaN                NaN  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = result.to_pandas()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   context_recall  context_precision\n",
       "0             NaN                NaN\n",
       "1             NaN                NaN\n",
       "2             NaN                NaN\n",
       "3             NaN                NaN\n",
       "4             NaN                NaN\n",
       "5             NaN                NaN\n",
       "6             NaN                NaN\n",
       "7             NaN                NaN\n",
       "8             NaN                NaN\n",
       "9             NaN                NaN"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.loc[:, 'context_recall':'context_precision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|context_precision (문맥 정밀도)|faithfulness (신뢰성)|answer_relevancy (답변 적합성)|context_recall (문맥 재현율)|\n",
    "|---|---|---|---|\n",
    "|답변에 사용된 문맥이 정확했는지|답변이 문맥에 기반해 사실적(factual)인지|답변이 질문과 관련성이 있는지|답변이 문맥에서 얼마나 많은 정보를 활용했는지|\n",
    "|1.0: 답변에 사용된 정보가 문맥에서 매우 정확하게 활용됨|1.0: 답변이 문맥과 매우 정확하게 일치|1.0: 답변이 질문과 매우 높은 관련성을 가짐|1.0: 답변이 문맥의 모든 중요한 정보를 잘 회상|\n",
    "|0.0: 답변이 문맥과 무관하거나, 부정확한 정보를 활용|0.0: 답변이 문맥에 기반하지 않고, 사실과 다를 가능성이 큼|0.0: 답변이 질문과 관련이 없거나, 부정확|0.0: 답변이 문맥을 활용하지 못함|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.40.2-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (11.0.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (4.25.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (18.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (2.32.3)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (8.5.0)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (4.12.2)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.15.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading rpds_py-0.22.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rmarket\\anaconda3\\envs\\langchain\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
      "Using cached streamlit-1.40.2-py2.py3-none-any.whl (8.6 MB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading narwhals-1.15.1-py3-none-any.whl (232 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.22.0-cp311-cp311-win_amd64.whl (231 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: watchdog, toml, smmap, rpds-py, narwhals, mdurl, blinker, referencing, pydeck, markdown-it-py, gitdb, rich, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed altair-5.5.0 blinker-1.9.0 gitdb-4.0.11 gitpython-3.1.43 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 markdown-it-py-3.0.0 mdurl-0.1.2 narwhals-1.15.1 pydeck-0.9.1 referencing-0.35.1 rich-13.9.4 rpds-py-0.22.0 smmap-5.0.1 streamlit-1.40.2 toml-0.10.2 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
