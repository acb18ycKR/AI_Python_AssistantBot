{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê¸°ë³¸ rag ì„±ëŠ¥í‰ê°€ - gpt-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RMARKET\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI ì´ˆê¸°í™”\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "client = OpenAI()\n",
    "\n",
    "# PDF ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• \n",
    "def load_and_split_pdf(pdf_path, chunk_size=1100, chunk_overlap=100):\n",
    "    try:\n",
    "        loader = PDFPlumberLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        return text_splitter.split_documents(docs)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"PDF ë¡œë“œ ë° ë¶„í•  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ìƒì„±\n",
    "def create_embeddings():\n",
    "    return OpenAIEmbeddings()\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "def create_vector_store(documents, embeddings):\n",
    "    try:\n",
    "        return FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "# RAG ì²´ì¸ ìƒì„±\n",
    "def create_rag_chain(vectorstore):\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"ë„ˆëŠ” ìš©ì–´ ì‚¬ì „ì— ëŒ€í•œ ì „ë¬¸ê°€ì•¼. ë‹¤ìŒ ê²€ìƒ‰ëœ contextë¥¼ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì— ë§ëŠ” ìš©ì–´ë¥¼ ì •ì˜í•´ì¤˜.\n",
    "        ë‹µì„ ëª¨ë¥´ë©´, 'ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ëŒ€ë‹µí•´.\n",
    "\n",
    "        # Context : {context}\n",
    "        # Question : {question}\n",
    "        # Answer :\n",
    "        \"\"\"\n",
    "    )\n",
    "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "    return (\n",
    "        {'context': retriever, 'question': RunnablePassthrough()}  \n",
    "        | prompt  \n",
    "        | llm  \n",
    "        | StrOutputParser()  \n",
    "    )\n",
    "\n",
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "def generate_rag_answer(question, rag_chain):\n",
    "    try:\n",
    "        return rag_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "\n",
    "# PDF ë¬¸ì„œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "def initialize_rag_system(pdf_path):\n",
    "    try:\n",
    "        documents = load_and_split_pdf(pdf_path)\n",
    "        embeddings = create_embeddings()\n",
    "        vectorstore = create_vector_store(documents, embeddings)\n",
    "        return create_rag_chain(vectorstore)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    # PDF íŒŒì¼ ê²½ë¡œ\n",
    "    pdf_path = \"C:\\AI_Python_AssistantBot\\data\\converted_data_with_metadata.pdf\"\n",
    "    \n",
    "    try:\n",
    "        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "        rag_chain = initialize_rag_system(pdf_path)\n",
    "        \n",
    "        print(\"âœ… RAG ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì…ë ¥\n",
    "        question = \"íŒŒì´ì¬ì˜ forë¬¸ì˜ ì •ì˜ë¥¼ ì•Œë ¤ì¤˜\"\n",
    "        print(f\"ì§ˆë¬¸: {question}\")\n",
    "        \n",
    "        # ë‹µë³€ ìƒì„±\n",
    "        answer = generate_rag_answer(question, rag_chain)\n",
    "        print(f\"ë‹µë³€: {answer}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "# RAG ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\n",
    "def ragas_test(question, answer, retrieved_context):\n",
    "    \"\"\"\n",
    "    ì§ˆë¬¸, ë‹µë³€, ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\n",
    "    Args:\n",
    "        question (str): ì§ˆë¬¸\n",
    "        answer (str): ëª¨ë¸ì˜ ë‹µë³€\n",
    "        retrieved_context (list): ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸\n",
    "    Returns:\n",
    "        dict: í‰ê°€ ê²°ê³¼\n",
    "    \"\"\"\n",
    "    # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    dataset = [{\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"retrieved_context\": \"\\n\\n\".join(retrieved_context)\n",
    "    }]\n",
    "\n",
    "    # í‰ê°€ ì§€í‘œ ì •ì˜\n",
    "    def answer_relevancy(dataset):\n",
    "        relevancies = [1 if d[\"answer\"] in d[\"retrieved_context\"] else 0 for d in dataset]\n",
    "        return sum(relevancies) / len(relevancies)\n",
    "\n",
    "    def faithfulness(dataset):\n",
    "        faithfulness_scores = [1 if d[\"answer\"] == d[\"retrieved_context\"] else 0 for d in dataset]\n",
    "        return sum(faithfulness_scores) / len(faithfulness_scores)\n",
    "\n",
    "    def context_recall(dataset):\n",
    "        recalls = [1 if d[\"answer\"] in d[\"retrieved_context\"] else 0 for d in dataset]\n",
    "        return sum(recalls) / len(recalls)\n",
    "\n",
    "    def context_precision(dataset):\n",
    "        precisions = [1 if d[\"answer\"] in d[\"retrieved_context\"] else 0 for d in dataset]\n",
    "        return sum(precisions) / len(precisions)\n",
    "\n",
    "    # í‰ê°€ ì‹¤í–‰\n",
    "    metrics = [\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_recall,\n",
    "        context_precision\n",
    "    ]\n",
    "    results = {metric.__name__: metric(dataset) for metric in metrics}\n",
    "    return results\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    # ì˜ˆì‹œ ë°ì´í„°\n",
    "    question = \"íŒŒì´ì¬ì˜ forë¬¸ì˜ ì •ì˜ë¥¼ ì•Œë ¤ì¤˜\"\n",
    "    answer = \"forë¬¸ì€ ë°˜ë³µë¬¸ìœ¼ë¡œ, ë¦¬ìŠ¤íŠ¸ë‚˜ íŠœí”Œ ë“±ì˜ í•­ëª©ì„ ë°˜ë³µì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•œë‹¤.\"\n",
    "    retrieved_context = [\n",
    "        \"forë¬¸ì€ ë°˜ë³µë¬¸ìœ¼ë¡œ, ë¦¬ìŠ¤íŠ¸ë‚˜ íŠœí”Œ ë“±ì˜ í•­ëª©ì„ ë°˜ë³µì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•œë‹¤.\",\n",
    "        \"íŒŒì´ì¬ì˜ ë°˜ë³µë¬¸ì—ëŠ” forë¬¸ê³¼ whileë¬¸ì´ ìˆë‹¤.\"\n",
    "    ]\n",
    "    \n",
    "    # RAG ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰\n",
    "    result = ragas_test(question, answer, retrieved_context)\n",
    "    print(\"RAG ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê¸°ë³¸ rag ì„±ëŠ¥í‰ê°€ - gpt-4o-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI ì´ˆê¸°í™”\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "client = OpenAI()\n",
    "\n",
    "# PDF ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• \n",
    "def load_and_split_pdf(pdf_path, chunk_size=1100, chunk_overlap=100):\n",
    "    try:\n",
    "        loader = PDFPlumberLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        return text_splitter.split_documents(docs)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"PDF ë¡œë“œ ë° ë¶„í•  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ìƒì„±\n",
    "def create_embeddings():\n",
    "    return OpenAIEmbeddings()\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "def create_vector_store(documents, embeddings):\n",
    "    try:\n",
    "        return FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "# RAG ì²´ì¸ ìƒì„±\n",
    "def create_rag_chain(vectorstore):\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"ë„ˆëŠ” ìš©ì–´ ì‚¬ì „ì— ëŒ€í•œ ì „ë¬¸ê°€ì•¼. ë‹¤ìŒ ê²€ìƒ‰ëœ contextë¥¼ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì— ë§ëŠ” ìš©ì–´ë¥¼ ì •ì˜í•´ì¤˜.\n",
    "        ë‹µì„ ëª¨ë¥´ë©´, 'ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ëŒ€ë‹µí•´.\n",
    "\n",
    "        # Context : {context}\n",
    "        # Question : {question}\n",
    "        # Answer :\n",
    "        \"\"\"\n",
    "    )\n",
    "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "    return (\n",
    "        {'context': retriever, 'question': RunnablePassthrough()}  \n",
    "        | prompt  \n",
    "        | llm  \n",
    "        | StrOutputParser()  \n",
    "    )\n",
    "\n",
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "def generate_rag_answer(question, rag_chain):\n",
    "    try:\n",
    "        return rag_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "\n",
    "# PDF ë¬¸ì„œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "def initialize_rag_system(pdf_path):\n",
    "    try:\n",
    "        documents = load_and_split_pdf(pdf_path)\n",
    "        embeddings = create_embeddings()\n",
    "        vectorstore = create_vector_store(documents, embeddings)\n",
    "        return create_rag_chain(vectorstore)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    # PDF íŒŒì¼ ê²½ë¡œ\n",
    "    pdf_path = \"C:\\AI_Python_AssistantBot\\data\\converted_data_with_metadata.pdf\"\n",
    "    \n",
    "    try:\n",
    "        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "        rag_chain = initialize_rag_system(pdf_path)\n",
    "        \n",
    "        print(\"âœ… RAG ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì…ë ¥\n",
    "        question = \"íŒŒì´ì¬ì˜ forë¬¸ì˜ ì •ì˜ë¥¼ ì•Œë ¤ì¤˜\"\n",
    "        print(f\"ì§ˆë¬¸: {question}\")\n",
    "        \n",
    "        # ë‹µë³€ ìƒì„±\n",
    "        answer = generate_rag_answer(question, rag_chain)\n",
    "        print(f\"ë‹µë³€: {answer}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ê¸°ë³¸ rag ì„±ëŠ¥í‰ê°€ -  gpt-4-turbo-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI ì´ˆê¸°í™”\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "client = OpenAI()\n",
    "\n",
    "# PDF ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• \n",
    "def load_and_split_pdf(pdf_path, chunk_size=1100, chunk_overlap=100):\n",
    "    try:\n",
    "        loader = PDFPlumberLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        return text_splitter.split_documents(docs)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"PDF ë¡œë“œ ë° ë¶„í•  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ìƒì„±\n",
    "def create_embeddings():\n",
    "    return OpenAIEmbeddings()\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ìƒì„±\n",
    "def create_vector_store(documents, embeddings):\n",
    "    try:\n",
    "        return FAISS.from_documents(documents=documents, embedding=embeddings)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "# RAG ì²´ì¸ ìƒì„±\n",
    "def create_rag_chain(vectorstore):\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"ë„ˆëŠ” ìš©ì–´ ì‚¬ì „ì— ëŒ€í•œ ì „ë¬¸ê°€ì•¼. ë‹¤ìŒ ê²€ìƒ‰ëœ contextë¥¼ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ì— ë§ëŠ” ìš©ì–´ë¥¼ ì •ì˜í•´ì¤˜.\n",
    "        ë‹µì„ ëª¨ë¥´ë©´, 'ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ëŒ€ë‹µí•´.\n",
    "\n",
    "        # Context : {context}\n",
    "        # Question : {question}\n",
    "        # Answer :\n",
    "        \"\"\"\n",
    "    )\n",
    "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "    return (\n",
    "        {'context': retriever, 'question': RunnablePassthrough()}  \n",
    "        | prompt  \n",
    "        | llm  \n",
    "        | StrOutputParser()  \n",
    "    )\n",
    "\n",
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "def generate_rag_answer(question, rag_chain):\n",
    "    try:\n",
    "        return rag_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "\n",
    "# PDF ë¬¸ì„œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "def initialize_rag_system(pdf_path):\n",
    "    try:\n",
    "        documents = load_and_split_pdf(pdf_path)\n",
    "        embeddings = create_embeddings()\n",
    "        vectorstore = create_vector_store(documents, embeddings)\n",
    "        return create_rag_chain(vectorstore)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    # PDF íŒŒì¼ ê²½ë¡œ\n",
    "    pdf_path = \"C:\\AI_Python_AssistantBot\\data\\converted_data_with_metadata.pdf\"\n",
    "    \n",
    "    try:\n",
    "        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "        rag_chain = initialize_rag_system(pdf_path)\n",
    "        \n",
    "        print(\"âœ… RAG ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì…ë ¥\n",
    "        question = \"íŒŒì´ì¬ì˜ forë¬¸ì˜ ì •ì˜ë¥¼ ì•Œë ¤ì¤˜\"\n",
    "        print(f\"ì§ˆë¬¸: {question}\")\n",
    "        \n",
    "        # ë‹µë³€ ìƒì„±\n",
    "        answer = generate_rag_answer(question, rag_chain)\n",
    "        print(f\"ë‹µë³€: {answer}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT4o - RAPTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import umap\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "DB_INDEX = \"RAPTOR_DB\"\n",
    "\n",
    "# Embedding Initialization\n",
    "embd = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Chat Model Initialization\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "\n",
    "# PDF ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• \n",
    "# PDF ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• \n",
    "def load_and_split_pdf(pdf_path, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loader = PDFPlumberLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        print(f\"âœ… PDFì—ì„œ ë¡œë“œëœ ë¬¸ì„œ íƒ€ì…: {type(docs)}\")\n",
    "        print(f\"âœ… PDFì—ì„œ ë¡œë“œëœ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}\")\n",
    "        \n",
    "        for i, doc in enumerate(docs[:5]):  # ì²˜ìŒ 5ê°œ ë¬¸ì„œ íƒ€ì…ê³¼ ë‚´ìš©ì„ ì¶œë ¥\n",
    "            print(f\"ğŸ” ë¬¸ì„œ {i} íƒ€ì…: {type(doc)}\")\n",
    "            print(f\"ğŸ” ë¬¸ì„œ {i} ë‚´ìš©: {doc}\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        split_docs = text_splitter.split_documents(docs)\n",
    "        print(f\"âœ… ë¶„í• ëœ ë¬¸ì„œ ê°œìˆ˜: {len(split_docs)}\")\n",
    "        \n",
    "        for i, split_doc in enumerate(split_docs[:5]):  # ë¶„í• ëœ ë¬¸ì„œ ì¤‘ 5ê°œì˜ íƒ€ì…ê³¼ ë‚´ìš©ì„ ì¶œë ¥\n",
    "            print(f\"ğŸ” ë¶„í• ëœ ë¬¸ì„œ {i} íƒ€ì…: {type(split_doc)}\")\n",
    "            print(f\"ğŸ” ë¶„í• ëœ ë¬¸ì„œ {i} ë‚´ìš©: {split_doc}\")\n",
    "            \n",
    "        return split_docs\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"PDF ë¡œë“œ ë° ë¶„í•  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "    \n",
    "# Global Clustering\n",
    "def global_cluster_embeddings(embeddings, dim, n_neighbors=None, metric=\"cosine\"):\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(n_neighbors=n_neighbors, n_components=dim, metric=metric).fit_transform(embeddings)\n",
    "\n",
    "# Perform Clustering\n",
    "def perform_clustering(embeddings, dim, threshold):\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    gm = GaussianMixture(n_components=5, random_state=RANDOM_SEED).fit(reduced_embeddings_global)\n",
    "    probs = gm.predict_proba(reduced_embeddings_global)\n",
    "    labels = [np.where(prob > threshold)[0].tolist() for prob in probs]\n",
    "    return labels\n",
    "\n",
    "# Embedding Texts\n",
    "def embed_texts(texts):\n",
    "    embeddings = embd.embed_documents(texts)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Create Vectorstore\n",
    "def create_vectorstore(documents):\n",
    "    \"\"\"\n",
    "    Vectorstoreë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        vectorstore = FAISS.from_documents(documents=documents, embedding=embd)\n",
    "        if os.path.exists(DB_INDEX):\n",
    "            local_index = FAISS.load_local(DB_INDEX, embd)\n",
    "            local_index.merge_from(vectorstore)\n",
    "            local_index.save_local(DB_INDEX)\n",
    "        else:\n",
    "            vectorstore.save_local(DB_INDEX)\n",
    "        return vectorstore.as_retriever()\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "# RAG Chain Initialization\n",
    "def create_raptor_rag_chain(vectorstore):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are an expert at summarizing complex information. Use the given context to answer the question.\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "    return {\n",
    "        \"context\": vectorstore | (lambda docs: \"\\n\\n\".join([doc.page_content for doc in docs])),\n",
    "        \"question\": RunnablePassthrough()\n",
    "    } | prompt | model | StrOutputParser()\n",
    "\n",
    "# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "\n",
    "def initialize_raptor_rag_system(pdf_path):\n",
    "    try:\n",
    "        # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• \n",
    "        documents = load_and_split_pdf(pdf_path)\n",
    "        \n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í™•ì¸\n",
    "        texts = [doc.page_content for doc in documents]\n",
    "        print(f\"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(texts)} ê°œ ë¬¸ì„œ\")\n",
    "        print(f\"ğŸ” ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {texts[:5]}\")  # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì¤‘ 5ê°œ ì¶œë ¥\n",
    "\n",
    "        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±\n",
    "        retriever = create_vectorstore(documents)  # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬\n",
    "        print(\"âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ\")\n",
    "        \n",
    "        # RAG ì²´ì¸ ìƒì„±\n",
    "        return create_raptor_rag_chain(retriever)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"RAPTOR RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "def generate_raptor_rag_answer(question, rag_chain):\n",
    "    try:\n",
    "        return rag_chain.invoke(question)\n",
    "    except Exception as e:\n",
    "        return f\"Error generating answer: {str(e)}\" \n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    # PDF íŒŒì¼ ê²½ë¡œ\n",
    "    pdf_path = r\"C:\\Users\\RMARKET\\workspace\\assistntbot\\AI_Python_AssistantBot\\data\\converted_data_test_with_metadata.pdf\"\n",
    "    \n",
    "    try:\n",
    "        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "        rag_chain = initialize_raptor_rag_system(pdf_path)\n",
    "        \n",
    "        print(\"âœ… RAG ì‹œìŠ¤í…œì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì…ë ¥\n",
    "        question = \"ìš©ì–´ì‚¬ì „ í•¨ìˆ˜\"\n",
    "        print(f\"ì§ˆë¬¸: {question}\")\n",
    "        \n",
    "        # ë‹µë³€ ìƒì„±\n",
    "        answer = generate_rag_answer(question, rag_chain)\n",
    "        print(f\"ë‹µë³€: {answer}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
