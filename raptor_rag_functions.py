import os
import numpy as np
import pandas as pd
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sklearn.mixture import GaussianMixture
import umap
from langchain_community.document_loaders import PDFPlumberLoader

RANDOM_SEED = 42
DB_INDEX = "RAPTOR_DB"

# Embedding Initialization
embd = OpenAIEmbeddings(model="text-embedding-ada-002")

# Chat Model Initialization
model = ChatOpenAI(model="gpt-4o", temperature=0)

# PDF ë¡œë“œ ë° í…ìŠ¤íŠ¸ ë¶„í• 
def load_and_split_pdf(pdf_path, chunk_size=1000, chunk_overlap=100):
    """
    PDF ë¬¸ì„œë¥¼ ë¡œë“œí•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
    """
    try:
        loader = PDFPlumberLoader(pdf_path)
        docs = loader.load()
        print(f"âœ… PDFì—ì„œ ë¡œë“œëœ ë¬¸ì„œ íƒ€ì…: {type(docs)}")
        print(f"âœ… PDFì—ì„œ ë¡œë“œëœ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}")
        
        for i, doc in enumerate(docs[:5]):  # ì²˜ìŒ 5ê°œ ë¬¸ì„œ íƒ€ì…ê³¼ ë‚´ìš©ì„ ì¶œë ¥
            print(f"ğŸ” ë¬¸ì„œ {i} íƒ€ì…: {type(doc)}")
            print(f"ğŸ” ë¬¸ì„œ {i} ë‚´ìš©: {doc}")
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        split_docs = text_splitter.split_documents(docs)
        print(f"âœ… ë¶„í• ëœ ë¬¸ì„œ ê°œìˆ˜: {len(split_docs)}")
        
        for i, split_doc in enumerate(split_docs[:5]):  # ë¶„í• ëœ ë¬¸ì„œ ì¤‘ 5ê°œì˜ íƒ€ì…ê³¼ ë‚´ìš©ì„ ì¶œë ¥
            print(f"ğŸ” ë¶„í• ëœ ë¬¸ì„œ {i} íƒ€ì…: {type(split_doc)}")
            print(f"ğŸ” ë¶„í• ëœ ë¬¸ì„œ {i} ë‚´ìš©: {split_doc}")
            
        return split_docs
    except Exception as e:
        raise ValueError(f"PDF ë¡œë“œ ë° ë¶„í•  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
# Global Clustering
def global_cluster_embeddings(embeddings, dim, n_neighbors=None, metric="cosine"):
    if n_neighbors is None:
        n_neighbors = int((len(embeddings) - 1) ** 0.5)
    return umap.UMAP(n_neighbors=n_neighbors, n_components=dim, metric=metric).fit_transform(embeddings)

# Perform Clustering
def perform_clustering(embeddings, dim, threshold):
    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)
    gm = GaussianMixture(n_components=5, random_state=RANDOM_SEED).fit(reduced_embeddings_global)
    probs = gm.predict_proba(reduced_embeddings_global)
    labels = [np.where(prob > threshold)[0].tolist() for prob in probs]
    return labels

# Embedding Texts
def embed_texts(texts):
    embeddings = embd.embed_documents(texts)
    return np.array(embeddings)

# Create Vectorstore
def create_vectorstore(documents):
    """
    Vectorstoreë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        vectorstore = FAISS.from_documents(documents=documents, embedding=embd)
        if os.path.exists(DB_INDEX):
            local_index = FAISS.load_local(DB_INDEX, embd)
            local_index.merge_from(vectorstore)
            local_index.save_local(DB_INDEX)
        else:
            vectorstore.save_local(DB_INDEX)
        return vectorstore.as_retriever()
    except Exception as e:
        raise ValueError(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# RAG Chain Initialization
def create_raptor_rag_chain(vectorstore):
    prompt = ChatPromptTemplate.from_template(
        """You are an expert at summarizing complex information. Use the given context to answer the question.
        Context: {context}
        Question: {question}
        Answer:"""
    )
    return {
        "context": vectorstore | (lambda docs: "\n\n".join([doc.page_content for doc in docs])),
        "question": RunnablePassthrough()
    } | prompt | model | StrOutputParser()

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
def initialize_raptor_rag_system(pdf_path):
    try:
        # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
        documents = load_and_split_pdf(pdf_path)
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í™•ì¸
        texts = [doc.page_content for doc in documents]
        print(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(texts)} ê°œ ë¬¸ì„œ")
        print(f"ğŸ” ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ìƒ˜í”Œ: {texts[:5]}")  # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì¤‘ 5ê°œ ì¶œë ¥

        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        retriever = create_vectorstore(documents)  # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
        print("âœ… ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
        
        # RAG ì²´ì¸ ìƒì„±
        return create_raptor_rag_chain(retriever)
    except Exception as e:
        raise ValueError(f"RAPTOR RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
def generate_raptor_rag_answer(question, rag_chain):
    try:
        return rag_chain.invoke(question)
    except Exception as e:
        return f"Error generating answer: {str(e)}" 
    
# RAG ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def ragas_test(question, answer, retrieved_context):
    """
    ì§ˆë¬¸, ë‹µë³€, ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    Args:
        question (str): ì§ˆë¬¸
        answer (str): ëª¨ë¸ì˜ ë‹µë³€
        retrieved_context (list): ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸
    Returns:
        dict: í‰ê°€ ê²°ê³¼
    """
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = [{
        "question": question,
        "answer": answer,
        "retrieved_context": "\n\n".join(retrieved_context)
    }]

    # í‰ê°€ ì§€í‘œ ì •ì˜
    def answer_relevancy(dataset):
        relevancies = [1 if d["answer"] in d["retrieved_context"] else 0 for d in dataset]
        return sum(relevancies) / len(relevancies)

    def faithfulness(dataset):
        faithfulness_scores = [1 if d["answer"] == d["retrieved_context"] else 0 for d in dataset]
        return sum(faithfulness_scores) / len(faithfulness_scores)

    def context_recall(dataset):
        recalls = [1 if d["answer"] in d["retrieved_context"] else 0 for d in dataset]
        return sum(recalls) / len(recalls)

    def context_precision(dataset):
        precisions = [1 if d["answer"] in d["retrieved_context"] else 0 for d in dataset]
        return sum(precisions) / len(precisions)

    # í‰ê°€ ì‹¤í–‰
    metrics = [
        answer_relevancy,
        faithfulness,
        context_recall,
        context_precision
    ]
    results = {metric.__name__: metric(dataset) for metric in metrics}
    return results

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    # ì˜ˆì‹œ ë°ì´í„°
    question = "íŒŒì´ì¬ì˜ forë¬¸ì˜ ì •ì˜ë¥¼ ì•Œë ¤ì¤˜"
    answer = "forë¬¸ì€ ë°˜ë³µë¬¸ìœ¼ë¡œ, ë¦¬ìŠ¤íŠ¸ë‚˜ íŠœí”Œ ë“±ì˜ í•­ëª©ì„ ë°˜ë³µì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•œë‹¤."
    retrieved_context = [
        "forë¬¸ì€ ë°˜ë³µë¬¸ìœ¼ë¡œ, ë¦¬ìŠ¤íŠ¸ë‚˜ íŠœí”Œ ë“±ì˜ í•­ëª©ì„ ë°˜ë³µì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆê²Œ í•œë‹¤.",
        "íŒŒì´ì¬ì˜ ë°˜ë³µë¬¸ì—ëŠ” forë¬¸ê³¼ whileë¬¸ì´ ìˆë‹¤."
    ]
    
    # RAG ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰
    result = ragas_test(question, answer, retrieved_context)
    print("RAG ì„±ëŠ¥ í‰ê°€ ê²°ê³¼:", result)
